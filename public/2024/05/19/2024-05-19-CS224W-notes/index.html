<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>CS224W_notes | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="1 Introduction，Machine learning for graphs大纲大纲  Traditional methods: Graphlets, Graph Kernels Methods for node embeddings: DeepWalk, Node2Vec Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224W_notes">
<meta property="og:url" content="http://example.com/2024/05/19/2024-05-19-CS224W-notes/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1 Introduction，Machine learning for graphs大纲大纲  Traditional methods: Graphlets, Graph Kernels Methods for node embeddings: DeepWalk, Node2Vec Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/05/19/2024-05-19-CS224W-notes/2024-05-19-CS224W-notes/image.png">
<meta property="og:image" content="http://example.com/2024-05-19-CS224W-notes/image-1.png">
<meta property="article:published_time" content="2024-05-19T17:01:34.000Z">
<meta property="article:modified_time" content="2024-12-15T04:16:27.000Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/05/19/2024-05-19-CS224W-notes/2024-05-19-CS224W-notes/image.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2024-05-19-CS224W-notes" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/19/2024-05-19-CS224W-notes/" class="article-date">
  <time class="dt-published" datetime="2024-05-19T17:01:34.000Z" itemprop="datePublished">2024-05-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      CS224W_notes
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-Introduction，Machine-learning-for-graphs"><a href="#1-Introduction，Machine-learning-for-graphs" class="headerlink" title="1 Introduction，Machine learning for graphs"></a>1 Introduction，Machine learning for graphs</h1><h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><p>大纲</p>
<ol>
<li>Traditional methods: Graphlets, Graph Kernels</li>
<li>Methods for node embeddings: DeepWalk, Node2Vec</li>
<li>Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs</li>
<li>Knowledge graphs and reasoning: TransE, BetaE</li>
<li>Deep generative models for graphs</li>
<li>Applications to Biomedicine, Science, Industry</li>
</ol>
<h2 id="Defs"><a href="#Defs" class="headerlink" title="Defs"></a>Defs</h2><ol start="0">
<li>$G&#x3D;(V, E, F)$ or $G(V, E)$</li>
<li>Directed&#x2F; undirected</li>
<li>Degree<br>Directed $\bar{k} &#x3D; \langle k \rangle &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} k_i &#x3D; \frac{2E}{N}$<br>Undirected in-degree + out-degree &#x3D; (total) degree, $\bar{k}&#x3D; \frac{E}{N}$ </li>
<li>Bipartite Graph</li>
<li>Folded&#x2F;Projected Bipartite graph</li>
<li>Representing graphs: Adjacency matrix Density of matrix $\frac{E}{N^2}$<ol>
<li>adjacency matrix</li>
<li>Edge List</li>
<li>Adjacency List</li>
</ol>
</li>
<li>Attribute of edges</li>
<li>Weighted&#x2F;Unweighted</li>
<li>Self-edges</li>
<li>Connectivity: (Un)Directed, Strong Connected Components (in Undirected)</li>
</ol>
<h1 id="2-Traditional-methods-for-ML-on-graph"><a href="#2-Traditional-methods-for-ML-on-graph" class="headerlink" title="2 Traditional methods for ML on graph"></a>2 Traditional methods for ML on graph</h1><p>Structural Feature&#x2F; Node features<br>Train on Random Forest, SVM, Neural Network; Apply on new graph. </p>
<h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><h3 id="1-Node-Centrality"><a href="#1-Node-Centrality" class="headerlink" title="(1) Node Centrality"></a>(1) Node Centrality</h3><p>233</p>
<ol>
<li>Eigenvector centrality: $c_v&#x3D;\frac{1}{\lambda}\sum_{u\in N(v)}$</li>
<li>Betweenness centrality<br>Here is the improved version of the formulas in the format you provided, while keeping the same style and explanation:</li>
</ol>
<hr>
<ol>
<li><p><strong>Betweenness Centrality</strong><br>Betweenness centrality ( c_v ) measures the extent to which a node ( v ) lies on the shortest paths between other nodes.  </p>
<p>$$<br>c_v &#x3D; \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}<br>$$</p>
<ul>
<li>( \sigma_{st} ): The number of shortest paths between nodes ( s ) and ( t ).  </li>
<li>( \sigma_{st}(v) ): The number of shortest paths between ( s ) and ( t ) that pass through ( v ).</li>
</ul>
</li>
</ol>
<hr>
<ol start="2">
<li><p><strong>Closeness Centrality</strong><br>Closeness centrality ( c_v ) quantifies how close a node ( v ) is to all other nodes in the network.  </p>
<p>$$<br>c_v &#x3D; \frac{1}{\sum_{u \neq v} d(u, v)}<br>$$</p>
<ul>
<li>( d(u, v) ): The shortest path length between node ( u ) and node ( v ).</li>
</ul>
</li>
</ol>
<h3 id="2-Clustering-Coefficient"><a href="#2-Clustering-Coefficient" class="headerlink" title="(2) Clustering Coefficient"></a>(2) Clustering Coefficient</h3><p>[<br>e_v &#x3D; \frac{\text{Number of edges between neighbors of } v}{\binom{k_v}{2}}, \quad \binom{k_v}{2} &#x3D; \frac{k_v (k_v - 1)}{2}<br>] </p>
<h3 id="3-Graphlet-Rooted-connected-non-isomorphic-subgraphs"><a href="#3-Graphlet-Rooted-connected-non-isomorphic-subgraphs" class="headerlink" title="(3) Graphlet: Rooted connected non-isomorphic subgraphs"></a>(3) Graphlet: Rooted connected non-isomorphic subgraphs</h3><p>Graphlet degree vector<br>Clustering coefficient.<br>Feature-based&#x2F; structure-based features.</p>
<h2 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h2><h3 id="1-Link-prediction"><a href="#1-Link-prediction" class="headerlink" title="(1) Link prediction"></a>(1) Link prediction</h3><ol>
<li>Links missing at random: Remove a random set of links and then aim to predict them</li>
<li>Links over time: Given $G[t_0, t_0’]$ a graph on edges up to time $t_0’$, output a ranked list $L$ of links (not in $G[t_0, t_0’]$) that are predicted to appear in $G[t_1, t_1’]$</li>
</ol>
<h3 id="2-Local-Neighborhood-Overlap"><a href="#2-Local-Neighborhood-Overlap" class="headerlink" title="(2) Local Neighborhood Overlap"></a>(2) Local Neighborhood Overlap</h3><ul>
<li>Common neighbors: $|N(v_1) \cap N(v_2)|$</li>
<li>Jaccard’s coefficient: $\frac{|N(v_1) \cap N(v_2)|}{|N(v_1) \cup N(v_2)|}$,<br>Normalize common neighbor, assuming having the same number of neighbors</li>
<li>Adamic-Adar index: $\sum_{u \in N(v_1) \cap N(v_2)} \frac{1}{\log k_u}$,<br>Penalize those who have many neighbors</li>
</ul>
<h3 id="3-Global-Neighborhood-Overlap"><a href="#3-Global-Neighborhood-Overlap" class="headerlink" title="(3) Global Neighborhood Overlap"></a>(3) Global Neighborhood Overlap</h3><ul>
<li>Katz index: $S_{uv}&#x3D; \sum_{l&#x3D;1}^{\infty} \beta^l A^l_{uv}$,<br>$A^l_{uv}$: number of paths of length $l$ between $u$ and $v$,<br>$\beta$: discount factor, the contribution of long paths  </li>
<li>Katz index matrix: $S&#x3D; \sum_{i&#x3D;1}^\infty \beta^iA^i &#x3D; (I-\beta A)^{-1}-I$</li>
</ul>
<h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><h3 id="Kernel-method"><a href="#Kernel-method" class="headerlink" title="Kernel method:"></a>Kernel method:</h3><ol>
<li><p><strong>Kernel function</strong>:<br>$K(G, G’)$ measures the similarity between two graphs $G$ and $G’$.<br>Maps the graphs into a higher-dimensional space where linear methods can be applied to perform complex, non-linear tasks in the original space.</p>
</li>
<li><p><strong>Kernel Matrix</strong>: $\mathbf{K} &#x3D; \left( K(G, G’) \right)_{G, G’}$ is a symmetric matrix.<br>It is positive semidefinite, meaning all its eigenvalues are non-negative.</p>
</li>
<li><p><strong>Feature Representation</strong>:<br>Feature mapping $\phi(\cdot)$ kernel function is expressed as dot product in feature space: $K(G, G’) &#x3D; \phi(G)^\top \phi(G’)$.</p>
</li>
</ol>
<p>cite: <a target="_blank" rel="noopener" href="https://blog.csdn.net/PolarisRisingWar/article/details/115598815">CSDN Blog</a></p>
<h3 id="Design-graph-feature-vector-phi-G"><a href="#Design-graph-feature-vector-phi-G" class="headerlink" title="Design graph feature vector $\phi(G)$"></a>Design graph feature vector $\phi(G)$</h3><p><strong>Bag-of-Words: BoW</strong><br>Key idea: use the word counts as features (#nodes as features, #degree, #graphlet, #color)</p>
<h3 id="Graphlet-features"><a href="#Graphlet-features" class="headerlink" title="Graphlet features"></a>Graphlet features</h3><p>Key idea: Count the number of different graphlets in a graph.<br>$G_k&#x3D;(g_1, g_2, \cdots, g_{n_k})$</p>
<p>Graphlet count vector $(f_G)&#x3D;#(g_i \subseteq G), ; i \leq n_k$</p>
<p>$h_G&#x3D; \frac{f_G}{\text{Sum}(F_G)}$,<br>$K(G, G’)&#x3D;H_G^\top H_G’$</p>
<p>NP-hard $O(nd^{k-1})$, expensive to calculate </p>
<h3 id="Weisfeiler-Lehman-Kernel"><a href="#Weisfeiler-Lehman-Kernel" class="headerlink" title="Weisfeiler-Lehman Kernel"></a>Weisfeiler-Lehman Kernel</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/PolarisRisingWar/article/details/117336622">Weisfeiler-Lehman Kernel Blog</a></p>
<h1 id="3-Node-Embedding"><a href="#3-Node-Embedding" class="headerlink" title="3 Node Embedding"></a>3 Node Embedding</h1><h2 id="3-1-Intro"><a href="#3-1-Intro" class="headerlink" title="3.1 Intro"></a>3.1 Intro</h2><p>Key: How to define node similarity  </p>
<!-- Avoid direct Feature Engineering, and also reflect structural features -->
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Feature representation ↔ feature embedding  </p>
<!-- If two nodes are similar in structure, they should be similar in embedding space -->
<!-- Use dot product to measure similarity -->
<p>Similarity $(u, v) \approx z_u^T z_v$,<br>$ENC(u) &#x3D; z_u$, $ENC(v) &#x3D; z_v$, <!--ENC: Encoder--><br>$z_u, z_v$ are $d$-dimensional in embedding space, $d$ usually 64-1000<br>$ENC(v)$ node in the input graph</p>
<p>$ENC(v) &#x3D; z_v &#x3D; Z \cdot v$<br>Encoder is a lookup, embedding matrix $Z \in \mathbb{R}^{d \times |V|}$, $v \in I^{|V|}$</p>
<h4 id="ENC"><a href="#ENC" class="headerlink" title="ENC"></a>ENC</h4><p>Shallow encoder: $d \times |V|$<br>Some ways for ENC: DeepWalk, Node2Vec</p>
<h2 id="3-2-Random-walk-for-Node-Embedding"><a href="#3-2-Random-walk-for-Node-Embedding" class="headerlink" title="3.2 Random walk for Node Embedding"></a>3.2 Random walk for Node Embedding</h2><h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul>
<li>Vector $z_u$: embedding vector of node $u$ (what we aim to find)</li>
<li>$P(v|z_u)$: probability of visiting node $v$ on random walk starting from $u$. Used to measure similarity. </li>
<li>Softmax: $\sigma(z)_i &#x3D; \frac{e^{z_i}}{\sum e^{z_j}}$</li>
<li>Sigmoid: $S(x) &#x3D; \frac{1}{1 + e^{-x}}$</li>
</ul>
<h3 id="Random-walk-embedding"><a href="#Random-walk-embedding" class="headerlink" title="Random walk embedding"></a>Random walk embedding</h3><p>Using random strategy $R$: $P_R(u|v)$<br>Given<br>$G&#x3D;(V, E)$<br>Goal: Learn a mapping $f: u \rightarrow \mathbb{R}^d$<br>$$<br>\mathcal{L} &#x3D; \sum_{u \in V} \sum_{v \in N_R(u)} -\log\left(\frac{\exp(z_u^\top z_v)}{\sum_{n \in V} \exp(z_u^\top z_n)}\right)<br>$$<br><strong>Negative sampling</strong><br>$$<br>\log\left(\frac{\exp(z_u^\top z_v)}{\sum_{n \in V} \exp(z_u^\top z_n)}\right) \approx \log\left(\sigma(z_u^\top z_v)\right) - \sum_{i&#x3D;1}^{k} \log\left(\sigma(z_u^\top z_{n_i})\right), \quad n_i \sim P_V<br>$$ –&gt;</p>
<h3 id="Random-walk-strategy"><a href="#Random-walk-strategy" class="headerlink" title="Random walk strategy"></a>Random walk strategy</h3><h4 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h4><p><a target="_blank" rel="noopener" href="https://www.vldb.org/pvldb/vol10/p13-wu.pdf">DeepWalk Paper</a></p>
<h4 id="Node2Vec"><a href="#Node2Vec" class="headerlink" title="Node2Vec"></a>Node2Vec</h4><p>Node2Vec</p>
<p><strong>Hyperparameters:</strong></p>
<ul>
<li>$p$: Return parameter</li>
<li>$q$: In-out parameter</li>
</ul>
<h2 id="3-3-Embedding-Entire-Graph"><a href="#3-3-Embedding-Entire-Graph" class="headerlink" title="3.3 Embedding Entire Graph"></a>3.3 Embedding Entire Graph</h2><h3 id="Approach-1"><a href="#Approach-1" class="headerlink" title="Approach 1"></a>Approach 1</h3><p>Sum&#x2F;mean $z_G &#x3D; \sum_{v \in G} z_v$<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1509.09292">Embedding Entire Graph</a></p>
<h3 id="Approach-2"><a href="#Approach-2" class="headerlink" title="Approach 2"></a>Approach 2</h3><p>Virtual node<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.05493">Virtual Node Paper</a></p>
<h3 id="Approach-3"><a href="#Approach-3" class="headerlink" title="Approach 3"></a>Approach 3</h3><p>Anonymous walk:</p>
<h4 id="Sampling-Anonymous-walks"><a href="#Sampling-Anonymous-walks" class="headerlink" title="Sampling Anonymous walks"></a>Sampling Anonymous walks</h4><p>Distribution have error less than $\epsilon$ with probability, less than $\delta$<br>$m &#x3D; f(\epsilon, \sigma, \delta)$</p>
<h4 id="Walk-Embedding"><a href="#Walk-Embedding" class="headerlink" title="Walk Embedding"></a>Walk Embedding</h4><p>$\Delta$</p>
<h1 id="4-Node-Embedding-using-Random-walk-PageRank"><a href="#4-Node-Embedding-using-Random-walk-PageRank" class="headerlink" title="4 Node Embedding using Random walk - PageRank"></a>4 Node Embedding using Random walk - PageRank</h1><h2 id="4-1-Intro"><a href="#4-1-Intro" class="headerlink" title="4.1 Intro"></a>4.1 Intro</h2><p>PageRank: $r_v &#x3D; \sum_{u \in N_R(v)} r_u \frac{A_{u,v}}{d_u}$<br>$d_u$: degree of node $u$<br>$A_{u,v}$: adjacency matrix<br>$r_v$: rank of node $v$<br>$N_R(v)$: neighbors of node $v$</p>
<h2 id="4-2-PageRank-for-Graph"><a href="#4-2-PageRank-for-Graph" class="headerlink" title="4.2 PageRank for Graph"></a>4.2 PageRank for Graph</h2><p>$r_v &#x3D; \sum_{u \in N_R(v)} r_u \frac{A_{u,v}}{d_u}$<br>$d_u$: degree of node $u$<br>$A_{u,v}$: adjacency matrix<br>$r_v$: rank of node $v$<br>$N_R(v)$: neighbors of node $v$</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>$r_v &#x3D; \sum_{u \in N_R(v)} r_u \frac{A_{u,v}}{d_u}$</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><ul>
<li>Personalized PageRank (Topic specific PageRank)<br>Rank proximity of nodes to the teleport nodes $S$,<br>Proximity on graphs: </li>
<li>PageRank with restarts:</li>
</ul>
<h3 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h3><p>Frobenius norm: $\min_z ||A - Z^\top Z||$<br><img src="2024-05-19-CS224W-notes/image.png" alt="Alt text" width="400" ></p>
<h1 id="5-Message-passing-Node-Classification"><a href="#5-Message-passing-Node-Classification" class="headerlink" title="5 Message passing &amp; Node Classification"></a>5 Message passing &amp; Node Classification</h1><p>Classical methods</p>
<p><strong>Correlation:</strong> nearby nodes have the same color<br>$A_{n \times n}$: Adjacency matrix<br>$Y &#x3D; {0, 1}^n$</p>
<h2 id="Collective-Classification"><a href="#Collective-Classification" class="headerlink" title="Collective Classification:"></a>Collective Classification:</h2><ol>
<li>Local classifier </li>
<li>Relational Classifier </li>
<li>Collective Inference<br>$1^{st}$ order Markov assumption: $P(Y_v) &#x3D; P(Y_v | N_v)$</li>
</ol>
<ul>
<li>Relational classification</li>
<li>Iterative classification </li>
<li>Belief propagation</li>
</ul>
<h1 id="6-GNN-Model"><a href="#6-GNN-Model" class="headerlink" title="6 GNN Model"></a>6 GNN Model</h1><h1 id="7-GNN-Design-Space"><a href="#7-GNN-Design-Space" class="headerlink" title="7 GNN Design Space"></a>7 GNN Design Space</h1><h1 id="8-Training-GNN"><a href="#8-Training-GNN" class="headerlink" title="8 Training GNN"></a>8 Training GNN</h1><h2 id="8-1-Data-augmentation"><a href="#8-1-Data-augmentation" class="headerlink" title="8.1 Data augmentation"></a>8.1 Data augmentation</h2><h3 id="Feature-based"><a href="#Feature-based" class="headerlink" title="Feature based"></a>Feature based</h3><h3 id="Structure-based"><a href="#Structure-based" class="headerlink" title="Structure based"></a>Structure based</h3><h2 id="8-2"><a href="#8-2" class="headerlink" title="8.2"></a>8.2</h2><h2 id="8-3"><a href="#8-3" class="headerlink" title="8.3"></a>8.3</h2><p><img src="/2024-05-19-CS224W-notes/image-1.png" alt="alt text"></p>
<p><strong>Node Prediction</strong>  </p>
<ul>
<li>Transductive setting</li>
<li>Inductive setting</li>
</ul>
<p><strong>Training</strong><br>Validation (tuning hyperparameters)<br>Test set</p>
<p><strong>Graph Prediction</strong>  </p>
<ul>
<li>Link Prediction –&gt;</li>
</ul>
<h1 id="9-Theory-of-GNN"><a href="#9-Theory-of-GNN" class="headerlink" title="9 Theory of GNN"></a>9 Theory of GNN</h1><p>GCN, GAT, GraphSAGE, design space </p>
<h2 id="9-1"><a href="#9-1" class="headerlink" title="9.1"></a>9.1</h2><h2 id="9-2"><a href="#9-2" class="headerlink" title="9.2"></a>9.2</h2><p>GCN Mean pooling fails<br>GraphSAGE mean-pool </p>
<p>Injective Multiset function: $\Phi(\cdot)$: a non-linear function:<br>$\Phi(\sum_{x \in S} f(x))$:<br>Multi-layer Perceptron<br><strong>Theorem:</strong> Universal approximation theorem<br>A neural network can model any injective multiset function:<br>$MLP_{\Phi}(\sum_{x \in S} MLP_{f}(x))$</p>
<p><strong>Graph Isomorphism Network (GIN) Xue 2019</strong> </p>
<p><strong>WL Graph Kernel</strong><br>Hash<br>$$<br>\left( c^{(k)}(v), { c^{(k)}(u) }_{u \in N(v)} \right)<br>$$</p>
<p>$$<br>\text{MLP}<em>{\Phi} \left( (1 + \epsilon) \cdot \text{MLP}</em>{f}(c^{(k)}(v)) + \sum_{u \in N(v)} \text{MLP}_{f}(c^{(k)}(u)) \right)<br>$$</p>
<p>where $\epsilon$ is a learnable scalar</p>
<p>$$<br>c^{(k+1)}(v) &#x3D; \text{HASH} \left( c^{(k)}(v), { c^{(k)}(u) }_{u \in N(v)} \right)<br>$$</p>
<p>$$<br>\text{GINConv} \left( c^{(k)}(v), { c^{(k)}(u) }<em>{u \in N(v)} \right) &#x3D; \text{MLP}</em>{\Phi} \left( (1 + \epsilon) \cdot c^{(k)}(v) + \sum_{u \in N(v)} c^{(k)}(u) \right)<br>$$</p>
<h1 id="10-Heterogeneous-Graphs-and-Knowledge-Graph-Embeddings"><a href="#10-Heterogeneous-Graphs-and-Knowledge-Graph-Embeddings" class="headerlink" title="10 Heterogeneous Graphs and Knowledge Graph Embeddings"></a>10 Heterogeneous Graphs and Knowledge Graph Embeddings</h1><h2 id="10-1"><a href="#10-1" class="headerlink" title="10.1"></a>10.1</h2><h3 id="Heterogeneous-Graphs"><a href="#Heterogeneous-Graphs" class="headerlink" title="Heterogeneous Graphs"></a>Heterogeneous Graphs</h3><p>$G&#x3D;(V, E, R, T)$</p>
<h2 id="RGCN"><a href="#RGCN" class="headerlink" title="RGCN"></a>RGCN</h2><h1 id="VGAE"><a href="#VGAE" class="headerlink" title="VGAE"></a>VGAE</h1><p>讲的比较好的GAE和VGAE</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.atyun.com/17976.html">AtYun Article</a></li>
<li><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/5253#%E7%BB%88%E7%82%B9%E7%AB%99">Spaces Article</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_16763983/article/details/120403055?spm=1001.2101.3001.6650.7&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-120403055-blog-119531815.235%5Ev43%5Epc_blog_bottom_relevance_base8&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-120403055-blog-119531815.235%5Ev43%5Epc_blog_bottom_relevance_base8&utm_relevant_index=13">CSDN Blog</a></li>
</ol>
<h3 id="CNN-code"><a href="#CNN-code" class="headerlink" title="CNN code"></a>CNN code</h3><ol>
<li>CNN 网络结构与部分 PyTorch: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/wpx123/p/17616156.html">CNBlogs 1</a>, <a target="_blank" rel="noopener" href="https://www.cnblogs.com/wpx123/p/17621303.html">CNBlogs 2</a></li>
</ol>
<h4 id="Optimizer："><a href="#Optimizer：" class="headerlink" title="Optimizer："></a>Optimizer：</h4><p>SGD, GD, Adam 都是 Optimizer 的种类</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346205754">PyTorch 源代码解读</a>, 以及各种参数 lr, gamma 的影响</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/xian0710830114/article/details/126551268">简单讲解了 SGD， Adam 的原理</a></li>
</ol>
<h3 id="一个比较有用的-Casual-Inference-综述的博客："><a href="#一个比较有用的-Casual-Inference-综述的博客：" class="headerlink" title="一个比较有用的 Casual Inference 综述的博客："></a>一个比较有用的 Casual Inference 综述的博客：</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/caoyusang/p/13518354.html">Casual Inference 综述</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/19/2024-05-19-CS224W-notes/" data-id="cm9bnagps000yzc3d7hmqckbt" data-title="CS224W_notes" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/05/29/2024-05-29-Casual-Inference/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Casual Inference
        
      </div>
    </a>
  
  
    <a href="/2024/05/08/2024-05-08-Meetings-log/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">papers</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/music/" rel="tag">music</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/music/" style="font-size: 10px;">music</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/04/01/2025-04-01-Linux-server-proxy-issue/">2025-04-01-Linux-server-proxy-issue</a>
          </li>
        
          <li>
            <a href="/2025/04/01/2025-04-01-mt-implementation-log/">2025-04-01-mt-implementation-log</a>
          </li>
        
          <li>
            <a href="/2025/03/09/2025-03-08-medication/">2025-03-08 medication</a>
          </li>
        
          <li>
            <a href="/2025/02/15/2025-02-15-DS/">2025-02-15-DS</a>
          </li>
        
          <li>
            <a href="/2025/01/29/2025-01-29-CV/">2025-01-29-CV</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>