<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Casual Inference | Tlhey</title><meta name="author" content="Tlhey"><meta name="copyright" content="Tlhey"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="与贝叶斯有关的频率派的观点为 $p(X|\theta)\mathop{&#x3D;}\limits_{iid}\prod\limits {i&#x3D;1}^{N}p(x{i}|\theta)$  为了求常量 $\theta$ 的大小,最大对数似然MLE的方法：$\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\m">
<meta property="og:type" content="article">
<meta property="og:title" content="Casual Inference">
<meta property="og:url" content="https://tlhey.github.io/whatever/2024/05/29/2024-05-29-Casual-Inference/index.html">
<meta property="og:site_name" content="Tlhey">
<meta property="og:description" content="与贝叶斯有关的频率派的观点为 $p(X|\theta)\mathop{&#x3D;}\limits_{iid}\prod\limits {i&#x3D;1}^{N}p(x{i}|\theta)$  为了求常量 $\theta$ 的大小,最大对数似然MLE的方法：$\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\m">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://tlhey.github.io/whatever/img/bc12.jpeg">
<meta property="article:published_time" content="2024-05-29T17:53:31.000Z">
<meta property="article:modified_time" content="2024-12-15T11:14:33.000Z">
<meta property="article:author" content="Tlhey">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tlhey.github.io/whatever/img/bc12.jpeg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Casual Inference",
  "url": "https://tlhey.github.io/whatever/2024/05/29/2024-05-29-Casual-Inference/",
  "image": "https://tlhey.github.io/whatever/img/bc12.jpeg",
  "datePublished": "2024-05-29T17:53:31.000Z",
  "dateModified": "2024-12-15T11:14:33.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "Tlhey",
      "url": "https://tlhey.github.io/whatever/"
    }
  ]
}</script><link rel="shortcut icon" href="/whatever/img/favicon.png"><link rel="canonical" href="https://tlhey.github.io/whatever/2024/05/29/2024-05-29-Casual-Inference/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/whatever/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/whatever/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Casual Inference',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/Background.jpeg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/whatever/img/Avatar.jpg" onerror="this.onerror=null;this.src='/whatever/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/whatever/archives/"><div class="headline">Articles</div><div class="length-num">35</div></a><a href="/whatever/tags/"><div class="headline">Tags</div><div class="length-num">2</div></a><a href="/whatever/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/whatever/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/whatever/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/whatever/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/whatever/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/bc12.jpeg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/whatever/"><img class="site-icon" src="/whatever/img/Navi.jpg" alt="Logo"><span class="site-name">Tlhey</span></a><a class="nav-page-title" href="/whatever/"><span class="site-name">Casual Inference</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/whatever/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/whatever/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/whatever/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/whatever/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/whatever/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Casual Inference</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-05-29T17:53:31.000Z" title="Created 2024-05-29 13:53:31">2024-05-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-12-15T11:14:33.000Z" title="Updated 2024-12-15 06:14:33">2024-12-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="与贝叶斯有关的"><a href="#与贝叶斯有关的" class="headerlink" title="与贝叶斯有关的"></a>与贝叶斯有关的</h2><p>频率派的观点<br>为 $p(X|\theta)\mathop{&#x3D;}\limits_{iid}\prod\limits <em>{i&#x3D;1}^{N}p(x</em>{i}|\theta)$ </p>
<p>为了求常量 $\theta$ 的大小,最大对数似然MLE的方法：<br>$\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{&#x3D;}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)$</p>
<p>贝叶斯派的观点<br>$p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的先验的分布 $\theta\sim p(\theta)$</p>
<p>$p(\theta|X)&#x3D;\frac{p(X|\theta)\cdot p(\theta)}{p(X)}&#x3D;\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}$</p>
<p>为了求 $\theta$ 的值，我们要最大化这个参数后验MAP：</p>
<p>$\theta_{MAP}&#x3D;\mathop{argmax}\limits _{\theta}p(\theta|X)&#x3D;\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)$</p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p><img src="/whatever/2024-05-29-Casual-Inference/image.png" alt="alt text"><br><img src="/whatever/2024-05-29-Casual-Inference/image-1.png" alt="alt text"></p>
<p>基础知识和概念，包括d-分离<br>do算子<br>后门调整<br>前门调整<br>逆概率加权<br>反事实<br>因果关系发现中最基本的两类方法：基于独立性测试的方法，以及通过加性噪声模型的形式分析残差与预测者独立性关系的方法</p>
<h2 id="chap1"><a href="#chap1" class="headerlink" title="chap1"></a>chap1</h2><p>partition, law of total probability<br>summing up its probabilities over all Bi is called marginalizing over $B$, and the resulting probability P(A) is called the marginal probability of $A$.<br>$P(A)&#x3D;P(A,B_1)+P(A,B_2)+···+P(A,B_n)$</p>
<p>Def conditional probabilities<br>$P(A|B)&#x3D;P(A,B)∕P(B)$</p>
<p>independence, giving no additional information<br>$P(A,B)&#x3D;P(A)P(B)$</p>
<p>$P(A|B)&#x3D;\frac{P(B|A)P(A)}{P(B)}$</p>
<p>$P(A)&#x3D;P(A|B_1)P(B_1)+P(A|B_2)P(B-2)+···+P(A|B_k)P(B_k)$</p>
<p>Sructual Casual Models SCM<br>U exogenous variables, external to the model;</p>
<h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><h2 id="https-www-yuque-com-bystander-wg876-yc5f72-dvgo5b机器学习模型可以从频率派和贝叶斯派频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC-等-Def-lambda-pi-A-B-pi-is-the-initial-state-distribution-o-t-来表示观测变量，-O-为观测序列，-V-v-1-v-2-cdots-v-M-表示观测的值域-i-t-表示状态变量，-I-为状态序列，-Q-q-1-q-2-cdots-q-N-表示状态变量的值域-A-a-ij-p-i-t-1-q-j-i-t-q-i-状态转移矩阵-B-b-j-k-p-o-t-v-k-i-t-q-j-发射矩阵"><a href="#https-www-yuque-com-bystander-wg876-yc5f72-dvgo5b机器学习模型可以从频率派和贝叶斯派频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC-等-Def-lambda-pi-A-B-pi-is-the-initial-state-distribution-o-t-来表示观测变量，-O-为观测序列，-V-v-1-v-2-cdots-v-M-表示观测的值域-i-t-表示状态变量，-I-为状态序列，-Q-q-1-q-2-cdots-q-N-表示状态变量的值域-A-a-ij-p-i-t-1-q-j-i-t-q-i-状态转移矩阵-B-b-j-k-p-o-t-v-k-i-t-q-j-发射矩阵" class="headerlink" title="https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b机器学习模型可以从频率派和贝叶斯派频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC 等### Def$\lambda&#x3D;(\pi,A,B)$- $\pi$ is the initial state distribution- $o_t$ 来表示观测变量，$O$ 为观测序列，$V&#x3D;{v_1,v_2,\cdots,v_M}$ 表示观测的值域- $i_t$ 表示状态变量，$I$ 为状态序列，$Q&#x3D;{q_1,q_2,\cdots,q_N}$ 表示状态变量的值域- $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$状态转移矩阵- $B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ 发射矩阵"></a><a target="_blank" rel="noopener" href="https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b">https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b</a><br>机器学习模型可以从频率派和贝叶斯派<br>频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC 等<br>### Def<br>$\lambda&#x3D;(\pi,A,B)$<br><br>- $\pi$ is the initial state distribution<br>- $o_t$ 来表示观测变量，$O$ 为观测序列，$V&#x3D;{v_1,v_2,\cdots,v_M}$ 表示观测的值域<br>- $i_t$ 表示状态变量，$I$ 为状态序列，$Q&#x3D;{q_1,q_2,\cdots,q_N}$ 表示状态变量的值域<br>- $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$状态转移矩阵<br>- $B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ 发射矩阵</h2><h4 id="两个基本假设"><a href="#两个基本假设" class="headerlink" title="两个基本假设"></a>两个基本假设</h4><ol>
<li>齐次 Markov 假设（未来只依赖于当前）：<br>$p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)&#x3D;p(i_{t+1}|i_t)$</li>
<li>观测独立假设：<br>$p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)&#x3D;p(o_t|i_t)$</li>
</ol>
<h4 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h4><ol>
<li>Evaluation：$p(O|\lambda)$，Forward-Backward </li>
<li>Learning：$\lambda&#x3D;\mathop{argmax}\limits_{\lambda}p(O|\lambda)$，EM （Baum-Welch）</li>
<li>Decoding：$I&#x3D;\mathop{argmax}\limits_{I}p(I|O,\lambda)$，Vierbi 算法<br>  a. 预测问题：$p(i_{t+1}|o_1,o_2,\cdots,o_t)$<br>  b. 滤波问题：$p(i_t|o_1,o_2,\cdots,o_t)$</li>
</ol>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h2 id="-2"><a href="#-2" class="headerlink" title=""></a></h2><h2 id="有关MCMC"><a href="#有关MCMC" class="headerlink" title="有关MCMC"></a>有关MCMC</h2><h2 id="一个比较基础的介绍：-https-zhuanlan-zhihu-com-p-420214359-Abstract-贝叶斯推断估计参数的方法是：我们可以算出参数-Theta-的分布函数-P-Theta-，我们用参数分布的数学期望作为对参数的估计值-MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计-我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值"><a href="#一个比较基础的介绍：-https-zhuanlan-zhihu-com-p-420214359-Abstract-贝叶斯推断估计参数的方法是：我们可以算出参数-Theta-的分布函数-P-Theta-，我们用参数分布的数学期望作为对参数的估计值-MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计-我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值" class="headerlink" title="一个比较基础的介绍： https://zhuanlan.zhihu.com/p/420214359- Abstract:  - 贝叶斯推断估计参数的方法是：我们可以算出参数$\Theta$的分布函数$P(\Theta)$，我们用参数分布的数学期望作为对参数的估计值  - MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计  - 我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值"></a>一个比较基础的介绍： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420214359">https://zhuanlan.zhihu.com/p/420214359</a><br>- Abstract:<br>  - 贝叶斯推断估计参数的方法是：我们可以算出参数$\Theta$的分布函数$P(\Theta)$，我们用参数分布的数学期望作为对参数的估计值<br>  - MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计<br>  - 我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值</h2><h3 id="1-Monte-Carlo-Sampling"><a href="#1-Monte-Carlo-Sampling" class="headerlink" title="1 Monte Carlo Sampling"></a>1 Monte Carlo Sampling</h3><p>如果$X$服从$f(x)$这个概率分布，我怎么获得$E(X)$<br>最常见的一种Monte Carlo方法的使用场景就是：对随机变量进行充分多的采样后，使用这些采样的均值来估计总体的期望</p>
<p>对于随机变量$X$，它的概率密度函数为$p(x)$，因此它的数学期望为<br>$E(x)&#x3D;\int_{-\infty}^{+\infty}xp(x)dx$<br>我们对于这个随机变量随机采样得到$n$个采样值$x_i$，根据大数定理，有<br>$\lim_{n\rightarrow+\infty}{\frac1n\sum_i^n{x_i}}&#x3D;E(X)$</p>
<h3 id="2-Bayes-MCMC"><a href="#2-Bayes-MCMC" class="headerlink" title="2 Bayes &amp; MCMC"></a>2 Bayes &amp; MCMC</h3><h4 id="2-1-Bayes-Model-参数-Theta-，Observed-data-D"><a href="#2-1-Bayes-Model-参数-Theta-，Observed-data-D" class="headerlink" title="2.1 Bayes Model: 参数$\Theta$ ，Observed data: $D$"></a>2.1 Bayes Model: 参数$\Theta$ ，Observed data: $D$</h4><p>贝叶斯公式：$P(\Theta|D)&#x3D;\frac1{P(D)}P(D|\Theta)P(\Theta)$</p>
<p>由于$P(D)$是一个无关紧要的常数，因此上式往往直接写成一个正比关系式：<br>$P(\Theta|D)\propto P(D|\Theta)P(\Theta)$</p>
<p>在贝叶斯推断里：</p>
<ol>
<li>通过$P(\Theta|D)$来得到$\Theta$的估计值</li>
<li>模型给出$P(D|\Theta)$， 即likelihood$P(D|\Theta)$</li>
<li>还可以通过$P(\Theta)$来对参数的分布情况做一些先验的猜测。（如果你什么都不知道，$P(\Theta)$自然可以猜一个均匀分布）</li>
</ol>
<h4 id="2-2-通过后验概率-P-Theta-D-获取参数-Theta-的估计值"><a href="#2-2-通过后验概率-P-Theta-D-获取参数-Theta-的估计值" class="headerlink" title="2.2 通过后验概率$P(\Theta|D)$获取参数$\Theta$的估计值"></a>2.2 通过后验概率$P(\Theta|D)$获取参数$\Theta$的估计值</h4><p>想法：众数或者期望作为<br>估计值</p>
<ol>
<li>众数：$\hat\Theta&#x3D;\arg\max_\Theta{P(\Theta|D)}$</li>
<li>期望：$\hat\Theta&#x3D;\int_\Theta\Theta P(\Theta|D)d\Theta$</li>
</ol>
<p>MCMC就是教我们怎么在一个没有解析形式的数据上「抽样几个数据算平均值」的方法</p>
<h3 id="3-Sampling-采样"><a href="#3-Sampling-采样" class="headerlink" title="3 Sampling 采样"></a>3 Sampling 采样</h3><ol>
<li>Uniform</li>
<li>Gaussian:<br>Given $U_1,U_2$<br>  $$<br>   Z_0 &#x3D; \sqrt{-2 \ln U_1} \cos(2\pi U_2)<br>   $$<br>   $$<br>   Z_1 &#x3D; \sqrt{-2 \ln U_1} \sin(2\pi U_2)<br>   $$</li>
<li>Reject-Accept: 用于对很不规则的$f(x)$采样。具体细节没看</li>
<li></li>
</ol>
<h3 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov Chain"></a>Markov Chain</h3><p>一个对马尔可夫状态讲的比较详细的文章：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250146007">https://zhuanlan.zhihu.com/p/250146007</a></p>
<h4 id="Def"><a href="#Def" class="headerlink" title="Def"></a>Def</h4><p>转移概率矩阵：</p>
<p>$P&#x3D;\begin{bmatrix}p_{11} &amp; p_{12} &amp;p_{13} \ p_{21} &amp; p_{22} &amp;p_{23} \ p_{31} &amp; p_{32} &amp;p_{33}\end{bmatrix}$ </p>
<p>其中 $p_{ij}&#x3D;P(X_{t}&#x3D;i|X_{t-1}&#x3D;j)$ 。</p>
<p>定义：马尔科夫链在 $t$ 时刻的概率分布称为 $t$ 时刻的状态分布：</p>
<p>$\pi (t)&#x3D;\begin{bmatrix}\pi_{1}(t) \ \pi_{2}(t) \ \pi_{3}(t)\end{bmatrix}$ </p>
<p>其中  $\pi_{i} (t)&#x3D;P(X_{t}&#x3D;i),i&#x3D;1,2,…$ 。</p>
<h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><ol>
<li><p>定理：给定一个马尔科夫链 $X&#x3D;\left{ X_0,X_1,…,X_t,… \right}$ ， $t$ 时刻的状态分布：<br> $\pi&#x3D;(\pi_1,\pi_2,…)$ 是 $X$ 的平稳分布的条件是 $\pi&#x3D;(\pi_1,\pi_2,…)$ 是下列方程组的解：<br> $x_{i}&#x3D;\sum_{j}{p_{ij}x_j},i&#x3D;1,2,…$<br> $x_i\geq0,i&#x3D;1,2,…$<br> $\sum_{i}{x_{i}&#x3D;1}$</p>
</li>
<li></li>
</ol>
<h3 id="MCMC具体细节"><a href="#MCMC具体细节" class="headerlink" title="MCMC具体细节"></a>MCMC具体细节</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/253784711">https://zhuanlan.zhihu.com/p/253784711</a></p>
<h2 id="Conditional-Random-Field-CRF"><a href="#Conditional-Random-Field-CRF" class="headerlink" title="Conditional Random Field(CRF)"></a>Conditional Random Field(CRF)</h2><ul>
<li>HMM 生成模型</li>
<li>MEMM Maximum Entropy Markov Model</li>
</ul>
<p><img src="/whatever/2024-05-29-Casual-Inference/image-4.png" alt="alt text"></p>
<ul>
<li>HMM:<br>$$ P(\mathbf{X}, \mathbf{Y} | \lambda) &#x3D; P(\mathbf{Y} | \lambda) P(\mathbf{X} | \mathbf{Y}, \lambda) $$</li>
<li>MEMM:<br>$$ P(y_t | y_{t-1}, x_t) $$</li>
<li>CRF:<br>$$ P(\mathbf{Y} | \mathbf{X}, \lambda) &#x3D; \frac{1}{Z(\mathbf{X})} \exp \left( \sum_{t&#x3D;1}^{T} \lambda_t f(y_t, y_{t-1}, \mathbf{X}, t) \right) $$</li>
</ul>
<h2 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1BW41117xo/?spm_id_from=333.999.0.0&vd_source=441679270dda23308fe16f3c5602b058">https://www.bilibili.com/video/BV1BW41117xo/?spm_id_from=333.999.0.0&amp;vd_source=441679270dda23308fe16f3c5602b058</a></p>
<h3 id="概率和图"><a href="#概率和图" class="headerlink" title="概率和图"></a>概率和图</h3><p>概率图</p>
<ul>
<li>表示 Representation<ul>
<li>有向图 Beyesian Netowrk</li>
<li>无向图</li>
<li>高斯图（连续的随机变量）</li>
</ul>
</li>
<li>推断 Inference<ul>
<li>精确推断</li>
<li>近似推断<ul>
<li>确定性推断（变分）</li>
<li>随机近似 MCMC</li>
</ul>
</li>
</ul>
</li>
<li>学习<ul>
<li>参数学习<ul>
<li>完备数据</li>
<li>隐变量</li>
</ul>
</li>
<li>结构学习</li>
</ul>
</li>
</ul>
<p>高维随机变量</p>
<ul>
<li>sum: $P(x_1) &#x3D; \int P(x_1, x_2)dx_2$</li>
<li>product: $P(x_1|x_2) &#x3D; P(x_1|x_2)P(x_2)&#x3D;P(x_2|x_1)P(x_1)$</li>
</ul>
<p>链式法则</p>
<ul>
<li>$$P(X_1,X_2,…,X_n)&#x3D;P(X_1)P(X_2|X_1)P(X_3|X_2,X_1)···P(X_n|X_{n-1},X_{n-2},…,X_1)$$</li>
</ul>
<p>全概率公式</p>
<ul>
<li>$P(X_i)&#x3D;\sum_{j}{P(X_i,X_j)}&#x3D;\sum_{j}{P(X_i|X_j)P(X_j)}$</li>
</ul>
<p>贝叶斯公式</p>
<ul>
<li>$P(X_i|X_j)&#x3D;\frac{P(X_i,X_j)}{P(X_j)}&#x3D;\frac{P(X_i|X_j)P(X_j)}{P(X_j)}$</li>
</ul>
<p>困境：<br>维度高$P(X_1,X_2,…,X_n)$计算复杂</p>
<ul>
<li>1.假设$X_i$相互独立:<ul>
<li>$P(X_1,X_2,…,X_n)&#x3D;\prod_{i}{P(X_i)}$</li>
</ul>
</li>
<li>2.Markov Property(HMM齐次马尔可夫):<ul>
<li>$x_j\perp x_i+1|x_i, j&lt;i$ </li>
<li>$P(X_1, X_2,…,X_n)&#x3D;P(X_1)P(X_2|X_1)P(X_3|X_2,X_1)···P(X_n|X_{n-1},X_{n-2},…,X_1)$</li>
</ul>
</li>
<li>3.假设$X_i$条件独立: <ul>
<li>$x_a\perp x_b|x_c$</li>
<li>$P(X_1,X_2,…,X_n)&#x3D;\prod_{i}{P(X_i|X_{i-1})}$</li>
</ul>
</li>
</ul>
<h4 id="概率补充知识"><a href="#概率补充知识" class="headerlink" title="概率补充知识"></a>概率补充知识</h4><p>指数族分布</p>
<ul>
<li>充分统计量$\phi(x)$</li>
<li>共轭</li>
<li>最大熵</li>
<li>广义线性模型</li>
<li>概率图模型</li>
<li>变分推断</li>
</ul>
<p>充分统计量</p>
<ul>
<li>$P(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))$<ul>
<li>$h(x)$: base measure</li>
<li>$\eta$: parameter 参数向量</li>
<li>$\phi(x)$: feature function</li>
<li>$A(\eta)$： log partition function 配分函数</li>
</ul>
</li>
<li>$P(x|\theta)&#x3D;\frac{1}{z}\hat P(x|\theta)$<ul>
<li>$z&#x3D;\int \hat P(x|\theta)dx$ 归一化因子</li>
</ul>
</li>
<li>$P(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))$<ul>
<li>$&#x3D;\frac{1}{exp(A(\eta))}h(x)exp(\eta^T\phi(x))$</li>
<li>$&#x3D;\frac{1}{z}\hat P(x|\eta)$<ul>
<li>$\hat P(x|\eta)&#x3D;h(x)exp(\eta^T\phi(x))$</li>
<li>$z&#x3D;exp(A(\eta))$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>共轭, 先后验分布同组方便计算</p>
<ul>
<li>$P(\theta|x)&#x3D;\frac{P(x|\theta)P(\theta)}{P(x)}$</li>
<li>$P(\theta|x)$和$P(x|\theta)$属于同一个指数族</li>
<li>$P(\theta|x)$的参数是$P(\theta|x)$的参数的函数</li>
</ul>
<p>先验</p>
<ul>
<li>共轭 - 计算方便</li>
<li>最大熵 无信息先验</li>
<li>Jerrif</li>
</ul>
<p>广义线性模型</p>
<ul>
<li>线性组合 $w^Tx$</li>
<li>Link funciton -&gt;aactivation function</li>
<li>指数分布 $y|x\sim$指数组分布（$Bernulli, Poisson, N(\mu, \Sigma)$）</li>
</ul>
<h5 id="Gaussian"><a href="#Gaussian" class="headerlink" title="Gaussian"></a>Gaussian</h5><ul>
<li>$P(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}|\Sigma|^{1&#x2F;2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$</li>
<li>$\eta&#x3D;<br>\left(!<br>  \begin{array}{c}<br>\eta_1 &#x3D;\frac{\mu}{\sigma^2}\<br>\eta_2&#x3D;-\frac{1}{\sigma^2}<br>  \end{array}<br>  !\right)$</li>
<li>$\phi(x)&#x3D;\left(!<br>  \begin{array}{c}<br>x\<br>x^2<br>  \end{array}<br>  !\right)$</li>
<li>$A(\eta)&#x3D;-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\ln(-\frac{\pi}{\eta_2})$</li>
</ul>
<h4 id="-3"><a href="#-3" class="headerlink" title=""></a></h4><ul>
<li>$P(x | \eta) &#x3D; h(x) \exp (\eta^T \phi(x) - A(\eta))$<ul>
<li>$\eta$: 参数 (parameter)</li>
<li>$\phi(x)$: 充分统计量 (sufficient statistics)</li>
<li>$A(\eta)$: 对数配分函数 (log partition function)</li>
</ul>
</li>
</ul>
<ol>
<li>$A’(\eta) &#x3D; \mathbb{E}_{P(x|\eta)}[\phi(x)]$</li>
<li>$A’’(\eta) &#x3D; \text{Var}[\phi(x)]$</li>
<li>$A(\eta)$ 是凸函数 (convex function)</li>
</ol>
<p>对这个函数求导</p>
<ul>
<li>$\exp(A(\eta)) &#x3D; \int h(x) \exp(\eta^T \phi(x)) , dx$</li>
<li>$A’(\eta) &#x3D; \frac{\partial}{\partial \eta} \log \left( \int h(x) \exp(\eta^T \phi(x)) , dx \right)$</li>
<li>$A’(\eta) &#x3D; \int \frac{h(x) \exp(\eta^T \phi(x)) \phi(x) , dx}{\exp(A(\eta))}$</li>
<li>$A’(\eta) &#x3D; \int P(x | \eta) \phi(x) , dx &#x3D; \mathbb{E}_{P(x|\eta)}[\phi(x)]$</li>
</ul>
<h5 id="MLE"><a href="#MLE" class="headerlink" title="MLE"></a>MLE</h5><p>$D&#x3D;{x_1, \cdots, x_N}$</p>
<p>$\eta_{MLE}&#x3D;\text{argmax  } log(P(D|\eta))$</p>
<ul>
<li>$&#x3D;\sum_{i&#x3D;1}^N\text{argmax  }log\cdot h(x_i)+(\eta^T \phi(x_i) - A(\eta))$</li>
<li><ul>
<li>$&#x3D;\sum_{i&#x3D;1}^N\text{argmax  }\eta^T \phi(x_i) - A(\eta)$</li>
</ul>
</li>
</ul>
<p>set $\frac{\partial \eta_{MLE}}{\partial \eta}&#x3D;0$</p>
<ul>
<li>$\sum \phi(x_i)-NA’(\eta)&#x3D;0$</li>
</ul>
<h5 id="Entorpy-最大熵"><a href="#Entorpy-最大熵" class="headerlink" title="Entorpy 最大熵"></a>Entorpy 最大熵</h5><p>信息熵 $-log\ p$<br>熵，（对可能性的衡量）</p>
<ul>
<li><p>$E_{p(x)}[-log\ p]&#x3D;\int -p(x)log\ p(x)dx&#x3D; \sum -p(x)log\ p(x)$</p>
</li>
<li><p>$\hat p_i&#x3D;\text{argmax } H(p)$ </p>
</li>
<li><p>拉格朗日$\mathcal{L}(p, \lambda)&#x3D;\sum p_ilog\ p_i$</p>
<ul>
<li>$\frac{\partial \mathcal{L}}{\partial p_i}&#x3D;log\ p_i+1-\lambda$</li>
<li>$\hat p_i&#x3D;exp(\lambda-1)&#x3D;1&#x2F;k$</li>
</ul>
</li>
</ul>
<h5 id="经验分布"><a href="#经验分布" class="headerlink" title="经验分布"></a>经验分布</h5><p>$Data &#x3D; {x_1, \cdots, x_N}$</p>
<ul>
<li><strong>频率分布</strong>： $P(x_1, x_2, \ldots, x_n) \approx \hat{P}(x) &#x3D; \frac{\text{count}(x)}{N}$</li>
<li><strong>经验期望</strong>： $\mathbb{E}<em>p[f(x)] &#x3D; \Delta \approx \frac{1}{N} \sum</em>{i&#x3D;1}^N f(x_i)$</li>
</ul>
<p>最大熵问题的求解<br>$$<br>\begin{aligned}<br>\min_{p(x)} &amp; \sum_x p(x) \log p(x) \<br>\text{subject to} &amp; \sum_x p(x) &#x3D; 1 \<br>&amp; \mathbb{E}<em>p[f(x)] &#x3D; \mathbb{E}</em>{\hat{p}}[f(x)] &#x3D; \Delta<br>\end{aligned}<br>$$</p>
<p>拉格朗日乘数法求解<br>$$<br>\begin{aligned}<br>L(p, \lambda, \eta) &amp;&#x3D; \sum_x p(x) \log p(x) + \lambda (1 - \sum_x p(x)) + \eta (\Delta - \sum_x p(x) f(x)) \<br>\frac{\partial}{\partial p(x)} &amp;&#x3D; \log p(x) + 1 - \lambda_0 - \lambda f(x) &#x3D; 0 \<br>p(x) &amp;&#x3D; \exp(\lambda_1 f(x) + \lambda_0 - 1) \<br>&amp;&#x3D; \frac{\exp(\eta^T f(x))}{Z(\eta)}<br>\end{aligned}<br>$$</p>
<h3 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h3><p>有向无环图<br>因子分解</p>
<ul>
<li>$P(X_1,X_2,…,X_n)&#x3D;\prod_{i}{P(X_i|Pa(X_i))}$</li>
<li>$P(X_i|Pa(X_i))$是局部概率分布</li>
<li>$Pa(X_i)$是$X_i$的父节点集合</li>
</ul>
<h4 id="三种模型"><a href="#三种模型" class="headerlink" title="三种模型"></a>三种模型</h4><h5 id="tail-to-tail"><a href="#tail-to-tail" class="headerlink" title="tail-to-tail"></a>tail-to-tail</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a--&gt; b</span><br><span class="line">  a--&gt; c</span><br></pre></td></tr></table></figure>
<p>因子分解：</p>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c|a)$</li>
</ul>
<p>链式法则：</p>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c|a, b)$</li>
</ul>
<p>$\implies P(c|a)&#x3D;P(c|a, b)\implies c\perp b |a$<br>若$b$ 被观测则路径被阻塞： </p>
<h5 id="head-to-tail"><a href="#head-to-tail" class="headerlink" title="head-to-tail"></a>head-to-tail</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a --&gt;b </span><br><span class="line">  b --&gt;c </span><br></pre></td></tr></table></figure>
<p>$a\perp c |b$<br>若$b$ 被观测则路径被阻塞： </p>
<h5 id="head-to-head"><a href="#head-to-head" class="headerlink" title="head-to-head"></a>head-to-head</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  b--&gt; c</span><br><span class="line">  a--&gt; c</span><br></pre></td></tr></table></figure>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b)P(c|a, b)$</li>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c | a, b)$</li>
</ul>
<p>$\implies P(b)&#x3D;P(b|a)\implies a\perp b$<br>若$b$ 被观测则路径被连通：</p>
<h4 id="D-seperation"><a href="#D-seperation" class="headerlink" title="D-seperation"></a>D-seperation</h4>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a  --&gt; b1</span><br><span class="line">  a  --&gt; b2</span><br><span class="line">  a  --&gt; b*</span><br><span class="line">  b1 --&gt; c</span><br><span class="line">  b2 --&gt; c</span><br><span class="line">  c  --&gt; b*</span><br></pre></td></tr></table></figure>
<ul>
<li>如果$b1, b2\in B$被观测了<br>，那么$a$和$c$被阻断，</li>
<li>但是$b*$没有被观测到,且$b*$的后续节点都不在$b$中<br>$a\perp c|b$</li>
</ul>
<p><img src="/whatever/2024-05-29-Casual-Inference/image-2.png" alt="alt text"><br>马尔可夫毯(Markov Blanket)</p>
<ul>
<li>$x_{pa(i)}$：$x_i$的父节点</li>
<li>$x_{child(i)}$: childs of $x_i$</li>
<li>$x_{pa(child(i))}$: parent of $x_{child(i)}$</li>
<li>$x_{-i}&#x3D;x&#x2F;x_i$ 表示除了 $x_i$ 以外的所有变量。<ul>
<li>和$x$有关:$\Delta$<ul>
<li>$P(x_i|x_{Pa(i)})&#x3D;f(\bar \Delta)$</li>
</ul>
</li>
<li>和$x$无关:$\bar \Delta$</li>
</ul>
</li>
</ul>
<p>马尔可夫毯的作用是在给定马尔可夫毯内所有节点的情况下，$x_i$ 与网络中其他节点条件独立。</p>
<p>$$<br>P(x_i | x_{-i}) &#x3D; \frac{P(x_i, x_{-i})}{P(x_{-i})} &#x3D; \frac{P(x)}{\int_{x_i} P(x_{-i})} &#x3D; \frac{ \int_{x_i} P(x) , dx_i }{ \int_{x_i} P(x_j | x_{\text{pa}(j)}) , dx_i }<br>$$</p>
<p>$$<br>P(x_{\text{child}(i)} | x_i, x_{\text{Parent}(\text{Child}(i))})<br>$$</p>
<h4 id="贝叶斯网络-Beyesian-Network"><a href="#贝叶斯网络-Beyesian-Network" class="headerlink" title="贝叶斯网络 Beyesian Network"></a>贝叶斯网络 Beyesian Network</h4><p>NB<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  y--&gt; x1 </span><br><span class="line">  y--&gt; xp</span><br></pre></td></tr></table></figure></p>
<p>GMM<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  z--&gt; x</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>Beyesian Network</p>
<ul>
<li>单一： Naive Bayes<ul>
<li>p维： $P(x|y)&#x3D;\prod^p_{i&#x3D;1}P(x_i|y&#x3D;1)$</li>
<li>$x_1\perp x_2|y$</li>
</ul>
</li>
<li>混合：GMM<ul>
<li>Z discrete, z&#x3D;1,2,3,4</li>
</ul>
</li>
<li>时间<ul>
<li>Markov Chain</li>
<li>Gaussian Process(无限维分布)</li>
</ul>
</li>
<li>连续： Gaussian Network</li>
</ul>
<p>动态模型</p>
<ul>
<li>HMM 离散</li>
<li>LDS Kalman Filter 连续线性</li>
<li>Particle filter 非线性非高斯</li>
</ul>
<h3 id="Markov-Network"><a href="#Markov-Network" class="headerlink" title="Markov Network"></a>Markov Network</h3><p>条件独立性</p>
<ul>
<li>全局 Global Markov Property<ul>
<li>$X_A \perp X_C \mid X_B$</li>
<li>如果集合 $X_A$ 和 $X_C$ 被集合 $X_B$ 分隔开，那么 $X_A$ 和 $X_C$ 是条件独立的</li>
</ul>
</li>
<li>局部 Local Markov Property<ul>
<li>$a \perp {Non-Neighbour} \mid {Neighbour}$</li>
<li>{ }：集合</li>
</ul>
</li>
<li>成对 Pairwise Markov Property<ul>
<li>$x_i \perp x_j \mid x_{-ij}$</li>
<li>如果节点 $x_i$ 和 $x_j$ 直接相连，那么在给定其他所有节点的情况下，$x_i$ 和 $x_j$ 是条件独立的</li>
</ul>
</li>
</ul>
<h4 id="Factorization"><a href="#Factorization" class="headerlink" title="Factorization"></a>Factorization</h4><p>团: Clique<br>最大团: Maximal Clique<br>$$<br>P(X) &#x3D; \frac{1}{Z} \prod_{i&#x3D;1}^K \psi(X_{C_i})<br>$$</p>
<ul>
<li>$c_i$ 最大团</li>
<li>$x_{c_i}$: 最大团随机变量集合</li>
<li>$\psi(x_{c_i})$: 势函数</li>
<li>$Z$:<ul>
<li>$Z &#x3D; \sum_{X} \prod_{i&#x3D;1}^K \psi(X_{C_i})$</li>
</ul>
</li>
<li>$\psi(X_{C_i})$ 是定义在最大团 $C_i$ 上的因子函数</li>
</ul>
<p>何弃疗<br><img src="/whatever/2024-05-29-Casual-Inference/image-3.png" alt="alt text"></p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li><p>联合概率 Joint Probability</p>
<ul>
<li>$P(X) &#x3D; P(x_1, x_2, \ldots, x_p)$</li>
</ul>
</li>
<li><p>边缘概率 Marginal Probability</p>
<ul>
<li>$P(x) &#x3D; \sum_{x_j} P(x_j)$</li>
</ul>
</li>
<li><p>条件概率 Conditional Probability</p>
<ul>
<li>$P(x_i | x_j)$, 其中$x_j &#x3D; {x \backslash x_i}$</li>
</ul>
</li>
<li><p>最大后验概率估计 MAP Inference</p>
<ul>
<li>$\hat{z} &#x3D; \arg \max_z P(z | x) \propto \arg \max_z P(z, x)$</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>精确推断 Exact Inference</p>
<ul>
<li>Variable Elimination (VE)</li>
<li>Belief Propagation (BP) → Sum-Product Algorithm (求和-乘积算法)</li>
<li>Junction Tree Algorithm (树形算法)</li>
</ul>
</li>
<li><p>近似推断 Approximate Inference</p>
<ul>
<li>Loop Belief Propagation (循环信念传播)</li>
<li>Monte Carlo Inference: Importance Sampling, MCMC (蒙特卡罗推断：重要性采样，MCMC)</li>
<li>Variational Inference (变分推断)</li>
</ul>
</li>
</ul>
<h4 id="Variable-Elimination"><a href="#Variable-Elimination" class="headerlink" title="Variable Elimination"></a>Variable Elimination</h4><p>$P(x) &#x3D; \prod_{i} \phi_i(x_i)$</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>假设我们有四个二值随机变量 $a, b, c, d\in {0,1}$。我们想计算边缘概率 $P(d)$。<br>$a\rightarrow b \rightarrow c \rightarrow d$</p>
<ol>
<li><p>展开联合分布：</p>
<ul>
<li>$P(d) &#x3D; \sum_{a, b, c} P(a, b, c, d)$</li>
</ul>
</li>
<li><p>Chain rule</p>
<ul>
<li>$P(a, b, c, d) &#x3D; P(a) P(b | a) P(c | b) P(d | c)$</li>
</ul>
</li>
<li><p>1-&gt;2</p>
<ul>
<li>$P(d) &#x3D; \sum_{a, b, c} P(a) P(b | a) P(c | b) P(d | c)$<ul>
<li>$&#x3D; \sum_{b, c} P(d | c) \left( \sum_{a} P(a) P(b | a) \right) P(c | b)$</li>
</ul>
</li>
</ul>
</li>
<li><p>定义新的因子函数：</p>
<ul>
<li>$\phi_1(b, c) &#x3D; \sum_{a} P(a) P(b | a)$</li>
</ul>
</li>
<li><p>最终得到：</p>
<ul>
<li>$P(d) &#x3D; \sum_{c} P(d | c) \left( \sum_{b} \phi_1(b, c) P(c | b) \right)$</li>
</ul>
</li>
<li><p>得到另一个因子函数 $\phi_2(c, d)$：</p>
<ul>
<li>$P(d) &#x3D; \sum_{c} \phi_2(c, d)$</li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://tlhey.github.io/whatever">Tlhey</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://tlhey.github.io/whatever/2024/05/29/2024-05-29-Casual-Inference/">https://tlhey.github.io/whatever/2024/05/29/2024-05-29-Casual-Inference/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/whatever/img/bc12.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/whatever/2024/05/19/2024-05-19-CS224W-notes/" title="CS224W_notes"><img class="cover" src="/whatever/img/bc10.jpeg" onerror="onerror=null;src='/whatever/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">CS224W_notes</div></div><div class="info-2"><div class="info-item-1">1 Introduction，Machine learning for graphs大纲大纲  Traditional methods: Graphlets, Graph Kernels Methods for node embeddings: DeepWalk, Node2Vec Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs Knowledge graphs and reasoning: TransE, BetaE Deep generative models for graphs Applications to Biomedicine, Science, Industry  Defs $G&#x3D;(V, E, F)$ or $G(V, E)$ Directed&#x2F; undirected DegreeDirected $\bar{k} &#x3D; \langle k \rangle &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} k_i &#x3D;...</div></div></div></a><a class="pagination-related" href="/whatever/2024/06/03/2024-06-03-Cytoid-AI-Charting/" title="Cytoid AI Charting"><img class="cover" src="/whatever/img/bc9.jpeg" onerror="onerror=null;src='/whatever/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Cytoid AI Charting</div></div><div class="info-2"><div class="info-item-1"> 相关论文实现  Survey 1: https://www.qbitai.com/2022/03/33133.html 1.1 现有技术(1)：100k songs, 44GB datahttps://github.com/chrisdonahue/ddchttps://arxiv.org/pdf/1703.06891.pdf 1.2 GeneLive在DDC基础上improve：现有技术2：GenéLive! Generating Rhythm Actions in Love Live! | Proceedings of the AAAI Conference on Artificial Intelligencehttps://arxiv.org/abs/2202.12823https://github.com/chrisdonahue/ddc 1.3 现有技术3：MuG...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/whatever/img/Avatar.jpg" onerror="this.onerror=null;this.src='/whatever/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Tlhey</div><div class="author-info-description"></div><div class="site-data"><a href="/whatever/archives/"><div class="headline">Articles</div><div class="length-num">35</div></a><a href="/whatever/tags/"><div class="headline">Tags</div><div class="length-num">2</div></a><a href="/whatever/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%89%E5%85%B3%E7%9A%84"><span class="toc-number">1.</span> <span class="toc-text">与贝叶斯有关的</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text"></span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#chap1"><span class="toc-number">1.</span> <span class="toc-text">chap1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HMM"><span class="toc-number">2.</span> <span class="toc-text">HMM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#https-www-yuque-com-bystander-wg876-yc5f72-dvgo5b%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E4%BB%8E%E9%A2%91%E7%8E%87%E6%B4%BE%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE%E9%A2%91%E7%8E%87%E6%B4%BE%E7%9A%84%E6%96%B9%E6%B3%95%E4%B8%AD%E7%9A%84%E6%A0%B8%E5%BF%83%E6%98%AF%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%8C%E8%80%8C%E5%9C%A8%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE%E7%9A%84%E6%96%B9%E6%B3%95%E4%B8%AD%EF%BC%8C%E6%A0%B8%E5%BF%83%E6%98%AF%E7%A7%AF%E5%88%86%E9%97%AE%E9%A2%98%EF%BC%8C%E4%B9%9F%E5%8F%91%E5%B1%95%E5%87%BA%E6%9D%A5%E4%BA%86%E4%B8%80%E7%B3%BB%E5%88%97%E7%9A%84%E7%A7%AF%E5%88%86%E6%96%B9%E6%B3%95%E5%A6%82%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%EF%BC%8CMCMC-%E7%AD%89-Def-lambda-pi-A-B-pi-is-the-initial-state-distribution-o-t-%E6%9D%A5%E8%A1%A8%E7%A4%BA%E8%A7%82%E6%B5%8B%E5%8F%98%E9%87%8F%EF%BC%8C-O-%E4%B8%BA%E8%A7%82%E6%B5%8B%E5%BA%8F%E5%88%97%EF%BC%8C-V-v-1-v-2-cdots-v-M-%E8%A1%A8%E7%A4%BA%E8%A7%82%E6%B5%8B%E7%9A%84%E5%80%BC%E5%9F%9F-i-t-%E8%A1%A8%E7%A4%BA%E7%8A%B6%E6%80%81%E5%8F%98%E9%87%8F%EF%BC%8C-I-%E4%B8%BA%E7%8A%B6%E6%80%81%E5%BA%8F%E5%88%97%EF%BC%8C-Q-q-1-q-2-cdots-q-N-%E8%A1%A8%E7%A4%BA%E7%8A%B6%E6%80%81%E5%8F%98%E9%87%8F%E7%9A%84%E5%80%BC%E5%9F%9F-A-a-ij-p-i-t-1-q-j-i-t-q-i-%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BB%E7%9F%A9%E9%98%B5-B-b-j-k-p-o-t-v-k-i-t-q-j-%E5%8F%91%E5%B0%84%E7%9F%A9%E9%98%B5"><span class="toc-number">3.</span> <span class="toc-text">https:&#x2F;&#x2F;www.yuque.com&#x2F;bystander-wg876&#x2F;yc5f72&#x2F;dvgo5b机器学习模型可以从频率派和贝叶斯派频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC 等### Def$\lambda&#x3D;(\pi,A,B)$- $\pi$ is the initial state distribution- $o_t$ 来表示观测变量，$O$ 为观测序列，$V&#x3D;{v_1,v_2,\cdots,v_M}$ 表示观测的值域- $i_t$ 表示状态变量，$I$ 为状态序列，$Q&#x3D;{q_1,q_2,\cdots,q_N}$ 表示状态变量的值域- $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$状态转移矩阵- $B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ 发射矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE"><span class="toc-number">3.0.1.</span> <span class="toc-text">两个基本假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98"><span class="toc-number">3.0.2.</span> <span class="toc-text">三个基本问题</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#-1"><span class="toc-number">4.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#-2"><span class="toc-number">5.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%89%E5%85%B3MCMC"><span class="toc-number">6.</span> <span class="toc-text">有关MCMC</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E6%AF%94%E8%BE%83%E5%9F%BA%E7%A1%80%E7%9A%84%E4%BB%8B%E7%BB%8D%EF%BC%9A-https-zhuanlan-zhihu-com-p-420214359-Abstract-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD%E4%BC%B0%E8%AE%A1%E5%8F%82%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95%E6%98%AF%EF%BC%9A%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%AE%97%E5%87%BA%E5%8F%82%E6%95%B0-Theta-%E7%9A%84%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0-P-Theta-%EF%BC%8C%E6%88%91%E4%BB%AC%E7%94%A8%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83%E7%9A%84%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B%E4%BD%9C%E4%B8%BA%E5%AF%B9%E5%8F%82%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1%E5%80%BC-MCMC%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%EF%BC%9A%E5%8F%AF%E4%BB%A5%E5%B8%AE%E6%88%91%E4%BB%AC%E4%BB%8E%E4%BB%BB%E6%84%8F%EF%BC%88%E6%97%A0%E8%AE%BA%E6%9C%89%E6%B2%A1%E6%9C%89%E8%A7%A3%E6%9E%90%E5%BD%A2%E5%BC%8F%E7%9A%84%EF%BC%89%E5%88%86%E5%B8%83%E4%B8%8A%E6%8A%BD%E6%A0%B7%E4%B8%80%E6%89%B9%E6%95%B0%E6%8D%AE%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E8%BF%99%E5%A0%86%E6%8A%BD%E6%A0%B7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%9D%87%E5%80%BC%E4%BD%9C%E4%B8%BA%E5%AF%B9%E8%BF%99%E4%B8%AA%E5%88%86%E5%B8%83%E6%9C%9F%E6%9C%9B%E7%9A%84%E4%BC%B0%E8%AE%A1-%E6%88%91%E4%BB%AC%E7%94%A8MCMC%E8%BF%99%E7%A7%8D%E6%B1%82%E6%9C%9F%E6%9C%9B%E7%9A%84%E6%96%B9%E6%B3%95%E6%B1%82%E5%8F%82%E6%95%B0%E5%88%86%E5%B8%83%E6%9C%9F%E6%9C%9B%E7%9A%84%E4%BC%B0%E8%AE%A1%E5%80%BC%EF%BC%8C%E4%BB%A5%E6%AD%A4%E6%B1%82%E5%87%BA%E5%8F%82%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1%E5%80%BC"><span class="toc-number">7.</span> <span class="toc-text">一个比较基础的介绍： https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;420214359- Abstract:  - 贝叶斯推断估计参数的方法是：我们可以算出参数$\Theta$的分布函数$P(\Theta)$，我们用参数分布的数学期望作为对参数的估计值  - MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计  - 我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Monte-Carlo-Sampling"><span class="toc-number">7.1.</span> <span class="toc-text">1 Monte Carlo Sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Bayes-MCMC"><span class="toc-number">7.2.</span> <span class="toc-text">2 Bayes &amp; MCMC</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Bayes-Model-%E5%8F%82%E6%95%B0-Theta-%EF%BC%8CObserved-data-D"><span class="toc-number">7.2.1.</span> <span class="toc-text">2.1 Bayes Model: 参数$\Theta$ ，Observed data: $D$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E9%80%9A%E8%BF%87%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87-P-Theta-D-%E8%8E%B7%E5%8F%96%E5%8F%82%E6%95%B0-Theta-%E7%9A%84%E4%BC%B0%E8%AE%A1%E5%80%BC"><span class="toc-number">7.2.2.</span> <span class="toc-text">2.2 通过后验概率$P(\Theta|D)$获取参数$\Theta$的估计值</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Sampling-%E9%87%87%E6%A0%B7"><span class="toc-number">7.3.</span> <span class="toc-text">3 Sampling 采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Chain"><span class="toc-number">7.4.</span> <span class="toc-text">Markov Chain</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Def"><span class="toc-number">7.4.1.</span> <span class="toc-text">Def</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%B4%A8"><span class="toc-number">7.4.2.</span> <span class="toc-text">性质</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MCMC%E5%85%B7%E4%BD%93%E7%BB%86%E8%8A%82"><span class="toc-number">7.5.</span> <span class="toc-text">MCMC具体细节</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conditional-Random-Field-CRF"><span class="toc-number">8.</span> <span class="toc-text">Conditional Random Field(CRF)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.</span> <span class="toc-text">概率图模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E5%92%8C%E5%9B%BE"><span class="toc-number">9.1.</span> <span class="toc-text">概率和图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86"><span class="toc-number">9.1.1.</span> <span class="toc-text">概率补充知识</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Gaussian"><span class="toc-number">9.1.1.1.</span> <span class="toc-text">Gaussian</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#-3"><span class="toc-number">9.1.2.</span> <span class="toc-text"></span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MLE"><span class="toc-number">9.1.2.1.</span> <span class="toc-text">MLE</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Entorpy-%E6%9C%80%E5%A4%A7%E7%86%B5"><span class="toc-number">9.1.2.2.</span> <span class="toc-text">Entorpy 最大熵</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83"><span class="toc-number">9.1.2.3.</span> <span class="toc-text">经验分布</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">9.2.</span> <span class="toc-text">贝叶斯网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.2.1.</span> <span class="toc-text">三种模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#tail-to-tail"><span class="toc-number">9.2.1.1.</span> <span class="toc-text">tail-to-tail</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#head-to-tail"><span class="toc-number">9.2.1.2.</span> <span class="toc-text">head-to-tail</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#head-to-head"><span class="toc-number">9.2.1.3.</span> <span class="toc-text">head-to-head</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#D-seperation"><span class="toc-number">9.2.2.</span> <span class="toc-text">D-seperation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C-Beyesian-Network"><span class="toc-number">9.2.3.</span> <span class="toc-text">贝叶斯网络 Beyesian Network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Network"><span class="toc-number">9.3.</span> <span class="toc-text">Markov Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Factorization"><span class="toc-number">9.3.1.</span> <span class="toc-text">Factorization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference"><span class="toc-number">9.4.</span> <span class="toc-text">Inference</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Variable-Elimination"><span class="toc-number">9.4.1.</span> <span class="toc-text">Variable Elimination</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">9.4.2.</span> <span class="toc-text">示例</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/whatever/2025/04/10/2025-04-10-Movies/" title="2025-04-10-Movies"><img src="/whatever/img/bc9.jpeg" onerror="this.onerror=null;this.src='/whatever/img/404.jpg'" alt="2025-04-10-Movies"/></a><div class="content"><a class="title" href="/whatever/2025/04/10/2025-04-10-Movies/" title="2025-04-10-Movies">2025-04-10-Movies</a><time datetime="2025-04-10T18:22:44.000Z" title="Created 2025-04-10 14:22:44">2025-04-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/whatever/2025/04/01/2025-04-01-Linux-server-proxy-issue/" title="2025-04-01-Linux-server-proxy-issue"><img src="/whatever/img/bc12.jpeg" onerror="this.onerror=null;this.src='/whatever/img/404.jpg'" alt="2025-04-01-Linux-server-proxy-issue"/></a><div class="content"><a class="title" href="/whatever/2025/04/01/2025-04-01-Linux-server-proxy-issue/" title="2025-04-01-Linux-server-proxy-issue">2025-04-01-Linux-server-proxy-issue</a><time datetime="2025-04-01T19:23:11.000Z" title="Created 2025-04-01 15:23:11">2025-04-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/whatever/2025/04/01/2025-04-01-mt-implementation-log/" title="2025-04-01-mt-implementation-log"><img src="/whatever/img/bc10.jpeg" onerror="this.onerror=null;this.src='/whatever/img/404.jpg'" alt="2025-04-01-mt-implementation-log"/></a><div class="content"><a class="title" href="/whatever/2025/04/01/2025-04-01-mt-implementation-log/" title="2025-04-01-mt-implementation-log">2025-04-01-mt-implementation-log</a><time datetime="2025-04-01T16:58:02.000Z" title="Created 2025-04-01 12:58:02">2025-04-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/whatever/2025/03/09/2025-03-08-medication/" title="2025-03-08 medication"><img src="/whatever/img/bc9.jpeg" onerror="this.onerror=null;this.src='/whatever/img/404.jpg'" alt="2025-03-08 medication"/></a><div class="content"><a class="title" href="/whatever/2025/03/09/2025-03-08-medication/" title="2025-03-08 medication">2025-03-08 medication</a><time datetime="2025-03-09T10:58:05.000Z" title="Created 2025-03-09 06:58:05">2025-03-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/whatever/2025/02/15/2025-02-15-DS/" title="2025-02-15-DS"><img src="/whatever/img/bc12.jpeg" onerror="this.onerror=null;this.src='/whatever/img/404.jpg'" alt="2025-02-15-DS"/></a><div class="content"><a class="title" href="/whatever/2025/02/15/2025-02-15-DS/" title="2025-02-15-DS">2025-02-15-DS</a><time datetime="2025-02-15T18:17:25.000Z" title="Created 2025-02-15 13:17:25">2025-02-15</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/bc12.jpeg);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Tlhey</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/whatever/js/utils.js"></script><script src="/whatever/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>