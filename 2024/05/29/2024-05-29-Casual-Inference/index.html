<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Casual Inference | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="与贝叶斯有关的频率派的观点为 $p(X|\theta)\mathop{&#x3D;}\limits_{iid}\prod\limits {i&#x3D;1}^{N}p(x{i}|\theta)$  为了求常量 $\theta$ 的大小,最大对数似然MLE的方法：$\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\m">
<meta property="og:type" content="article">
<meta property="og:title" content="Casual Inference">
<meta property="og:url" content="http://example.com/2024/05/29/2024-05-29-Casual-Inference/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="与贝叶斯有关的频率派的观点为 $p(X|\theta)\mathop{&#x3D;}\limits_{iid}\prod\limits {i&#x3D;1}^{N}p(x{i}|\theta)$  为了求常量 $\theta$ 的大小,最大对数似然MLE的方法：$\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\m">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024-05-29-Casual-Inference/image.png">
<meta property="og:image" content="http://example.com/2024-05-29-Casual-Inference/image-1.png">
<meta property="og:image" content="http://example.com/2024-05-29-Casual-Inference/image-4.png">
<meta property="og:image" content="http://example.com/2024-05-29-Casual-Inference/image-2.png">
<meta property="og:image" content="http://example.com/2024-05-29-Casual-Inference/image-3.png">
<meta property="article:published_time" content="2024-05-29T17:53:31.000Z">
<meta property="article:modified_time" content="2024-12-15T11:14:33.000Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024-05-29-Casual-Inference/image.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2024-05-29-Casual-Inference" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/29/2024-05-29-Casual-Inference/" class="article-date">
  <time class="dt-published" datetime="2024-05-29T17:53:31.000Z" itemprop="datePublished">2024-05-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Casual Inference
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="与贝叶斯有关的"><a href="#与贝叶斯有关的" class="headerlink" title="与贝叶斯有关的"></a>与贝叶斯有关的</h2><p>频率派的观点<br>为 $p(X|\theta)\mathop{&#x3D;}\limits_{iid}\prod\limits <em>{i&#x3D;1}^{N}p(x</em>{i}|\theta)$ </p>
<p>为了求常量 $\theta$ 的大小,最大对数似然MLE的方法：<br>$\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{&#x3D;}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)$</p>
<p>贝叶斯派的观点<br>$p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的先验的分布 $\theta\sim p(\theta)$</p>
<p>$p(\theta|X)&#x3D;\frac{p(X|\theta)\cdot p(\theta)}{p(X)}&#x3D;\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}$</p>
<p>为了求 $\theta$ 的值，我们要最大化这个参数后验MAP：</p>
<p>$\theta_{MAP}&#x3D;\mathop{argmax}\limits _{\theta}p(\theta|X)&#x3D;\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)$</p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p><img src="/2024-05-29-Casual-Inference/image.png" alt="alt text"><br><img src="/2024-05-29-Casual-Inference/image-1.png" alt="alt text"></p>
<p>基础知识和概念，包括d-分离<br>do算子<br>后门调整<br>前门调整<br>逆概率加权<br>反事实<br>因果关系发现中最基本的两类方法：基于独立性测试的方法，以及通过加性噪声模型的形式分析残差与预测者独立性关系的方法</p>
<h2 id="chap1"><a href="#chap1" class="headerlink" title="chap1"></a>chap1</h2><p>partition, law of total probability<br>summing up its probabilities over all Bi is called marginalizing over $B$, and the resulting probability P(A) is called the marginal probability of $A$.<br>$P(A)&#x3D;P(A,B_1)+P(A,B_2)+···+P(A,B_n)$</p>
<p>Def conditional probabilities<br>$P(A|B)&#x3D;P(A,B)∕P(B)$</p>
<p>independence, giving no additional information<br>$P(A,B)&#x3D;P(A)P(B)$</p>
<p>$P(A|B)&#x3D;\frac{P(B|A)P(A)}{P(B)}$</p>
<p>$P(A)&#x3D;P(A|B_1)P(B_1)+P(A|B_2)P(B-2)+···+P(A|B_k)P(B_k)$</p>
<p>Sructual Casual Models SCM<br>U exogenous variables, external to the model;</p>
<h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><h2 id="https-www-yuque-com-bystander-wg876-yc5f72-dvgo5b机器学习模型可以从频率派和贝叶斯派频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC-等-Def-lambda-pi-A-B-pi-is-the-initial-state-distribution-o-t-来表示观测变量，-O-为观测序列，-V-v-1-v-2-cdots-v-M-表示观测的值域-i-t-表示状态变量，-I-为状态序列，-Q-q-1-q-2-cdots-q-N-表示状态变量的值域-A-a-ij-p-i-t-1-q-j-i-t-q-i-状态转移矩阵-B-b-j-k-p-o-t-v-k-i-t-q-j-发射矩阵"><a href="#https-www-yuque-com-bystander-wg876-yc5f72-dvgo5b机器学习模型可以从频率派和贝叶斯派频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC-等-Def-lambda-pi-A-B-pi-is-the-initial-state-distribution-o-t-来表示观测变量，-O-为观测序列，-V-v-1-v-2-cdots-v-M-表示观测的值域-i-t-表示状态变量，-I-为状态序列，-Q-q-1-q-2-cdots-q-N-表示状态变量的值域-A-a-ij-p-i-t-1-q-j-i-t-q-i-状态转移矩阵-B-b-j-k-p-o-t-v-k-i-t-q-j-发射矩阵" class="headerlink" title="https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b机器学习模型可以从频率派和贝叶斯派频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC 等### Def$\lambda&#x3D;(\pi,A,B)$- $\pi$ is the initial state distribution- $o_t$ 来表示观测变量，$O$ 为观测序列，$V&#x3D;{v_1,v_2,\cdots,v_M}$ 表示观测的值域- $i_t$ 表示状态变量，$I$ 为状态序列，$Q&#x3D;{q_1,q_2,\cdots,q_N}$ 表示状态变量的值域- $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$状态转移矩阵- $B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ 发射矩阵"></a><a target="_blank" rel="noopener" href="https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b">https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b</a><br>机器学习模型可以从频率派和贝叶斯派<br>频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC 等<br>### Def<br>$\lambda&#x3D;(\pi,A,B)$<br><br>- $\pi$ is the initial state distribution<br>- $o_t$ 来表示观测变量，$O$ 为观测序列，$V&#x3D;{v_1,v_2,\cdots,v_M}$ 表示观测的值域<br>- $i_t$ 表示状态变量，$I$ 为状态序列，$Q&#x3D;{q_1,q_2,\cdots,q_N}$ 表示状态变量的值域<br>- $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$状态转移矩阵<br>- $B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ 发射矩阵</h2><h4 id="两个基本假设"><a href="#两个基本假设" class="headerlink" title="两个基本假设"></a>两个基本假设</h4><ol>
<li>齐次 Markov 假设（未来只依赖于当前）：<br>$p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)&#x3D;p(i_{t+1}|i_t)$</li>
<li>观测独立假设：<br>$p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)&#x3D;p(o_t|i_t)$</li>
</ol>
<h4 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h4><ol>
<li>Evaluation：$p(O|\lambda)$，Forward-Backward </li>
<li>Learning：$\lambda&#x3D;\mathop{argmax}\limits_{\lambda}p(O|\lambda)$，EM （Baum-Welch）</li>
<li>Decoding：$I&#x3D;\mathop{argmax}\limits_{I}p(I|O,\lambda)$，Vierbi 算法<br>  a. 预测问题：$p(i_{t+1}|o_1,o_2,\cdots,o_t)$<br>  b. 滤波问题：$p(i_t|o_1,o_2,\cdots,o_t)$</li>
</ol>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h2 id="-2"><a href="#-2" class="headerlink" title=""></a></h2><h2 id="有关MCMC"><a href="#有关MCMC" class="headerlink" title="有关MCMC"></a>有关MCMC</h2><h2 id="一个比较基础的介绍：-https-zhuanlan-zhihu-com-p-420214359-Abstract-贝叶斯推断估计参数的方法是：我们可以算出参数-Theta-的分布函数-P-Theta-，我们用参数分布的数学期望作为对参数的估计值-MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计-我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值"><a href="#一个比较基础的介绍：-https-zhuanlan-zhihu-com-p-420214359-Abstract-贝叶斯推断估计参数的方法是：我们可以算出参数-Theta-的分布函数-P-Theta-，我们用参数分布的数学期望作为对参数的估计值-MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计-我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值" class="headerlink" title="一个比较基础的介绍： https://zhuanlan.zhihu.com/p/420214359- Abstract:  - 贝叶斯推断估计参数的方法是：我们可以算出参数$\Theta$的分布函数$P(\Theta)$，我们用参数分布的数学期望作为对参数的估计值  - MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计  - 我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值"></a>一个比较基础的介绍： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420214359">https://zhuanlan.zhihu.com/p/420214359</a><br>- Abstract:<br>  - 贝叶斯推断估计参数的方法是：我们可以算出参数$\Theta$的分布函数$P(\Theta)$，我们用参数分布的数学期望作为对参数的估计值<br>  - MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计<br>  - 我们用MCMC这种求期望的方法求参数分布期望的估计值，以此求出参数的估计值</h2><h3 id="1-Monte-Carlo-Sampling"><a href="#1-Monte-Carlo-Sampling" class="headerlink" title="1 Monte Carlo Sampling"></a>1 Monte Carlo Sampling</h3><p>如果$X$服从$f(x)$这个概率分布，我怎么获得$E(X)$<br>最常见的一种Monte Carlo方法的使用场景就是：对随机变量进行充分多的采样后，使用这些采样的均值来估计总体的期望</p>
<p>对于随机变量$X$，它的概率密度函数为$p(x)$，因此它的数学期望为<br>$E(x)&#x3D;\int_{-\infty}^{+\infty}xp(x)dx$<br>我们对于这个随机变量随机采样得到$n$个采样值$x_i$，根据大数定理，有<br>$\lim_{n\rightarrow+\infty}{\frac1n\sum_i^n{x_i}}&#x3D;E(X)$</p>
<h3 id="2-Bayes-MCMC"><a href="#2-Bayes-MCMC" class="headerlink" title="2 Bayes &amp; MCMC"></a>2 Bayes &amp; MCMC</h3><h4 id="2-1-Bayes-Model-参数-Theta-，Observed-data-D"><a href="#2-1-Bayes-Model-参数-Theta-，Observed-data-D" class="headerlink" title="2.1 Bayes Model: 参数$\Theta$ ，Observed data: $D$"></a>2.1 Bayes Model: 参数$\Theta$ ，Observed data: $D$</h4><p>贝叶斯公式：$P(\Theta|D)&#x3D;\frac1{P(D)}P(D|\Theta)P(\Theta)$</p>
<p>由于$P(D)$是一个无关紧要的常数，因此上式往往直接写成一个正比关系式：<br>$P(\Theta|D)\propto P(D|\Theta)P(\Theta)$</p>
<p>在贝叶斯推断里：</p>
<ol>
<li>通过$P(\Theta|D)$来得到$\Theta$的估计值</li>
<li>模型给出$P(D|\Theta)$， 即likelihood$P(D|\Theta)$</li>
<li>还可以通过$P(\Theta)$来对参数的分布情况做一些先验的猜测。（如果你什么都不知道，$P(\Theta)$自然可以猜一个均匀分布）</li>
</ol>
<h4 id="2-2-通过后验概率-P-Theta-D-获取参数-Theta-的估计值"><a href="#2-2-通过后验概率-P-Theta-D-获取参数-Theta-的估计值" class="headerlink" title="2.2 通过后验概率$P(\Theta|D)$获取参数$\Theta$的估计值"></a>2.2 通过后验概率$P(\Theta|D)$获取参数$\Theta$的估计值</h4><p>想法：众数或者期望作为<br>估计值</p>
<ol>
<li>众数：$\hat\Theta&#x3D;\arg\max_\Theta{P(\Theta|D)}$</li>
<li>期望：$\hat\Theta&#x3D;\int_\Theta\Theta P(\Theta|D)d\Theta$</li>
</ol>
<p>MCMC就是教我们怎么在一个没有解析形式的数据上「抽样几个数据算平均值」的方法</p>
<h3 id="3-Sampling-采样"><a href="#3-Sampling-采样" class="headerlink" title="3 Sampling 采样"></a>3 Sampling 采样</h3><ol>
<li>Uniform</li>
<li>Gaussian:<br>Given $U_1,U_2$<br>  $$<br>   Z_0 &#x3D; \sqrt{-2 \ln U_1} \cos(2\pi U_2)<br>   $$<br>   $$<br>   Z_1 &#x3D; \sqrt{-2 \ln U_1} \sin(2\pi U_2)<br>   $$</li>
<li>Reject-Accept: 用于对很不规则的$f(x)$采样。具体细节没看</li>
<li></li>
</ol>
<h3 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov Chain"></a>Markov Chain</h3><p>一个对马尔可夫状态讲的比较详细的文章：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250146007">https://zhuanlan.zhihu.com/p/250146007</a></p>
<h4 id="Def"><a href="#Def" class="headerlink" title="Def"></a>Def</h4><p>转移概率矩阵：</p>
<p>$P&#x3D;\begin{bmatrix}p_{11} &amp; p_{12} &amp;p_{13} \ p_{21} &amp; p_{22} &amp;p_{23} \ p_{31} &amp; p_{32} &amp;p_{33}\end{bmatrix}$ </p>
<p>其中 $p_{ij}&#x3D;P(X_{t}&#x3D;i|X_{t-1}&#x3D;j)$ 。</p>
<p>定义：马尔科夫链在 $t$ 时刻的概率分布称为 $t$ 时刻的状态分布：</p>
<p>$\pi (t)&#x3D;\begin{bmatrix}\pi_{1}(t) \ \pi_{2}(t) \ \pi_{3}(t)\end{bmatrix}$ </p>
<p>其中  $\pi_{i} (t)&#x3D;P(X_{t}&#x3D;i),i&#x3D;1,2,…$ 。</p>
<h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><ol>
<li><p>定理：给定一个马尔科夫链 $X&#x3D;\left{ X_0,X_1,…,X_t,… \right}$ ， $t$ 时刻的状态分布：<br> $\pi&#x3D;(\pi_1,\pi_2,…)$ 是 $X$ 的平稳分布的条件是 $\pi&#x3D;(\pi_1,\pi_2,…)$ 是下列方程组的解：<br> $x_{i}&#x3D;\sum_{j}{p_{ij}x_j},i&#x3D;1,2,…$<br> $x_i\geq0,i&#x3D;1,2,…$<br> $\sum_{i}{x_{i}&#x3D;1}$</p>
</li>
<li></li>
</ol>
<h3 id="MCMC具体细节"><a href="#MCMC具体细节" class="headerlink" title="MCMC具体细节"></a>MCMC具体细节</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/253784711">https://zhuanlan.zhihu.com/p/253784711</a></p>
<h2 id="Conditional-Random-Field-CRF"><a href="#Conditional-Random-Field-CRF" class="headerlink" title="Conditional Random Field(CRF)"></a>Conditional Random Field(CRF)</h2><ul>
<li>HMM 生成模型</li>
<li>MEMM Maximum Entropy Markov Model</li>
</ul>
<p><img src="/2024-05-29-Casual-Inference/image-4.png" alt="alt text"></p>
<ul>
<li>HMM:<br>$$ P(\mathbf{X}, \mathbf{Y} | \lambda) &#x3D; P(\mathbf{Y} | \lambda) P(\mathbf{X} | \mathbf{Y}, \lambda) $$</li>
<li>MEMM:<br>$$ P(y_t | y_{t-1}, x_t) $$</li>
<li>CRF:<br>$$ P(\mathbf{Y} | \mathbf{X}, \lambda) &#x3D; \frac{1}{Z(\mathbf{X})} \exp \left( \sum_{t&#x3D;1}^{T} \lambda_t f(y_t, y_{t-1}, \mathbf{X}, t) \right) $$</li>
</ul>
<h2 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1BW41117xo/?spm_id_from=333.999.0.0&vd_source=441679270dda23308fe16f3c5602b058">https://www.bilibili.com/video/BV1BW41117xo/?spm_id_from=333.999.0.0&amp;vd_source=441679270dda23308fe16f3c5602b058</a></p>
<h3 id="概率和图"><a href="#概率和图" class="headerlink" title="概率和图"></a>概率和图</h3><p>概率图</p>
<ul>
<li>表示 Representation<ul>
<li>有向图 Beyesian Netowrk</li>
<li>无向图</li>
<li>高斯图（连续的随机变量）</li>
</ul>
</li>
<li>推断 Inference<ul>
<li>精确推断</li>
<li>近似推断<ul>
<li>确定性推断（变分）</li>
<li>随机近似 MCMC</li>
</ul>
</li>
</ul>
</li>
<li>学习<ul>
<li>参数学习<ul>
<li>完备数据</li>
<li>隐变量</li>
</ul>
</li>
<li>结构学习</li>
</ul>
</li>
</ul>
<p>高维随机变量</p>
<ul>
<li>sum: $P(x_1) &#x3D; \int P(x_1, x_2)dx_2$</li>
<li>product: $P(x_1|x_2) &#x3D; P(x_1|x_2)P(x_2)&#x3D;P(x_2|x_1)P(x_1)$</li>
</ul>
<p>链式法则</p>
<ul>
<li>$$P(X_1,X_2,…,X_n)&#x3D;P(X_1)P(X_2|X_1)P(X_3|X_2,X_1)···P(X_n|X_{n-1},X_{n-2},…,X_1)$$</li>
</ul>
<p>全概率公式</p>
<ul>
<li>$P(X_i)&#x3D;\sum_{j}{P(X_i,X_j)}&#x3D;\sum_{j}{P(X_i|X_j)P(X_j)}$</li>
</ul>
<p>贝叶斯公式</p>
<ul>
<li>$P(X_i|X_j)&#x3D;\frac{P(X_i,X_j)}{P(X_j)}&#x3D;\frac{P(X_i|X_j)P(X_j)}{P(X_j)}$</li>
</ul>
<p>困境：<br>维度高$P(X_1,X_2,…,X_n)$计算复杂</p>
<ul>
<li>1.假设$X_i$相互独立:<ul>
<li>$P(X_1,X_2,…,X_n)&#x3D;\prod_{i}{P(X_i)}$</li>
</ul>
</li>
<li>2.Markov Property(HMM齐次马尔可夫):<ul>
<li>$x_j\perp x_i+1|x_i, j&lt;i$ </li>
<li>$P(X_1, X_2,…,X_n)&#x3D;P(X_1)P(X_2|X_1)P(X_3|X_2,X_1)···P(X_n|X_{n-1},X_{n-2},…,X_1)$</li>
</ul>
</li>
<li>3.假设$X_i$条件独立: <ul>
<li>$x_a\perp x_b|x_c$</li>
<li>$P(X_1,X_2,…,X_n)&#x3D;\prod_{i}{P(X_i|X_{i-1})}$</li>
</ul>
</li>
</ul>
<h4 id="概率补充知识"><a href="#概率补充知识" class="headerlink" title="概率补充知识"></a>概率补充知识</h4><p>指数族分布</p>
<ul>
<li>充分统计量$\phi(x)$</li>
<li>共轭</li>
<li>最大熵</li>
<li>广义线性模型</li>
<li>概率图模型</li>
<li>变分推断</li>
</ul>
<p>充分统计量</p>
<ul>
<li>$P(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))$<ul>
<li>$h(x)$: base measure</li>
<li>$\eta$: parameter 参数向量</li>
<li>$\phi(x)$: feature function</li>
<li>$A(\eta)$： log partition function 配分函数</li>
</ul>
</li>
<li>$P(x|\theta)&#x3D;\frac{1}{z}\hat P(x|\theta)$<ul>
<li>$z&#x3D;\int \hat P(x|\theta)dx$ 归一化因子</li>
</ul>
</li>
<li>$P(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))$<ul>
<li>$&#x3D;\frac{1}{exp(A(\eta))}h(x)exp(\eta^T\phi(x))$</li>
<li>$&#x3D;\frac{1}{z}\hat P(x|\eta)$<ul>
<li>$\hat P(x|\eta)&#x3D;h(x)exp(\eta^T\phi(x))$</li>
<li>$z&#x3D;exp(A(\eta))$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>共轭, 先后验分布同组方便计算</p>
<ul>
<li>$P(\theta|x)&#x3D;\frac{P(x|\theta)P(\theta)}{P(x)}$</li>
<li>$P(\theta|x)$和$P(x|\theta)$属于同一个指数族</li>
<li>$P(\theta|x)$的参数是$P(\theta|x)$的参数的函数</li>
</ul>
<p>先验</p>
<ul>
<li>共轭 - 计算方便</li>
<li>最大熵 无信息先验</li>
<li>Jerrif</li>
</ul>
<p>广义线性模型</p>
<ul>
<li>线性组合 $w^Tx$</li>
<li>Link funciton -&gt;aactivation function</li>
<li>指数分布 $y|x\sim$指数组分布（$Bernulli, Poisson, N(\mu, \Sigma)$）</li>
</ul>
<h5 id="Gaussian"><a href="#Gaussian" class="headerlink" title="Gaussian"></a>Gaussian</h5><ul>
<li>$P(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}|\Sigma|^{1&#x2F;2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$</li>
<li>$\eta&#x3D;<br>\left(!<br>  \begin{array}{c}<br>\eta_1 &#x3D;\frac{\mu}{\sigma^2}\<br>\eta_2&#x3D;-\frac{1}{\sigma^2}<br>  \end{array}<br>  !\right)$</li>
<li>$\phi(x)&#x3D;\left(!<br>  \begin{array}{c}<br>x\<br>x^2<br>  \end{array}<br>  !\right)$</li>
<li>$A(\eta)&#x3D;-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\ln(-\frac{\pi}{\eta_2})$</li>
</ul>
<h4 id="-3"><a href="#-3" class="headerlink" title=""></a></h4><ul>
<li>$P(x | \eta) &#x3D; h(x) \exp (\eta^T \phi(x) - A(\eta))$<ul>
<li>$\eta$: 参数 (parameter)</li>
<li>$\phi(x)$: 充分统计量 (sufficient statistics)</li>
<li>$A(\eta)$: 对数配分函数 (log partition function)</li>
</ul>
</li>
</ul>
<ol>
<li>$A’(\eta) &#x3D; \mathbb{E}_{P(x|\eta)}[\phi(x)]$</li>
<li>$A’’(\eta) &#x3D; \text{Var}[\phi(x)]$</li>
<li>$A(\eta)$ 是凸函数 (convex function)</li>
</ol>
<p>对这个函数求导</p>
<ul>
<li>$\exp(A(\eta)) &#x3D; \int h(x) \exp(\eta^T \phi(x)) , dx$</li>
<li>$A’(\eta) &#x3D; \frac{\partial}{\partial \eta} \log \left( \int h(x) \exp(\eta^T \phi(x)) , dx \right)$</li>
<li>$A’(\eta) &#x3D; \int \frac{h(x) \exp(\eta^T \phi(x)) \phi(x) , dx}{\exp(A(\eta))}$</li>
<li>$A’(\eta) &#x3D; \int P(x | \eta) \phi(x) , dx &#x3D; \mathbb{E}_{P(x|\eta)}[\phi(x)]$</li>
</ul>
<h5 id="MLE"><a href="#MLE" class="headerlink" title="MLE"></a>MLE</h5><p>$D&#x3D;{x_1, \cdots, x_N}$</p>
<p>$\eta_{MLE}&#x3D;\text{argmax  } log(P(D|\eta))$</p>
<ul>
<li>$&#x3D;\sum_{i&#x3D;1}^N\text{argmax  }log\cdot h(x_i)+(\eta^T \phi(x_i) - A(\eta))$</li>
<li><ul>
<li>$&#x3D;\sum_{i&#x3D;1}^N\text{argmax  }\eta^T \phi(x_i) - A(\eta)$</li>
</ul>
</li>
</ul>
<p>set $\frac{\partial \eta_{MLE}}{\partial \eta}&#x3D;0$</p>
<ul>
<li>$\sum \phi(x_i)-NA’(\eta)&#x3D;0$</li>
</ul>
<h5 id="Entorpy-最大熵"><a href="#Entorpy-最大熵" class="headerlink" title="Entorpy 最大熵"></a>Entorpy 最大熵</h5><p>信息熵 $-log\ p$<br>熵，（对可能性的衡量）</p>
<ul>
<li><p>$E_{p(x)}[-log\ p]&#x3D;\int -p(x)log\ p(x)dx&#x3D; \sum -p(x)log\ p(x)$</p>
</li>
<li><p>$\hat p_i&#x3D;\text{argmax } H(p)$ </p>
</li>
<li><p>拉格朗日$\mathcal{L}(p, \lambda)&#x3D;\sum p_ilog\ p_i$</p>
<ul>
<li>$\frac{\partial \mathcal{L}}{\partial p_i}&#x3D;log\ p_i+1-\lambda$</li>
<li>$\hat p_i&#x3D;exp(\lambda-1)&#x3D;1&#x2F;k$</li>
</ul>
</li>
</ul>
<h5 id="经验分布"><a href="#经验分布" class="headerlink" title="经验分布"></a>经验分布</h5><p>$Data &#x3D; {x_1, \cdots, x_N}$</p>
<ul>
<li><strong>频率分布</strong>： $P(x_1, x_2, \ldots, x_n) \approx \hat{P}(x) &#x3D; \frac{\text{count}(x)}{N}$</li>
<li><strong>经验期望</strong>： $\mathbb{E}<em>p[f(x)] &#x3D; \Delta \approx \frac{1}{N} \sum</em>{i&#x3D;1}^N f(x_i)$</li>
</ul>
<p>最大熵问题的求解<br>$$<br>\begin{aligned}<br>\min_{p(x)} &amp; \sum_x p(x) \log p(x) \<br>\text{subject to} &amp; \sum_x p(x) &#x3D; 1 \<br>&amp; \mathbb{E}<em>p[f(x)] &#x3D; \mathbb{E}</em>{\hat{p}}[f(x)] &#x3D; \Delta<br>\end{aligned}<br>$$</p>
<p>拉格朗日乘数法求解<br>$$<br>\begin{aligned}<br>L(p, \lambda, \eta) &amp;&#x3D; \sum_x p(x) \log p(x) + \lambda (1 - \sum_x p(x)) + \eta (\Delta - \sum_x p(x) f(x)) \<br>\frac{\partial}{\partial p(x)} &amp;&#x3D; \log p(x) + 1 - \lambda_0 - \lambda f(x) &#x3D; 0 \<br>p(x) &amp;&#x3D; \exp(\lambda_1 f(x) + \lambda_0 - 1) \<br>&amp;&#x3D; \frac{\exp(\eta^T f(x))}{Z(\eta)}<br>\end{aligned}<br>$$</p>
<h3 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h3><p>有向无环图<br>因子分解</p>
<ul>
<li>$P(X_1,X_2,…,X_n)&#x3D;\prod_{i}{P(X_i|Pa(X_i))}$</li>
<li>$P(X_i|Pa(X_i))$是局部概率分布</li>
<li>$Pa(X_i)$是$X_i$的父节点集合</li>
</ul>
<h4 id="三种模型"><a href="#三种模型" class="headerlink" title="三种模型"></a>三种模型</h4><h5 id="tail-to-tail"><a href="#tail-to-tail" class="headerlink" title="tail-to-tail"></a>tail-to-tail</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a--&gt; b</span><br><span class="line">  a--&gt; c</span><br></pre></td></tr></table></figure>
<p>因子分解：</p>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c|a)$</li>
</ul>
<p>链式法则：</p>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c|a, b)$</li>
</ul>
<p>$\implies P(c|a)&#x3D;P(c|a, b)\implies c\perp b |a$<br>若$b$ 被观测则路径被阻塞： </p>
<h5 id="head-to-tail"><a href="#head-to-tail" class="headerlink" title="head-to-tail"></a>head-to-tail</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a --&gt;b </span><br><span class="line">  b --&gt;c </span><br></pre></td></tr></table></figure>
<p>$a\perp c |b$<br>若$b$ 被观测则路径被阻塞： </p>
<h5 id="head-to-head"><a href="#head-to-head" class="headerlink" title="head-to-head"></a>head-to-head</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  b--&gt; c</span><br><span class="line">  a--&gt; c</span><br></pre></td></tr></table></figure>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b)P(c|a, b)$</li>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c | a, b)$</li>
</ul>
<p>$\implies P(b)&#x3D;P(b|a)\implies a\perp b$<br>若$b$ 被观测则路径被连通：</p>
<h4 id="D-seperation"><a href="#D-seperation" class="headerlink" title="D-seperation"></a>D-seperation</h4>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a  --&gt; b1</span><br><span class="line">  a  --&gt; b2</span><br><span class="line">  a  --&gt; b*</span><br><span class="line">  b1 --&gt; c</span><br><span class="line">  b2 --&gt; c</span><br><span class="line">  c  --&gt; b*</span><br></pre></td></tr></table></figure>
<ul>
<li>如果$b1, b2\in B$被观测了<br>，那么$a$和$c$被阻断，</li>
<li>但是$b*$没有被观测到,且$b*$的后续节点都不在$b$中<br>$a\perp c|b$</li>
</ul>
<p><img src="/2024-05-29-Casual-Inference/image-2.png" alt="alt text"><br>马尔可夫毯(Markov Blanket)</p>
<ul>
<li>$x_{pa(i)}$：$x_i$的父节点</li>
<li>$x_{child(i)}$: childs of $x_i$</li>
<li>$x_{pa(child(i))}$: parent of $x_{child(i)}$</li>
<li>$x_{-i}&#x3D;x&#x2F;x_i$ 表示除了 $x_i$ 以外的所有变量。<ul>
<li>和$x$有关:$\Delta$<ul>
<li>$P(x_i|x_{Pa(i)})&#x3D;f(\bar \Delta)$</li>
</ul>
</li>
<li>和$x$无关:$\bar \Delta$</li>
</ul>
</li>
</ul>
<p>马尔可夫毯的作用是在给定马尔可夫毯内所有节点的情况下，$x_i$ 与网络中其他节点条件独立。</p>
<p>$$<br>P(x_i | x_{-i}) &#x3D; \frac{P(x_i, x_{-i})}{P(x_{-i})} &#x3D; \frac{P(x)}{\int_{x_i} P(x_{-i})} &#x3D; \frac{ \int_{x_i} P(x) , dx_i }{ \int_{x_i} P(x_j | x_{\text{pa}(j)}) , dx_i }<br>$$</p>
<p>$$<br>P(x_{\text{child}(i)} | x_i, x_{\text{Parent}(\text{Child}(i))})<br>$$</p>
<h4 id="贝叶斯网络-Beyesian-Network"><a href="#贝叶斯网络-Beyesian-Network" class="headerlink" title="贝叶斯网络 Beyesian Network"></a>贝叶斯网络 Beyesian Network</h4><p>NB<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  y--&gt; x1 </span><br><span class="line">  y--&gt; xp</span><br></pre></td></tr></table></figure></p>
<p>GMM<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  z--&gt; x</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>Beyesian Network</p>
<ul>
<li>单一： Naive Bayes<ul>
<li>p维： $P(x|y)&#x3D;\prod^p_{i&#x3D;1}P(x_i|y&#x3D;1)$</li>
<li>$x_1\perp x_2|y$</li>
</ul>
</li>
<li>混合：GMM<ul>
<li>Z discrete, z&#x3D;1,2,3,4</li>
</ul>
</li>
<li>时间<ul>
<li>Markov Chain</li>
<li>Gaussian Process(无限维分布)</li>
</ul>
</li>
<li>连续： Gaussian Network</li>
</ul>
<p>动态模型</p>
<ul>
<li>HMM 离散</li>
<li>LDS Kalman Filter 连续线性</li>
<li>Particle filter 非线性非高斯</li>
</ul>
<h3 id="Markov-Network"><a href="#Markov-Network" class="headerlink" title="Markov Network"></a>Markov Network</h3><p>条件独立性</p>
<ul>
<li>全局 Global Markov Property<ul>
<li>$X_A \perp X_C \mid X_B$</li>
<li>如果集合 $X_A$ 和 $X_C$ 被集合 $X_B$ 分隔开，那么 $X_A$ 和 $X_C$ 是条件独立的</li>
</ul>
</li>
<li>局部 Local Markov Property<ul>
<li>$a \perp {Non-Neighbour} \mid {Neighbour}$</li>
<li>{ }：集合</li>
</ul>
</li>
<li>成对 Pairwise Markov Property<ul>
<li>$x_i \perp x_j \mid x_{-ij}$</li>
<li>如果节点 $x_i$ 和 $x_j$ 直接相连，那么在给定其他所有节点的情况下，$x_i$ 和 $x_j$ 是条件独立的</li>
</ul>
</li>
</ul>
<h4 id="Factorization"><a href="#Factorization" class="headerlink" title="Factorization"></a>Factorization</h4><p>团: Clique<br>最大团: Maximal Clique<br>$$<br>P(X) &#x3D; \frac{1}{Z} \prod_{i&#x3D;1}^K \psi(X_{C_i})<br>$$</p>
<ul>
<li>$c_i$ 最大团</li>
<li>$x_{c_i}$: 最大团随机变量集合</li>
<li>$\psi(x_{c_i})$: 势函数</li>
<li>$Z$:<ul>
<li>$Z &#x3D; \sum_{X} \prod_{i&#x3D;1}^K \psi(X_{C_i})$</li>
</ul>
</li>
<li>$\psi(X_{C_i})$ 是定义在最大团 $C_i$ 上的因子函数</li>
</ul>
<p>何弃疗<br><img src="/2024-05-29-Casual-Inference/image-3.png" alt="alt text"></p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li><p>联合概率 Joint Probability</p>
<ul>
<li>$P(X) &#x3D; P(x_1, x_2, \ldots, x_p)$</li>
</ul>
</li>
<li><p>边缘概率 Marginal Probability</p>
<ul>
<li>$P(x) &#x3D; \sum_{x_j} P(x_j)$</li>
</ul>
</li>
<li><p>条件概率 Conditional Probability</p>
<ul>
<li>$P(x_i | x_j)$, 其中$x_j &#x3D; {x \backslash x_i}$</li>
</ul>
</li>
<li><p>最大后验概率估计 MAP Inference</p>
<ul>
<li>$\hat{z} &#x3D; \arg \max_z P(z | x) \propto \arg \max_z P(z, x)$</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>精确推断 Exact Inference</p>
<ul>
<li>Variable Elimination (VE)</li>
<li>Belief Propagation (BP) → Sum-Product Algorithm (求和-乘积算法)</li>
<li>Junction Tree Algorithm (树形算法)</li>
</ul>
</li>
<li><p>近似推断 Approximate Inference</p>
<ul>
<li>Loop Belief Propagation (循环信念传播)</li>
<li>Monte Carlo Inference: Importance Sampling, MCMC (蒙特卡罗推断：重要性采样，MCMC)</li>
<li>Variational Inference (变分推断)</li>
</ul>
</li>
</ul>
<h4 id="Variable-Elimination"><a href="#Variable-Elimination" class="headerlink" title="Variable Elimination"></a>Variable Elimination</h4><p>$P(x) &#x3D; \prod_{i} \phi_i(x_i)$</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>假设我们有四个二值随机变量 $a, b, c, d\in {0,1}$。我们想计算边缘概率 $P(d)$。<br>$a\rightarrow b \rightarrow c \rightarrow d$</p>
<ol>
<li><p>展开联合分布：</p>
<ul>
<li>$P(d) &#x3D; \sum_{a, b, c} P(a, b, c, d)$</li>
</ul>
</li>
<li><p>Chain rule</p>
<ul>
<li>$P(a, b, c, d) &#x3D; P(a) P(b | a) P(c | b) P(d | c)$</li>
</ul>
</li>
<li><p>1-&gt;2</p>
<ul>
<li>$P(d) &#x3D; \sum_{a, b, c} P(a) P(b | a) P(c | b) P(d | c)$<ul>
<li>$&#x3D; \sum_{b, c} P(d | c) \left( \sum_{a} P(a) P(b | a) \right) P(c | b)$</li>
</ul>
</li>
</ul>
</li>
<li><p>定义新的因子函数：</p>
<ul>
<li>$\phi_1(b, c) &#x3D; \sum_{a} P(a) P(b | a)$</li>
</ul>
</li>
<li><p>最终得到：</p>
<ul>
<li>$P(d) &#x3D; \sum_{c} P(d | c) \left( \sum_{b} \phi_1(b, c) P(c | b) \right)$</li>
</ul>
</li>
<li><p>得到另一个因子函数 $\phi_2(c, d)$：</p>
<ul>
<li>$P(d) &#x3D; \sum_{c} \phi_2(c, d)$</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/29/2024-05-29-Casual-Inference/" data-id="cm9bnagsv001nzc3d2zxgc2mm" data-title="Casual Inference" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/06/03/2024-06-03-Cytoid-AI-Charting/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Cytoid AI Charting
        
      </div>
    </a>
  
  
    <a href="/2024/05/19/2024-05-19-CS224W-notes/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">CS224W_notes</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/music/" rel="tag">music</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/music/" style="font-size: 10px;">music</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/04/01/2025-04-01-Linux-server-proxy-issue/">2025-04-01-Linux-server-proxy-issue</a>
          </li>
        
          <li>
            <a href="/2025/04/01/2025-04-01-mt-implementation-log/">2025-04-01-mt-implementation-log</a>
          </li>
        
          <li>
            <a href="/2025/03/09/2025-03-08-medication/">2025-03-08 medication</a>
          </li>
        
          <li>
            <a href="/2025/02/15/2025-02-15-DS/">2025-02-15-DS</a>
          </li>
        
          <li>
            <a href="/2025/01/29/2025-01-29-CV/">2025-01-29-CV</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>