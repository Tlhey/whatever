<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-2024-08-14-Webpages" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/14/2024-08-14-Webpages/" class="article-date">
  <time class="dt-published" datetime="2024-08-14T22:00:02.000Z" itemprop="datePublished">2024-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/14/2024-08-14-Webpages/">Webpages</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Wordpress-æ”¶è´¹"><a href="#Wordpress-æ”¶è´¹" class="headerlink" title="Wordpress (æ”¶è´¹)"></a>Wordpress (æ”¶è´¹)</h1><p>é›¶ä»£ç ç¼–è¾‘ç½‘é¡µ Wordpress<br><a target="_blank" rel="noopener" href="https://wordpress.com/">https://wordpress.com/</a><br>Wordpress éƒ¨ç½²ï¼š<br><a target="_blank" rel="noopener" href="https://wordpress.com/setup/domain-transfer/intro">https://wordpress.com/setup/domain-transfer/intro</a><br>ï¼ˆè¿™ä¸ªæ”¶è´¹çš„ï¼Œæ­å®Œæ‰å‘ç°ï¼Œï¼Œï¼Œï¼‰</p>
<p>æ­é…æ¨¡æ¿<br><a target="_blank" rel="noopener" href="https://startertemplatecloud.com/g11/">https://startertemplatecloud.com/g11/</a></p>
<p>é™¤äº†é‚£äº›æµè¡Œçš„æ¨¡æ¿æ¡†æ¶ï¼Œè¿˜æœ‰ä¸€äº›æ¯”è¾ƒå°ä¼—ä½†å¾ˆæœ‰ç‰¹è‰²çš„é€‰æ‹©ï¼Œå®ƒä»¬å¯èƒ½æ›´é€‚åˆç‰¹å®šç±»å‹çš„é¡¹ç›®æˆ–è®¾è®¡åå¥½ï¼š</p>
<p>TurretCSS: ä¸€ä¸ªåŸºäºçº¦æŸçš„ CSS æ¡†æ¶ï¼Œé€šè¿‡å£°æ˜å¼è¯­æ³•å®ç°çµæ´»çš„å¸ƒå±€å’Œå“åº”å¼è®¾è®¡ã€‚<br>BassCSS: ä¸€ä¸ªè½»é‡çº§ã€æ¨¡å—åŒ–çš„ CSS æ¡†æ¶ï¼Œä¸“æ³¨äºæä¾›ç®€æ´ã€æ˜“äºç†è§£çš„æ ·å¼ã€‚<br>Milligram: ä¸€ä¸ªæç®€ä¸»ä¹‰çš„ CSS æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›æœ€å°çš„æ ·å¼ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿè‡ªç”±åœ°æ„å»ºè‡ªå®šä¹‰è®¾è®¡ã€‚<br>Picnic CSS: ä¸€ä¸ªè½»é‡çº§ã€æ˜“äºä½¿ç”¨çš„ CSS æ¡†æ¶ï¼Œæä¾›ç®€æ´ã€ç°ä»£çš„æ ·å¼å’Œç»„ä»¶ã€‚<br>Chota: ä¸€ä¸ªæç®€çš„ CSS æ¡†æ¶ï¼Œæ–‡ä»¶å¤§å°éå¸¸å°ï¼Œé€‚åˆå¯¹æ€§èƒ½è¦æ±‚é«˜çš„é¡¹ç›®ã€‚<br>Blaze CSS: ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„ CSS æ¡†æ¶ï¼Œæä¾›çµæ´»çš„ç½‘æ ¼ç³»ç»Ÿå’Œä¸°å¯Œçš„ç»„ä»¶ã€‚</p>
<h1 id="ä¸ªäººç½‘é¡µæ•™ç¨‹"><a href="#ä¸ªäººç½‘é¡µæ•™ç¨‹" class="headerlink" title="ä¸ªäººç½‘é¡µæ•™ç¨‹"></a>ä¸ªäººç½‘é¡µæ•™ç¨‹</h1><p>Hexo, WordPress<br><a target="_blank" rel="noopener" href="https://pdpeng.github.io/2022/01/19/setup-personal-blog/">https://pdpeng.github.io/2022/01/19/setup-personal-blog/</a></p>
<p>Wordpress<br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wongbingming/p/13819905.html">https://www.cnblogs.com/wongbingming/p/13819905.html</a></p>
<h1 id="Github-Page"><a href="#Github-Page" class="headerlink" title="Github Page"></a>Github Page</h1><p>Github page åŸŸåï¼š<br><a target="_blank" rel="noopener" href="https://pages.github.com/">https://pages.github.com/</a></p>
<h1 id="Free-Gallery-template"><a href="#Free-Gallery-template" class="headerlink" title="Free Gallery template"></a>Free Gallery template</h1><p>å¥½ç”¨å¥½çœ‹<br><a target="_blank" rel="noopener" href="https://www.free-css.com/template-categories/gallery">https://www.free-css.com/template-categories/gallery</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/14/2024-08-14-Webpages/" data-id="cm9bnagq10019zc3d98jd5oqk" data-title="Webpages" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-08-12-Music-tech-exploring" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/13/2024-08-12-Music-tech-exploring/" class="article-date">
  <time class="dt-published" datetime="2024-08-13T18:19:24.000Z" itemprop="datePublished">2024-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/13/2024-08-12-Music-tech-exploring/">Music tech exploring</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>è·ŸéŸ³ä¹ç›¸å…³çš„ML<br>åº”è¯¥å…ˆçœ‹survey è€Œä¸æ˜¯è‡ªå·±åšsurvey???</p>
<ol>
<li>survey 17: Deep Learning Techniques for Music Generation â€“ A Survey 17<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1709.01620">https://arxiv.org/pdf/1709.01620</a></li>
</ol>
<ul>
<li>symbolic AI â€“ dealing with high-level symbolic representations (e.g., chords, harmony. . . ) and processes (harmonization, analysis. . . ); and</li>
<li>sub-symbolic AI â€“ dealing with low-level representations (e.g., sound, timbre. . . ) and processes (pitch recognition,<br>classification. . . ).<br><img src="/2024-06-03-Cytoid-AI-Charting/image-1.png" alt="alt text"><br>(a) Musical score of a C-major scale. (b) Chromagram obtained from the score. (c) Audio recording of the C-major scale played on a piano. (d) Chromagram obtained from the audio recording.</li>
</ul>
<p>4.11.2 One-hot, Many-Hot and to Multi-One-Hot</p>
<p>Dataset:<br>4.12.2 Datasets and Libraries</p>
<ul>
<li>Classical piano MIDI database <a target="_blank" rel="noopener" href="http://piano-midi.de/">http://piano-midi.de/</a></li>
</ul>
<h1 id="CONCERT-94"><a href="#CONCERT-94" class="headerlink" title="CONCERT 94"></a>CONCERT 94</h1><p>Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multi-scale processingâ€</p>
<ul>
<li>estimating the probability of playing the next note</li>
<li>generated notes: $P(Y_{n+1}&#x3D;y_{n+1}|Y_n&#x3D;y_n, Y_{nâˆ’1}&#x3D;y_{nâˆ’1}, Y_{nâˆ’2}&#x3D;y_{nâˆ’2},â€¦)$</li>
</ul>
<h1 id="DeepJ-18"><a href="#DeepJ-18" class="headerlink" title="DeepJ 18"></a>DeepJ 18</h1><p>å¢å¼ºæ”¹å˜ Style å’Œ type<br>polyphonic music conditioned on a specific or a mixture of multiple composer styles</p>
<ul>
<li>polyphonic (å¤è°ƒ): not monophonic</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method:"></a>Method:</h2><ul>
<li>previous: comb of RNN, RBM (restricted Boltzmann machines):</li>
<li><strong>Novel</strong>:  Biaxial LSTM</li>
</ul>
<p>Biaxial LSTM</p>
<ol>
<li>rep: MIDI $NÃ—T$ </li>
<li>architecture:  $P(Y_{t, n}\vert Y_{t, n-1}, \ Y_{t, n-2}, \ \ldots, \ Y_{t-1, N}, \ Y_{t-1, N-1}, \ldots, \ Y_{1, 2}, \ Y_{1, 1})$<br><img src="/2024-06-03-Cytoid-AI-Charting/image.png" alt="alt text"></li>
</ol>
<h1 id="Bach-2-0"><a href="#Bach-2-0" class="headerlink" title="Bach 2.0"></a>Bach 2.0</h1><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S1877050919313444?via=ihub">https://www.sciencedirect.com/science/article/pii/S1877050919313444?via%3Dihub</a></p>
<h1 id="ISMIR"><a href="#ISMIR" class="headerlink" title="ISMIR"></a>ISMIR</h1><p><a target="_blank" rel="noopener" href="https://ismir.net/conferences/">https://ismir.net/conferences/</a></p>
<h1 id="Prof-Shlomo-Dubnov"><a href="#Prof-Shlomo-Dubnov" class="headerlink" title="Prof Shlomo Dubnov"></a>Prof Shlomo Dubnov</h1><p>Google scholar: <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=NJfiIl8AAAAJ&sortby=pubdate">https://scholar.google.com/citations?view_op=list_works&amp;hl=en&amp;hl=en&amp;user=NJfiIl8AAAAJ&amp;sortby=pubdate</a><br>Webpage: <a target="_blank" rel="noopener" href="http://shlomodubnov.wikidot.com/research">http://shlomodubnov.wikidot.com/research</a></p>
<h1 id="prof-Gus-Xia-çš„è®²åº§"><a href="#prof-Gus-Xia-çš„è®²åº§" class="headerlink" title="prof Gus Xia çš„è®²åº§"></a>prof Gus Xia çš„è®²åº§</h1><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=dPeh3XVlmlE">https://www.youtube.com/watch?v=dPeh3XVlmlE</a></p>
<h2 id="Intro-pitches-tuning"><a href="#Intro-pitches-tuning" class="headerlink" title="Intro: pitches tuning:"></a>Intro: pitches tuning:</h2><p>pythagorean tuning: ä¸‰åˆ†æŸç›Šæ³•ï¼Œ äº”åº¦ç›¸ç”Ÿå¾‹<br>Octave 2ï¼š1 with fifth 3:2<br>å…«åº¦ 2ï¼š1ï¼Œ äº”åº¦3ï¼š2<br>åäºŒå¹³å‡å¾‹ï¼š $2^{(x&#x2F;12)}$</p>
<h2 id="AI-Music-listening"><a href="#AI-Music-listening" class="headerlink" title="AI+ Music listening"></a>AI+ Music listening</h2><h3 id="Fingerprint"><a href="#Fingerprint" class="headerlink" title="Fingerprint"></a>Fingerprint</h3><p>Extraction Algorithm: Philips (2002)<br><img src="/2024-08-12-Music-tech-exploring/image.png" alt="alt text"><br>å‚…é‡Œå¶åˆ†æä»€ä¹ˆçš„<br>Short-time fourier transform<br>FFT size 0.37<br>Hanning window</p>
<h3 id="Query-by-Humming"><a href="#Query-by-Humming" class="headerlink" title="Query by Humming"></a>Query by Humming</h3><ol>
<li>partial matching</li>
<li>fuzzy match</li>
<li>out of pitch</li>
</ol>
<p>MIDI + åˆæˆå™¨<br>MIDI(straigthforward like a string) !&#x3D; Audio (quantization)</p>
<p>Absolute pitch<br>relatice pitch<br>IOI(Inter Onset Interval) ratio</p>
<p>Dynamic programming<br>Pattern recognition:</p>
<h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><p>spectrum(energy at diss frequencies), MFCC, Zero crossing, Chroma, Estimtates of tempo<br>chromagram, spectrogram<br><img src="/2024-08-12-Music-tech-exploring/image-1.png" alt="alt text"></p>
<h2 id="AI-Music-Composition"><a href="#AI-Music-Composition" class="headerlink" title="AI+ Music Composition"></a>AI+ Music Composition</h2><p>Algorithmic composition</p>
<p>Canon, </p>
<h3 id="Deterministic"><a href="#Deterministic" class="headerlink" title="Deterministic"></a>Deterministic</h3><p>Fractal:åˆ†å‹</p>
<h3 id="Stochastic-process"><a href="#Stochastic-process" class="headerlink" title="Stochastic process"></a>Stochastic process</h3><h3 id="ML-Music-as-sequence"><a href="#ML-Music-as-sequence" class="headerlink" title="ML: Music as sequence"></a>ML: Music as sequence</h3><h3 id="Mapping-natural-phenomena-ot-music"><a href="#Mapping-natural-phenomena-ot-music" class="headerlink" title="Mapping natural phenomena ot music"></a>Mapping natural phenomena ot music</h3><h2 id="AI-Music-Composition-1"><a href="#AI-Music-Composition-1" class="headerlink" title="AI+ Music Composition"></a>AI+ Music Composition</h2><h1 id="Fugue"><a href="#Fugue" class="headerlink" title="Fugue"></a>Fugue</h1><p>åœ¨èµ‹æ ¼ä¸­ï¼ŒåŸºæœ¬å…ƒç´ åŒ…æ‹¬ï¼š</p>
<p>ä¸»é¢˜ (Subject)ï¼šèµ‹æ ¼å¼€å§‹æ—¶ç”±ä¸€ä¸ªå£°éƒ¨æå‡ºçš„æ—‹å¾‹åŠ¨æœºã€‚<br>ç­”é¢˜ (Answer)ï¼šå¦ä¸€å£°éƒ¨ç´§æ¥ä¸»é¢˜ä»¥æ¨¡ä»¿çš„å½¢å¼è¿›å…¥ï¼Œä¸€èˆ¬ä¼šè½¬è°ƒè‡³å±éŸ³ã€‚<br>å¯¹é¢˜ (Countersubject)ï¼šä¼´éšç­”é¢˜å‡ºç°çš„ç¬¬äºŒä¸ªæ—‹å¾‹åŠ¨æœºã€‚<br>æ’æ®µ (Episode)ï¼šä¸»é¢˜å’Œç­”é¢˜å‡ºç°åï¼Œæ’å…¥çš„è‡ªç”±å‘å±•æ®µè½ã€‚<br>codetta<br>interlude<br>èµ‹æ ¼çš„é™ˆè¿°éƒ¨åˆ† (Exposition) æ˜¯å…¶æœ€ä¸¥æ ¼çš„éƒ¨åˆ†ï¼Œä¹‹åçš„æ®µè½åˆ™å…è®¸æ›´è‡ªç”±çš„å˜åŒ–ã€‚ä½œæ›²å®¶å¯ä»¥é€šè¿‡å„ç§æŠ€æœ¯æ‰‹æ®µå¦‚åŠ å€ (augmentation)ã€å‡ç¼© (diminution)ã€å€’å½± (inversion) å’Œç´§ç¼© (stretto) æ¥æ”¹å˜ä¸»é¢˜çš„åŸå‹ã€‚</p>
<h1 id="Fourier-Transform"><a href="#Fourier-Transform" class="headerlink" title="Fourier Transform"></a>Fourier Transform</h1><p><a target="_blank" rel="noopener" href="https://ltyxh.com/blog/2022/05/17/%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF4/">https://ltyxh.com/blog/2022/05/17/%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF4/</a><br>è¿™ä¸ªç³»åˆ—è®²çš„ç‰¹åˆ«æ¸…æ¥š</p>
<h1 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h1><p>ä»0æ•™ä½ VAEï¼š<br>    - äº¤å‰ç†µä¸KLæ•£åº¦(ä¿¡æ¯è®º)ï¼š<br>        - <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/345025351">https://zhuanlan.zhihu.com/p/345025351</a><br>            - $I(x) &#x3D; K log(P(x)), K&lt;0$ Information<br>            - $H(p) &#x3D; -\sum p(x_i)logp(x_i)$ Entropy<br>        - <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346518942">https://zhuanlan.zhihu.com/p/346518942</a><br>    - MSE, 0-1 loss, Logistic loss:<br>        -  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346935187">https://zhuanlan.zhihu.com/p/346935187</a><br>    - VAEï¼š<br>        - <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348498294">https://zhuanlan.zhihu.com/p/348498294</a><br>        - <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34998569">https://zhuanlan.zhihu.com/p/34998569</a><br>        - VAEè®²è§£ï¼š<a target="_blank" rel="noopener" href="https://www.zhangzhenhu.com/aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html">https://www.zhangzhenhu.com/aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html</a></p>
<h2 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h2><p>Cross Entropy Loss:<br>$H(p, q) &#x3D; \mathbb{E}_{X \sim p(X)} \left[ - \log q(X) \right].\$</p>
<p>KL Divergence:<br>å¯¹äºç¦»æ•£éšæœºå˜é‡ï¼Œåˆ†å¸ƒ$p$å’Œ$q$çš„KLæ•£åº¦çš„å®šä¹‰å¦‚ä¸‹ï¼š</p>
<p>$D_{K L}(p | q) &#x3D; -\sum_{i&#x3D;1}^{n} p(x_{i}) \cdot \log \frac{q(x_{i})}{p(x_{i})}.\$</p>
<p>å¯¹KLæ•£åº¦åœ¨ä¿¡æ¯è®ºä¸­çš„ä¸€ä¸ªç›´è§‚çš„ç†è§£æ˜¯å°†å…¶å†™å¼€ï¼Œå³</p>
<p>$\begin{aligned} D_{K L}(p | q) &amp;&#x3D; -\sum_{i&#x3D;1}^{n} p(x_{i}) \cdot \log \frac{q(x_{i})}{p(x_{i})} \                 &amp;&#x3D; -\sum_{i&#x3D;1}^{n} p(x_{i}) \cdot \log q(x_{i}) + \sum_{i&#x3D;1}^{n} p(x_{i}) \cdot \log p(x_{i}) \                 &amp;&#x3D; H(p,q) - H(p). \end{aligned}\$</p>
<h2 id="å¥½ä¸œè¥¿ï¼Œï¼Œ"><a href="#å¥½ä¸œè¥¿ï¼Œï¼Œ" class="headerlink" title="å¥½ä¸œè¥¿ï¼Œï¼Œ"></a>å¥½ä¸œè¥¿ï¼Œï¼Œ</h2><p>lossè¯¦è§£<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1334301979816820736">https://www.zhihu.com/column/c_1334301979816820736</a></p>
<p>é™¤äº†æˆ‘éƒ½ä¼šï¼Œï¼Œå›¾ç¥ç»ç½‘ç»œ<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1322582255018184704">https://www.zhihu.com/column/c_1322582255018184704</a></p>
<h2 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h2><p>æ€»çš„æ¥è¯´è§‰å¾—xå’Œzæ˜¯å¯¹ç§°çš„ï¼Œä½†xå¯é‡‡æ ·ï¼Œæ‰“ç ´å¯¹ç§°æ€§ã€‚<br>ä½†æ˜¯ä¸æ˜¯çš„ï¼Œï¼Œ</p>
<h2 id="è®²å¾—å¾ˆå¥½"><a href="#è®²å¾—å¾ˆå¥½" class="headerlink" title="è®²å¾—å¾ˆå¥½"></a>è®²å¾—å¾ˆå¥½</h2><p>DDPMè§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1p24y1K7Pf?spm_id_from=333.788.videopod.sections&vd_source=441679270dda23308fe16f3c5602b058">https://www.bilibili.com/video/BV1p24y1K7Pf?spm_id_from=333.788.videopod.sections&amp;vd_source=441679270dda23308fe16f3c5602b058</a><br>æ–‡ç« ï¼š<a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv23338176/?jump_opus=1">https://www.bilibili.com/read/cv23338176/?jump_opus=1</a><br>çŸ¥ä¹ç‰ˆæœ¬ï¼š <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624851115">https://zhuanlan.zhihu.com/p/624851115</a></p>
<p>odeå’Œå¯è§†åŒ–ï¼š <a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/">https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/</a></p>
<h1 id="Diffusion-model"><a href="#Diffusion-model" class="headerlink" title="Diffusion model"></a>Diffusion model</h1><p>ä»‹ç»äº†consisitency model, VAE, Diffusion model , stable diffusion, LoRA, Latent Consistency Model ä¹‹é—´çš„å…³ç³»ï¼Œ<br>ä»å¤´è®²è§£çš„Consistency model çš„åŸç†<br><a target="_blank" rel="noopener" href="https://wrong.wang/blog/20220605-%E4%BB%80%E4%B9%88%E6%98%AFdiffusion%E6%A8%A1%E5%9E%8B/">https://wrong.wang/blog/20220605-%E4%BB%80%E4%B9%88%E6%98%AFdiffusion%E6%A8%A1%E5%9E%8B/</a></p>
<p><a target="_blank" rel="noopener" href="https://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0speed_sd.html">https://antarina.tech/posts/notes/articles/%E7%AC%94%E8%AE%B0speed_sd.html</a></p>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></p>
<h1 id="Consisitency-Diffusion"><a href="#Consisitency-Diffusion" class="headerlink" title="Consisitency Diffusion"></a>Consisitency Diffusion</h1><h2 id="https-wrong-wang-blog-20231111-consistency-is-all-you-need-https-zhuanlan-zhihu-com-p-692998238https-zhuanlan-zhihu-com-p-706862530-DIFF-A-RIFF-Musical-Acoompanimetn-via-latent-diffusion-models1-Consistency-Autoencoder2-Elucidated-Diffusion-Models-EDMs-Related-1-End-to-End-Autoregressive-model-high-fidelity-and-ability-to-produce-coherent-long-range-sequences-expensive-cost-for-calculateion-GANs-VAEs-are-faster-but-limiting-fidelity-Denoising-Difussion-Implicit-Models-2-Latent-models-3-Control-Mechanism-Music-ControlNet"><a href="#https-wrong-wang-blog-20231111-consistency-is-all-you-need-https-zhuanlan-zhihu-com-p-692998238https-zhuanlan-zhihu-com-p-706862530-DIFF-A-RIFF-Musical-Acoompanimetn-via-latent-diffusion-models1-Consistency-Autoencoder2-Elucidated-Diffusion-Models-EDMs-Related-1-End-to-End-Autoregressive-model-high-fidelity-and-ability-to-produce-coherent-long-range-sequences-expensive-cost-for-calculateion-GANs-VAEs-are-faster-but-limiting-fidelity-Denoising-Difussion-Implicit-Models-2-Latent-models-3-Control-Mechanism-Music-ControlNet" class="headerlink" title="https://wrong.wang/blog/20231111-consistency-is-all-you-need/https://zhuanlan.zhihu.com/p/692998238https://zhuanlan.zhihu.com/p/706862530# DIFF-A-RIFF: Musical Acoompanimetn via latent diffusion models1. Consistency Autoencoder2. Elucidated Diffusion Models (EDMs)Related:1. End-to-End Autoregressive model:    - high fidelity and ability to produce coherent, long-range sequences    - expensive cost for calculateion    - GANs&#x2F;VAEs are faster but limiting fidelity    - Denoising Difussion Implicit Models:2. Latent models    -3. Control Mechanism   - Music ControlNet"></a><a target="_blank" rel="noopener" href="https://wrong.wang/blog/20231111-consistency-is-all-you-need/">https://wrong.wang/blog/20231111-consistency-is-all-you-need/</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/692998238">https://zhuanlan.zhihu.com/p/692998238</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706862530">https://zhuanlan.zhihu.com/p/706862530</a><br># DIFF-A-RIFF: Musical Acoompanimetn via latent diffusion models<br>1. Consistency Autoencoder<br>2. Elucidated Diffusion Models (EDMs)<br><br>Related:<br>1. End-to-End Autoregressive model:<br>    - high fidelity and ability to produce coherent, long-range sequences<br>    - expensive cost for calculateion<br>    - GANs&#x2F;VAEs are faster but limiting fidelity<br>    - Denoising Difussion Implicit Models:<br>2. Latent models<br>    -<br>3. Control Mechanism<br>   - Music ControlNet</h2><h1 id="Flow-matching"><a href="#Flow-matching" class="headerlink" title="Flow matching"></a>Flow matching</h1><h1 id="Score-matching"><a href="#Score-matching" class="headerlink" title="Score matching"></a>Score matching</h1><p>score<br>$\nabla_x log p_\sigma(x)$</p>
<p><a target="_blank" rel="noopener" href="https://www.zhangzhenhu.com/aigc/Guidance.html">https://www.zhangzhenhu.com/aigc/Guidance.html</a></p>
<p>çœ‹åˆ°å¥‡æ€ªçš„äººï¼š <a target="_blank" rel="noopener" href="https://www.zhihu.com/people/labsig">https://www.zhihu.com/people/labsig</a></p>
<p>zouæ•™æˆå‘çš„ä¸‰ç¯‡è®ºæ–‡<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.08384">https://arxiv.org/pdf/2406.08384</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.18503">https://arxiv.org/pdf/2405.18503</a></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=mUVydzrkgz">https://openreview.net/pdf?id=mUVydzrkgz</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/13/2024-08-12-Music-tech-exploring/" data-id="cm9bnagpv0011zc3dghi88hwr" data-title="Music tech exploring" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-07-14-Papers-Collection" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/07/14/2024-07-14-Papers-Collection/" class="article-date">
  <time class="dt-published" datetime="2024-07-15T01:24:36.000Z" itemprop="datePublished">2024-07-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/07/14/2024-07-14-Papers-Collection/">Papers Collection</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- 

# Counterfactual fairness
Counterfactual fairness
link: https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf

###
Definitions:
#### defs
$A$: Protected attributes, sensitive features\
$X$: features of individuals, excluding A\
$U$: latent features not observed, represented\
$Y$: predictor    
#### Fairness through unawareness (FTU):
_An algorithm is fair so long as any protected attributes $A$ are not explicitly used in the decision-making process._
Shortcoming: $X$ might intersects $A$

#### Individual Fairness (IF).
For distance metric(should be carefully choosen), $d(\cdot , \cdot)$, if $d(i, j)$ is small, then $\hat Y(X^{(i)}, A^{(i)}) \approx \hat Y(X^{(j)}, A^{(j)})$

#### Demographic Parity (DP)(äººå£ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„å¹³ç­‰)
Predictor $\hat Y$ satisfies demographic partiy if $P(\hat Y|A=0)=P(\hat Y|A=1)$ 
#### Equality of Opportunity
$P(\hat Y|A=0, Y=1)=P(\hat Y|A=1, Y=1)$ 

### Causal Models(å› æœæ¨æ–­), Counterfacutalã€
Casual Model $(U, V, F)$,\
$U$: latent background variables,\
$V$: observed variables, \
$F=\{f_1. f_2, \cdots, f_n\}$, for each $V_i=f_i(pa_i, U_{pa_i})\in V, pa_i \subseteq V \backslash {V_i}$ 

**Three Steps of Inference**\
- Abductionï¼šfor a given prior on $U$, compute the posterior distribution of $U$ given the evidence $W = w$
- Actionï¼šsubstitute the equations for $Z$ with the interventional values $z$, resulting in the modified set of equations $F_z$
- Prediction: 



# FairGAD
https://openreview.net/forum?id=3cE6NKYy8x

https://arxiv.org/abs/2307.04937
## Fair GAD problem
**GAD**\
$G=(V, E, X)$, \
node feature matrix $X\in \R^{n\times d}$, \
Adjacency matrix $A\in \{0,1\}^{n\times n}$, \
Anomaly labels $Y\in \{0, 1\}^n$, predicted $\hat Y$, \
**Fair GAD**\
sensitive attributes $S\in \{0, 1\}^n$, a binary feature $X$.\
Performance matrix: accuracy and _AUCROC_: Area under the ROC Curve \
Unfairness Mextrics, Statistic Parity(SP):$SP = |P(\hat Y=1|S=0)âˆ’P(\hat Y =1|S=1)|$, \
Equality of Odds _(EOO)_: $SP = |P(\hat Y=1|S=0, Y=1)âˆ’P(\hat Y =1|S=1, Y=1)|$
## Data
- Reddit: 
graph structureï¼š linking two user posted the name subreddit within 24h.
Node feature: Embedding from post histories.
- Twitter: 
graph structure:: A follows B.
Node feature: demographic infromation using M3 system, multimodal, multilingual, multi attirbute demographix inderence framework.

## GAD Methods
### DOMINANT (Ding et al., 2019a)
### CONAD (Xu et al., 2022)
### COLA (Liu et al., 2021)
### VGOD (Huang et al., 2023)

## Non-Graph AD methods
- DONE (Bandyopadhyay et al., 2020)
- AdONE (Bandyopadhyay et al., 2020)
- ECOD (Li et al., 2022)
- VAE (Kingma & Welling, 2014)
- ONE (Bandyopadhyay et al., 2019)
- LOF (Breunig et al., 2000)
- F (Liu et al., 2008)

## Fainess Method:
### FAIROD (Shekhar et al., 2021)
### CORRELATION (Shekhar et al., 2021)
### HIN (Zeng et al., 2021)
### EDITS (Dong et al., 2022)
### FAIRWALK (Rahman et al., 2019)

## Distance 
### Wasserstein Distance
### Minkowski distance 


# 2024 Counterfactual Learning on Graphs: A Survey 
3.5.1 How to create synthetic dataset 

# 2022 Learning Fair Node Representations with Graph Counter factual Fairness
Two limitation on existing CF on graph:
1. $S_i$ affect the predetection. Red
2. $S_i$ affect $A, X_i$ Green 

GEAR: Graph Counterfactually Fair Node Representation
1. subgraph generation
Node **Importance Score** by prune range of casualmodel to **ego-centric subgraph**( node and its neighbour)
2. Counterfactual Data Argmentation: 
Graph Auto encodder and fair contrains: **self-pertubation**(flip its $S_i$), **neighbour pertubatiob**
3. Node Representation Learning  :
Siamese network to minimize discrepancy 

**Def, Graph conterfactual fairness:**
An encoder $\Phi(\cdot)$ satisfies graph counterfactual fairness if for any node $i$:
$$
P((Z_i)_{S \leftarrow s'} | X = \mathbf{X}, A = \mathbf{A}) = P((Z_i)_{S \leftarrow s''} | X = \mathbf{X}, A = \mathbf{A}),
$$
for all $s' \neq s''$, where $s', s'' \in \{0, 1\}^n$ are arbitrary sensitive attribute values of all nodes, $Z_i = (\Phi(\mathbf{X}, \mathbf{A}))_i$ denotes the node representations.

$\Phi$, minimize the discrepancy between representation $\Phi(X_{S\leftarrow s'}, A_{S\leftarrow s'})$ and $\Phi(X_{S\leftarrow s''}, A_{S\leftarrow s''})$


### GEAR
### 1) subgraph generation
Personalized Pagerank algorithm:
Importance score $\mathbf R=\alpha (\mathbf I-(1-\alpha \mathbf {\bar A}))$, $\mathbf I$, identity\
$R_{i,j}$ How node $j$ is important for node $i$, $\alpha \in [0,1]$

$\mathbf {\bar A}=\mathbf A \mathbf D^{-1} $ column-normalized adjacency matric, $\mathbf D: \mathbf D_{i, i}=\sum_j A{i, j}$

$\mathcal{G}^{(i)}=Sub(i, \mathcal{G}, k)$ :, subgraph generation

- $\mathcal{G}^{(i)} = \{ \mathcal{V}^{(i)}, \mathcal{E}^{(i)}, \mathbf{X}^{(i)} \} = \{ \mathbf{A}^{(i)}, \mathbf{X}^{(i)} \},
$ Vertive, Edge, Features with $S=\{s_i\}_{i=1}^n $ includes in $X$, and $X^{\neg s} = \{ x_1^{\neg s}, ..., x_n^{\neg s} \} $, where $ x_i^{\neg s} = x_i \setminus s_i$

- $\mathcal{V}^{(i)} = \text{TOP}(\mathbf{R}_{i,:}, k),$

- $\mathbf{A}^{(i)} = \mathbf{A}_{\mathcal{V}^{(i)}, \mathcal{V}^{(i)}}, \quad \mathbf{X}^{(i)} = \mathbf{X}_{\mathcal{V}^{(i)}, :},
$, 

### 2ï¼‰Counterfactual Data Augmentation
**GraphVAG**: graph variational auto-encoder\
latent embedding $H=\{h_1, h_2, \cdots, h_k\}$  $H$ is sampled from $q(H|X, A)$,  $p(ğ»)$ is a standard Normal prior distribution\
$\mathcal{L}=$

$\tilde{s}_i$: summary of neighbor info, aggregationof all nodes in subgarph $\mathcal{G}^{(i)}$\
$\tilde{s}_i = \frac{1}{|\mathcal{V}^{(i)}|} \sum_{j \in \mathcal{V}^{(i)}} s_j$

Discriminator,$D(\cdot)$\
$D(\mathbf{H}, b)$  predicts the probability of whether the summary of sensitive attribute values is in range $b$

Fairness Constraint\
$L_d = \sum_{b \in B} \mathbb{E} [\log(D(\mathbf{H}, b))]$\
$L_d$ is a regularizer to minimize the mutual information between the summary of sensitive attribute values and the
embeddings

**Final Loss** for Counterfactual Data Augmentation\
$L_a = L_r + \beta L_d$\
$\beta$ is a hyperparameter for the weight of fairness constraint\
Use alternating SGD for optimization: 
1) minimize $L_{a}$ by fixing the discriminator and updating parameters in other parts; 
2) minimize $âˆ’L_{a}$ with respect to the discriminator while other parts fixed.


#### Self-Perturbation
$\overline{\mathcal{G}}^{(i)} = \{ \mathcal{G}^{(i)}_{S_i \leftarrow 1-s_i} \}$ (flipping sensitive feature)

#### Neighbor-Perturbation
$\underline{\mathcal{G}}^{(i)} = \left\{ \mathcal{G}^{(i)}_{S^{(i)}_{\setminus i} \leftarrow \text{SMP}(S^{(i)}_{\mathcal{V}^{(i)}_{\setminus i}})} \right\}$

subgraph $\mathcal{G}^{(i)}$ ego($i$)-center subgraph with noes $\mathcal{V}^{(i)}$, exclude node $i$: $\mathcal{V}^{(i)}_{\setminus i}$, randomly preterbe the sentsitice value of other nodes: $SMP(\mathcal{V}^{(i)}_{\setminus i})$



Reconstruction Loss (GraphVAE Module)\
$L_r = \mathbb{E}_{q(\mathbf{H}|X, A)} \left[ -\log(p(X, A | \mathbf{H}, S)) \right] + \text{KL}[q(\mathbf{H} | X, A) \| p(\mathbf{H})]$


### 3) Fair Representation learning
**Fairness Loss**
$
L_f = \frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}} \left( (1 - \lambda_s) d(z_i, \bar{z}_i) + \lambda_s d(z_i, \underline{z}_i) \right),
$\
$\lambda_s$ hyperparam control neig-preturbation weight

**Node Representations**
- $
z_i = (\phi(\mathbf{X}^{(i)}, \mathbf{A}^{(i)}))_i,
$
- $
\bar{z}_i = \text{AGG} \left( \left\{ (\phi(\mathbf{X}^{(i)}_{S_i \leftarrow 1-s_i}, \mathbf{A}^{(i)}_{S_i \leftarrow 1-s_i}))_i \right\} \right),
$
- $
\underline{z}_i = \text{AGG} \left( \left\{ (\phi(\mathbf{X}^{(i)}_{S_i \leftarrow \text{SMP}(S^{(i)}_{\mathcal{V}^{(i)}_{\setminus i}})}, \mathbf{A}^{(i)}_{S_i \leftarrow \text{SMP}(S^{(i)}_{\mathcal{V}^{(i)}_{\setminus i}})})_i \right\} \right),
$

Prediction Loss
$L_p = \frac{1}{n} \sum_{i \in [n]} l(f(z_i), y_i),$ $l$: could be CE(Cross entropy), $f(\cdot)$ makes predictions for downstream tasks with the representations, i.e.$ \hat y_i=f(z_i)$

Overall Loss
$
L = L_p + \lambda L_f + \mu \| \theta \|^2,
$

### Dataset creation

Sensitive Attributes
$S_i \sim \text{Bernoulli}(p),$ $p=0.4$ percent $S_i=1$

Latent Embeddings
$Z_i \sim \mathcal{N}(0, \mathbf{I}),$ \
$\mathbf{I}$ identity, dimension of $Z_i$: $d_s=50$

Node Features
$X_i = \mathcal{S}(Z_i) + S_i \mathbf{v},$\
sampling operation $S(\cdot)$ select 25 dims from $Z_i$, $\mathbf{v} \sim \mathcal{N}(0, \mathbf{I})$

Graph Structure
$P(A_{i,j} = 1) = \sigma(\text{cos}(Z_i, Z_j) + a \mathbf{1}(S_i = S_j)),$\
$\sigma$ sigmoid function, $\mathbf{1}(S_i = S_j)==S_i = S_j. \alpha=0.01$

Node Labels
$Y_i = \mathcal{B}(w Z_i + w_s \frac{\sum_{j \in \mathcal{N}_i} S_j}{|\mathcal{N}_i|}),$\
$\mathcal{B}$ Bernulli distribution,$\mathcal{N}_i$ set of neighbors of node i $w, w_i$ weight vector

### Result
Using Synthetic dataset, Bail, Credit










# 24 Three Revisits to Node-Level Graph Anomaly Detection
Outliers, Message Passing and Hyperbolic Neural Networks

### Previous Outlier injection method
$\mathcal{G}=(\mathcal{V}, \mathcal{E}, X, y)$: vertice set, edge set, attibute matrix, label of class

- **Contextual(cntxt.) outlier injection**
Normalize features $x_i'=\frac{x_i}{||x_i||_1}$
Sample $o$ nodes from $\mathcal{V}$ as $\mathcal{V}_c$. without replacement
For node $i$ in $\mathcal{V}_c$, sample $q$ nodes from $\mathcal{V}_r=\mathcal{V}- \mathcal{V}_c$, among them choose the farthest one $j = \text{argmax}_k(||x_i'-x_k'||_2)$ to replace $x_i$ with $x_j$.

- **Strctural(stct.) outlier injection**
create $t$ groups sized $s$ with anomalous nodes.
sample $o=t\times s$ from $\mathcal{V}$ without replacement
Then randoms partition into $t$ groups.
Add edges to make them a clique(fully connected), then drop edges with $p$ probability

#### Score function
The farthest node will have large $||\tilde{\mathbf x}_i||_2$ \
A structural outlier node $i$ will have many neighbors leads to large $||\tilde{\mathbf a}_i||_1$ 


Score function: $score_{norm}(i)=\alpha||\tilde{\mathbf x}_i||_2+(1-\alpha)||\tilde {\mathbf a}_i||_1$,  $\tilde{\mathbf x}_i$: $x_i$ after outlier injection, $\tilde{\mathbf a}_i$: $a_i$ after outlier injection, $A_{ii}=1$\
where cntxt OD, $\alpha=1$, stct OD, $\alpha=0$ :  $\alpha$ ratio of two methods 


test 1: ROC-AUC
For each dataset, use original dataset v.s. l2-nrom for each $x_i$\
do anomaly injection. apply GAD Method to get  $score_{norm}$

### Novel Anomaly injection method

## Sum in terms of Dataset
ä»æ•°æ®é›†çš„è§’åº¦æ¥è¯´ï¼š
### FairGAD:
Reddit:
- æ•°æ®æ¥æºï¼šPost on politic related subReddit
- Labelling Y: based on FACTOID(Sakketou et al., 2022), use the num of posted link(left or right)
- Graph construciton: 




 












# CaD-VAE
 Causal Disentangled Variational Auto-Encoder 
Causal Disentangled Variational Auto-Encoder for Preference Understanding in Recommendation
Link: https://arxiv.org/pdf/2304.07922

Challenges: inability to disentangle the latent factor
DLR: Disentangled Representation learning 
     - DEAR: (Disentangled gEnerative cAusal Representation (DEAR)) https://arxiv.org/abs/2010.02637
     - CasualVAE: https://doi.org/10.1109/CVPR46437.2021.00947

![alt text](2024-07-14-Papers-Collection/image.png)

## In Casual Layer: The SCM is 
### 2.1
- $u \in \{1, \ldots, U\}$: user index
- $i \in \{1, \ldots, I\}$: item index 
 - $\mathcal{D}=(U, I, X)$: dataset  
   - For a user $u$, the historical interactions $D_u = \{x_{u,i} : x_{u,i} \in \{0,1\}\}$ form a multi-hot vector.
   - $x_{u,i} = 0$ means no recorded interaction between user $u$ and item $i$.
   - $x_{u,i} = 1$ means an interaction between user $u$ and item $i$, such as a click.

- $x_u$ denotes all interactions of the user $u$:
$$
x_u = \{x_{u,i} : x_{u,i} = 1\}
$$
   - Users may have diverse interests and interact with items that belong to many high-level concepts, such as preferred film directors, actors, genres, and year of production.

### 2.2

$$
z = g \left( (I - A^T)^{-1} \epsilon \right) := F_\alpha (\epsilon)
$$
 
- $z$: causal variable
- $\epsilon$: exogenous variables from a normal distribution - $\mathcal{N}(0, I)$
- $g$: nonlinear element-wise transformations
- $\alpha$: parameters $(A, g)$.
- $A$: weighted adjacency matrix: $A_{ij}$ is non-zero only if $[z]_i$ is a parent of $[z]_j$. The binary adjacency matrix $I_A$ indicates where $A \neq 0$
- To ensure disentanglement, labels of concepts $c$ are used as additional information


  - If $g$ is invertible, the equation can be rephrased as:
   $$
   g_i^{-1}(z_i) = A_i^T g_i^{-1}(z) + \epsilon_i
   $$
   This implies that after a nonlinear transformation $g$, the factors $z$ satisfy a linear SCM.

1. **Generative Model Assumption**:
   - For a user $u$, the generative model parameterized by $\theta$ assumes that the observed data are generated from the following distribution:
     $$
     p_\theta(x_u) = \mathbb{E}_{p_\theta(c)} \left[ \iint p_\theta (x_u | \epsilon, z_u, c) p_\theta (\epsilon, z_u | c) d\epsilon dz_u \right]
     $$
     Here, $x_u$ is the observed data for user $u$, $\epsilon$ are the exogenous variables, $z_u$ are the latent variables, and $c$ are the labels of the concepts.










# GUIDE
- Paper:
   https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9671990

- Github:
   https://github.com/yushuowiki/GUIDE_pytorch



![alt text](2024-07-14-Papers-Collection/image-1.png)


Structure: ä¸»è¦ç”¨ï¼ˆä¸‰é˜¶å’Œå››é˜¶ï¼‰Motifæ¥encode
EncoderResidual Attention Layer


Attribute: å°±æ˜¯æ™®é€šçš„X 
Encoderç”¨ä¸‰å±‚GCNã€‚









24.02çš„ FairGAD
https://arxiv.org/pdf/2402.15988
https://openreview.net/pdf?id=3cE6NKYy8x
https://github.com/nigelnnk/FairGAD
é€ æ•°æ®é›†çš„
DOMINANT 19
CONAD 22
Cola 21
 VGOD 23


23çš„GFCN
Graph Fairing Convolutional Networks for Anomaly Detection
https://github.com/MahsaMesgaran/GFCN
https://arxiv.org/pdf/2010.10274

VGOD 23.01
https://arxiv.org/pdf/2210.12941

Edits




å¾ˆå¤šæ•°æ®é›†å’Œmodel
https://proceedings.neurips.cc/paper_files/paper/2023/file/5eaafd67434a4cfb1cf829722c65f184-Paper-Datasets_and_Benchmarks.pdf
![alt text](2024-07-14-Papers-Collection/image-3.png)



# ä¸€è®²Deep Casual Learning 21çš„
https://arxiv.org/ftp/arxiv/papers/2211/2211.03374.pdf
![alt text](2024-07-14-Papers-Collection/image-4.png)

# Disentanglement learn
## Fair Rep learn by disentanglement 19
https://proceedings.mlr.press/v97/creager19a/creager19a.pdf
![alt text](2024-07-14-Papers-Collection/image-6.png)
## CAF ä¹Ÿæ˜¯disen,,
## DEFEND 24
paperï¼š https://arxiv.org/pdf/2406.00987
![alt text](2024-07-14-Papers-Collection/image-7.png)










# Counterfactual Augmentation
## CFGAD 24
https://ojs.aaai.org/index.php/AAAI/article/view/30524
Counterfactual Graph Learning for Anomaly Detection with Feature
Disentanglement and Generation (Student Abstract)
## NIFTY 21
- paper: https://arxiv.org/pdf/2102.13186
- Code: https://github.com/chirag126/nifty?tab=readme-ov-file

![alt text](2024-07-14-Papers-Collection/image-2.png)

Augmented:
-  Node level 
     - attribute masking $r \sim \mathcal{B}(P_n)$
     - $\tilde{\mathbf{x}}_u = \mathbf{x}_u + \mathbf{r} \circ \delta$, where $\delta \in \mathbb{R}^M$ is sampled from a normal distribution.
- sens attribute level
  - 
- edge level

## DEFEND
![alt text](2024-07-14-Papers-Collection/image-5.png)


## GEAR 22
c
## MCCNIFTY 21
## Fairness-Aware 21
## CAF 23
paper: https://arxiv.org/pdf/2307.04937
code: https://github.com/TimeLovercc/CAF-GNN?tab=readme-ov-file

![alt text](2024-07-14-Papers-Collection/image-10.png)
## FairGNN
uses adversarial training to achieve fairness on graphs. It trains the learned representation via an adversary which is optimized to predict the sensitive attribute
## EDITS 23
is a pre-processing method for fair graph learning. It aims to debias the input network to remove the sensitive
information in the graph data
## Fatra 24

## CAGAD 24
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10564850
heterophily dominant neighbors: most of its neighbors have different class labels from the target node
1. GPNN graph pointer nn:  detect heter nodes
   composed of encoder and decoder
2. DDMP (deniosing difussion probabilistic model): translate, create anomaly neigbors for heter nodes
3. GAT Graph attention network: detect anomaly nodes
æœ‰ç‚¹æƒ³åŠ ä¸€ä¸ªPRAUCçš„æµ‹è¯•æŒ‡æ ‡ï¼š æ‰€ä»¥å½“æˆ‘ä»¬å¸Œæœ›æ¨¡å‹åœ¨æ­£è´Ÿæ ·æœ¬ä¸Šéƒ½èƒ½è¡¨ç°è¾ƒå¥½æ—¶ä½¿ç”¨ ROC-AUC è¡¡é‡ï¼Œå¦‚æœæˆ‘ä»¬åªå…³æ³¨æ¨¡å‹å¯¹æ­£æ ·æœ¬çš„åˆ†è¾¨èƒ½åŠ›ä½¿ç”¨ PR-AUC æ›´å¥½


## GFCN 24
https://arxiv.org/abs/2010.10274


## GAD-NR 24 (in Pygod)
![alt text](2024-07-14-Papers-Collection/image-11.png)

# GAD with node rep learn
## a survey on GAD -21
method and datasets
https://github.com/XiaoxiaoMa-MQ/Awesome-Deep-Graph-Anomaly-Detection

## a survey 23: Graph Learning for Anomaly Analytics: Algorithms, Applications, and Challenges
https://dl.acm.org/doi/full/10.1145/3570906

## ADA-GAD 24 AAAIï¼ˆAnomaly-Denoised Autoencoders for Graph Anomaly Detectionï¼‰
https://ojs.aaai.org/index.php/AAAI/article/view/28691








æ„Ÿè§‰æœ€åå†™å‡ºæ¥çš„åº”è¯¥ç±»ä¼¼æ˜¯ Improving fairness for node-level GAE based GAD models via disentanglement learning




# Domain Adaptation
å±äºtransfer learning
2.2 GDAçš„ä¸¤ä¸ªç”¨é€”ï¼šnode/graph classification


- $\mathcal{U}\in\{S,T\}$ Domain
- $P_{\mathcal{U}}(X,Y)$: joint feature and label distribution 
- $\{(x_i,y_i)\}_{i=1}^N$: labeled source data 
- $\{(x_i)\}_{i=1}^M$: unlabeled target data IID sampled from the source and target domain respectively.
- $\phi:\mathcal{X}\rightarrow\mathcal{H}$: a feature encoder
- $g:\mathcal{H}\rightarrow\mathcal{Y}$: a classifier 
- $\epsilon_{\mathcal{U}}(g\circ\phi)=P_{\mathcal{U}}(g(\phi(X))\neq Y)$:classification error in domain $\mathcal{U}$ 
- The objective is to train the model with available data to minimize target error $\epsilon_T(g\circ\phi)$ when predicting target labels.

A popular DA strategy is to learn domain-invariant representation, ensuring similar $P_S(H)$ and $P_T(H)$ and minimizing the source error $\epsilon_S(g\circ\phi)$ to retain classification capability simultaneously ([Zhao et al., 2019](https://arxiv.org/abs/1904.05801)). This is achieved through 

- Feature Shift: $P_S(X|Y) \neq P_T(X|Y)$
  - Assume ndoe feature $x_u$ï¼Œ$u \in \mathcal{V}$ are IID sampled from $P(X|Y)$. Therefore, $P(X = x|Y = y) = \prod_{u \in \mathcal{V}} P(X = x_u|Y = y_u)$

Preassumption on model:

 $X\leftarrow Y \rightarrow A$. Lables are generated first, then A and X are generated.
- Strcture Shift: $P_S(A, Y) \neq P_T(A, Y)$
  - Given joint distribution of $A$, and node labels $P(A, Y)$


Preassumption:
1. Model: $X\leftarrow Y \rightarrow A$. Lables are generated first, then A and X are generated.
2. No Feature Shift: $P_S(X|Y) = P_T(X|Y)$

Structure Shift: $P_{U}(A, Y) = P_{U}(A|Y)P_{U}(Y)$ 
  - Conditional Structure Shift: $P_S(A|Y) \neq P_T(A|Y)$
  - Label Shift: $P_S(Y) \neq P_T(Y)$


Because of the interconnected nature of graph data, the IID is not satisfied for strcture shift, and new alogrithm is needed for solving CSS.

 structure shift is unique to graphs. In contrast to feature shift, which is analogous to non-IID feature shift in non-graph data, structure shift cannot be solved by adapting traditional conditional shift methods. Therefore, we assume feature shift is resolved, i.e., $P_S(X|Y) = P_T


Even if $P_S(H^{(k)}|Y) = P_T(H^{(k)}|Y)$\
CSS may lead to $P_S(H^{(k+1)}|Y) \neq P_T(H^{(k+1)}|Y)$



### GNN
  $$h_u^{(k+1)} = \text{UPT}\left(h_u^{(k)}, \text{AGG}\left(\{\{h_v^{(k)} : v \in \mathcal{N}_u\}\}\right)\right)$$
- $\{\{\cdot\}\}$: Multiset
- $h_u^{(k+1)}$: The updated representation of node $u$ at layer $k+1$.
- $\text{AGG}(\cdot)$: Aggregates message from neighbors.
- $\text{UPT}(\cdot)$: Update function


**Theorem 3.3 (Sufficient conditions for addressing CSS).**

*Given the following assumptions*

- *Conditional Alignment in the previous layer k* 
  - $P_S(H^{(k)}|Y) = P_T(H^{(k)}|Y)$ and $\forall u \in \mathcal{V}_u$, *given* $Y = y_u$, $h_u^{(k)}$ *is independently sampled from* $P_{\mathcal{U}}(H^{(k)}|Y)$.
- *Edge Conditional Independence* 
  - *Given node labels* $y$, *edges mutually independently exist in the graph*.

*If there exists a transformation that modifies the neighborhood of node* $u$: $\mathcal{N}_u \rightarrow \tilde{\mathcal{N}}_u, \forall u \in \mathcal{V}_S$, *such that*
- $P_S(|\tilde{\mathcal{N}}_u||Y_u = i) = P_T(|\tilde{\mathcal{N}}_u||Y_u = i)$ 
-  $P_S(Y_v|Y_u = i, v \in \tilde{\mathcal{N}}_u) = P_T(Y_v|Y_u = i, v \in \mathcal{N}_u), \forall i, v \in \mathcal{Y}$
  
*then*
$P_S(H^{(k+1)}|Y) = P_T(H^{(k+1)}|Y) \text{ is satisfied}$



$\phi_\gamma$: GNN encoding with edge weight adjusting\
$\phi$: GNN encoding without adjusting\
last-layer alignment $P_S(H^{(L)} \mid Y) = P_T(H^{(L)} \mid Y)$can be achieved with $h_S^{(L)} = \phi_\gamma(x_S, A_S)$ and $h_T^{(L)} = \phi(x_T, A_T)$. Note that based on conditional alignment in the distribution of randomly sampled node representations $P_S(H^{(L)} \mid Y) = P_T(H^{(L)} \mid Y)$ and under the conditions in Thm 3.3, $P_S(\mathbf{H}^{(L)} \mid Y) = P_T(\mathbf{H}^{(L)} \mid Y)$ can also be achieved in the matrix form.

$G_s=(A_s,X_s)$\
$G_t=(A_t,X_t)$\
$g \circ \phi_\gamma$\
$g \circ \phi$\
$\hat Y_s$
$\hat Y_t$

Drawback of StrucRW
1. using $w$ instead of $\gamma$ to reweigt $G_s$
2. Rough estimation for $w$
3. Not considering LS


# LLM GAD
## problem
GNNç¼ºç‚¹ï¼š
GNNçš„message passingä¼šå¯¼è‡´Nå’ŒAè¶‹åŒï¼Œé™ä½è¯†åˆ«ç‡
å°½ç®¡åœ¨Heterophilic graphä¸Šæœ‰æ”¹è¿›ï¼Œä½†æ˜¯æ²¡æœ‰æ”¹å˜single node repçš„æœ¬è´¨


## method
(1) Sequence Construction
(2) Coherence-Aware Rep Computation
(3) Anomaly Detection via LLMs

**text coherence**
elvalueated by 
- llama 2: LLM model
- LCD-G: cross-domain coherence eval od sentence
![alt text](2024-07-14-Papers-Collection/image-12.png)
48,509 normal sequences and 7,108 anomalous sequences

### Sequence Construction
Multi sequences for each node by random walk (local)
- starting from the target node (ä»¥target nodeä¸ºä¸­ç‚¹)
- iteratively sampling neighboring nodes and their connecting edges. (h:ç‚¹, e:è¾¹, hzhzhzh)
  
### Coherence-Aware Rep Computation
micro
- AGG info from edges within same sequence

macro
- holistic edge information within the
entire graph

### AD via LLMs




comment:
å¯ä»¥è§£é‡Šä¸€ä¸‹ä¸ºä»€ä¹ˆMulti-AD-MRè¡¨ç°å·®äºERå—
40% labeledï¼Ÿ-->

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/07/14/2024-07-14-Papers-Collection/" data-id="cm9bnagsw001ozc3d54l7fknk" data-title="Papers Collection" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-06-03-Cytoid-AI-Charting" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/06/03/2024-06-03-Cytoid-AI-Charting/" class="article-date">
  <time class="dt-published" datetime="2024-06-03T19:58:56.000Z" itemprop="datePublished">2024-06-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/06/03/2024-06-03-Cytoid-AI-Charting/">Cytoid AI Charting</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ol>
<li>ç›¸å…³è®ºæ–‡å®ç°</li>
</ol>
<p>Survey 1: <a target="_blank" rel="noopener" href="https://www.qbitai.com/2022/03/33133.html">https://www.qbitai.com/2022/03/33133.html</a></p>
<p>1.1 ç°æœ‰æŠ€æœ¯(1)ï¼š100k songs, 44GB data<br><a target="_blank" rel="noopener" href="https://github.com/chrisdonahue/ddc">https://github.com/chrisdonahue/ddc</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.06891.pdf">https://arxiv.org/pdf/1703.06891.pdf</a></p>
<p>1.2 GeneLiveåœ¨DDCåŸºç¡€ä¸Šimproveï¼š<br>ç°æœ‰æŠ€æœ¯2ï¼šGenÃ©Live! Generating Rhythm Actions in Love Live! | Proceedings of the AAAI Conference on Artificial Intelligence<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.12823">https://arxiv.org/abs/2202.12823</a><br><a target="_blank" rel="noopener" href="https://github.com/chrisdonahue/ddc">https://github.com/chrisdonahue/ddc</a></p>
<p>1.3 ç°æœ‰æŠ€æœ¯3ï¼š<br>MuG Diffusion:<br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Sg4y1j7sz/?vd_source=441679270dda23308fe16f3c5602b058">https://www.bilibili.com/video/BV1Sg4y1j7sz/?vd_source=441679270dda23308fe16f3c5602b058</a><br><a target="_blank" rel="noopener" href="https://github.com/Keytoyze/Mug-Diffusion">https://github.com/Keytoyze/Mug-Diffusion</a></p>
<ol start="2">
<li>éŸ³æ¸¸ç›¸å…³ç‰¹å¾</li>
</ol>
<ul>
<li>è¿™æ¬¡ä½¿ç”¨çš„éŸ³æ¸¸ï¼š<br>  <a target="_blank" rel="noopener" href="https://cytoid.io/">https://cytoid.io/</a></li>
<li>æ‰’è°±ç½‘ç«™ï¼š<br>  <a target="_blank" rel="noopener" href="https://cytoid.io/levels">https://cytoid.io/levels</a></li>
<li>æ‰’è°±å·¥å…·ï¼šï¼ˆåº”è¯¥ç”¨ä¸åˆ°ï¼‰<br>  <a target="_blank" rel="noopener" href="https://sites.google.com/site/cytoidcommunity/charting/introduction-cy2unity">https://sites.google.com/site/cytoidcommunity/charting/introduction-cy2unity</a></li>
<li>è°±é¢æ ¼å¼ä»‹ç»ï¼š<br>  <a target="_blank" rel="noopener" href="https://github.com/openmusicgame/omgc">https://github.com/openmusicgame/omgc</a></li>
<li>è¿™ä¸ªæ•™æˆç ”ç©¶å¾ˆå¤šéŸ³ä¹ï¼š<br>  <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=MgzHAPQAAAAJ&hl=en&oi=ao">https://scholar.google.com/citations?user=MgzHAPQAAAAJ&amp;hl=en&amp;oi=ao</a></li>
</ul>
<ol start="3">
<li><p>å‰äººä¸€äº›å·¥ç¨‹ä¸Šç»éªŒï¼ˆæŒ‰ç…§è§„æ¨¡æ’åºï¼‰</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/107010304">https://zhuanlan.zhihu.com/p/107010304</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.mirrorange.com/ai-beatmap-generator-train/">https://www.mirrorange.com/ai-beatmap-generator-train/</a></p>
</li>
<li><p>MuG Diffusion</p>
</li>
</ol>
<p>ChoreoGraph Chart for Musical Game<br>Step Placement: When to place step<br>Step Selection: Which step to place</p>
<p>4.éŸ³æ¸¸æ•°æ®<br><a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1J43x9f8u2lIzaHBolQaZveCv62XQM8Lv">https://drive.google.com/drive/folders/1J43x9f8u2lIzaHBolQaZveCv62XQM8Lv</a></p>
<p>Music library:<br><a target="_blank" rel="noopener" href="https://soundcloud.com/openai_audio/rachmaninoff">https://soundcloud.com/openai_audio/rachmaninoff</a></p>
<h1 id="DDC-Paper-17"><a href="#DDC-Paper-17" class="headerlink" title="DDC Paper 17"></a>DDC Paper 17</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.06891">https://arxiv.org/pdf/1703.06891</a></p>
<p>MIR Music information retrival<br>onset detection:<br>tasks: (learning to choreograph)</p>
<ol>
<li>step placement</li>
<li>step selection</li>
</ol>
<h1 id="GeneLive-23"><a href="#GeneLive-23" class="headerlink" title="GeneLive 23"></a>GeneLive 23</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.12823">https://arxiv.org/abs/2202.12823</a><br>generatiive deep learning</p>
<p>æ–‡ä¸­æåŠ BiLSTM æ¯” Transformerä¹Ÿè®¸æ›´é€‚åˆã€‚<br>two novel techniques: beat guide, multi-sclae conv-stack</p>
<ol>
<li>beat guideå¯»æ‰¾èŠ‚å¥å‹</li>
<li></li>
</ol>
<h1 id="TaikoNation-21"><a href="#TaikoNation-21" class="headerlink" title="TaikoNation 21:"></a>TaikoNation 21:</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.12506">https://arxiv.org/pdf/2107.12506</a><br>LSTM</p>
<h1 id="Other-related-papers"><a href="#Other-related-papers" class="headerlink" title="Other related papers"></a>Other related papers</h1><h2 id="19-via-DL"><a href="#19-via-DL" class="headerlink" title="19 via DL"></a>19 via DL</h2><p><a target="_blank" rel="noopener" href="https://inria.hal.science/hal-03652042v1/document">https://inria.hal.science/hal-03652042v1/document</a></p>
<h2 id="19-aaai-keysounded"><a href="#19-aaai-keysounded" class="headerlink" title="19 aaai keysounded"></a>19 aaai keysounded</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.11170">https://arxiv.org/pdf/1806.11170</a></p>
<h1 id="problems-encountered"><a href="#problems-encountered" class="headerlink" title="problems encountered"></a>problems encountered</h1><p>0.é¦–å…ˆåšçš„æ˜¯å…³äºç»™å®štå’Œdiffculty ï¼Œç”Ÿæˆå¯¹åº”çš„å¯¹åº”çš„ï¼ˆæ˜¯é¢„æµ‹ä¸‹ä¸ªtick æ˜¯å¦æ”¾ç½®keyè¿˜æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªnoteçš„å‡ºç°æ—¶é—´ï¼‰</p>
<ol>
<li><h2 id="combine-of-level-and-others-å¯ä»¥çœ‹ä¸€ä¸‹å‰äººè®ºæ–‡Genelive-æ˜¯æ€ä¹ˆè§£å†³çš„-Hetergenous-variable-in-BiLSTM-a-type-on-RNN"><a href="#combine-of-level-and-others-å¯ä»¥çœ‹ä¸€ä¸‹å‰äººè®ºæ–‡Genelive-æ˜¯æ€ä¹ˆè§£å†³çš„-Hetergenous-variable-in-BiLSTM-a-type-on-RNN" class="headerlink" title="combine of level and others- å¯ä»¥çœ‹ä¸€ä¸‹å‰äººè®ºæ–‡Genelive æ˜¯æ€ä¹ˆè§£å†³çš„- Hetergenous variable in BiLSTM(a type on RNN)"></a>combine of level and others<br>- å¯ä»¥çœ‹ä¸€ä¸‹å‰äººè®ºæ–‡Genelive æ˜¯æ€ä¹ˆè§£å†³çš„<br>- Hetergenous variable in BiLSTM(a type on RNN)</h2></li>
<li>time series</li>
<li>how to pose x.</li>
</ol>
<p>æœ€åå®ç°ï¼Œå°±æ˜¯ï¼Œ å¹¶æ²¡æœ‰å‚è€ƒä»»ä½•æŠ€æœ¯ï¼Œç›´æ¥LSTMå°±ä¸Šäº†ã€‚</p>
<p>Â </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/06/03/2024-06-03-Cytoid-AI-Charting/" data-id="cm9bnagq1001azc3d15e9g9wb" data-title="Cytoid AI Charting" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-05-29-Casual-Inference" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/29/2024-05-29-Casual-Inference/" class="article-date">
  <time class="dt-published" datetime="2024-05-29T17:53:31.000Z" itemprop="datePublished">2024-05-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/29/2024-05-29-Casual-Inference/">Casual Inference</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="ä¸è´å¶æ–¯æœ‰å…³çš„"><a href="#ä¸è´å¶æ–¯æœ‰å…³çš„" class="headerlink" title="ä¸è´å¶æ–¯æœ‰å…³çš„"></a>ä¸è´å¶æ–¯æœ‰å…³çš„</h2><p>é¢‘ç‡æ´¾çš„è§‚ç‚¹<br>ä¸º $p(X|\theta)\mathop{&#x3D;}\limits_{iid}\prod\limits <em>{i&#x3D;1}^{N}p(x</em>{i}|\theta)$ </p>
<p>ä¸ºäº†æ±‚å¸¸é‡ $\theta$ çš„å¤§å°,æœ€å¤§å¯¹æ•°ä¼¼ç„¶MLEçš„æ–¹æ³•ï¼š<br>$\theta_{MLE}&#x3D;\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{&#x3D;}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i&#x3D;1}^{N}\log p(x</em>{i}|\theta)$</p>
<p>è´å¶æ–¯æ´¾çš„è§‚ç‚¹<br>$p(x|\theta)$ ä¸­çš„ $\theta$ ä¸æ˜¯ä¸€ä¸ªå¸¸é‡ã€‚è¿™ä¸ª $\theta$ æ»¡è¶³ä¸€ä¸ªé¢„è®¾çš„å…ˆéªŒçš„åˆ†å¸ƒ $\theta\sim p(\theta)$</p>
<p>$p(\theta|X)&#x3D;\frac{p(X|\theta)\cdot p(\theta)}{p(X)}&#x3D;\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}$</p>
<p>ä¸ºäº†æ±‚ $\theta$ çš„å€¼ï¼Œæˆ‘ä»¬è¦æœ€å¤§åŒ–è¿™ä¸ªå‚æ•°åéªŒMAPï¼š</p>
<p>$\theta_{MAP}&#x3D;\mathop{argmax}\limits _{\theta}p(\theta|X)&#x3D;\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)$</p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p><img src="/2024-05-29-Casual-Inference/image.png" alt="alt text"><br><img src="/2024-05-29-Casual-Inference/image-1.png" alt="alt text"></p>
<p>åŸºç¡€çŸ¥è¯†å’Œæ¦‚å¿µï¼ŒåŒ…æ‹¬d-åˆ†ç¦»<br>doç®—å­<br>åé—¨è°ƒæ•´<br>å‰é—¨è°ƒæ•´<br>é€†æ¦‚ç‡åŠ æƒ<br>åäº‹å®<br>å› æœå…³ç³»å‘ç°ä¸­æœ€åŸºæœ¬çš„ä¸¤ç±»æ–¹æ³•ï¼šåŸºäºç‹¬ç«‹æ€§æµ‹è¯•çš„æ–¹æ³•ï¼Œä»¥åŠé€šè¿‡åŠ æ€§å™ªå£°æ¨¡å‹çš„å½¢å¼åˆ†ææ®‹å·®ä¸é¢„æµ‹è€…ç‹¬ç«‹æ€§å…³ç³»çš„æ–¹æ³•</p>
<h2 id="chap1"><a href="#chap1" class="headerlink" title="chap1"></a>chap1</h2><p>partition, law of total probability<br>summing up its probabilities over all Bi is called marginalizing over $B$, and the resulting probability P(A) is called the marginal probability of $A$.<br>$P(A)&#x3D;P(A,B_1)+P(A,B_2)+Â·Â·Â·+P(A,B_n)$</p>
<p>Def conditional probabilities<br>$P(A|B)&#x3D;P(A,B)âˆ•P(B)$</p>
<p>independence, giving no additional information<br>$P(A,B)&#x3D;P(A)P(B)$</p>
<p>$P(A|B)&#x3D;\frac{P(B|A)P(A)}{P(B)}$</p>
<p>$P(A)&#x3D;P(A|B_1)P(B_1)+P(A|B_2)P(B-2)+Â·Â·Â·+P(A|B_k)P(B_k)$</p>
<p>Sructual Casual Models SCM<br>U exogenous variables, external to the model;</p>
<h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><h2 id="https-www-yuque-com-bystander-wg876-yc5f72-dvgo5bæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥ä»é¢‘ç‡æ´¾å’Œè´å¶æ–¯æ´¾é¢‘ç‡æ´¾çš„æ–¹æ³•ä¸­çš„æ ¸å¿ƒæ˜¯ä¼˜åŒ–é—®é¢˜ï¼Œè€Œåœ¨è´å¶æ–¯æ´¾çš„æ–¹æ³•ä¸­ï¼Œæ ¸å¿ƒæ˜¯ç§¯åˆ†é—®é¢˜ï¼Œä¹Ÿå‘å±•å‡ºæ¥äº†ä¸€ç³»åˆ—çš„ç§¯åˆ†æ–¹æ³•å¦‚å˜åˆ†æ¨æ–­ï¼ŒMCMC-ç­‰-Def-lambda-pi-A-B-pi-is-the-initial-state-distribution-o-t-æ¥è¡¨ç¤ºè§‚æµ‹å˜é‡ï¼Œ-O-ä¸ºè§‚æµ‹åºåˆ—ï¼Œ-V-v-1-v-2-cdots-v-M-è¡¨ç¤ºè§‚æµ‹çš„å€¼åŸŸ-i-t-è¡¨ç¤ºçŠ¶æ€å˜é‡ï¼Œ-I-ä¸ºçŠ¶æ€åºåˆ—ï¼Œ-Q-q-1-q-2-cdots-q-N-è¡¨ç¤ºçŠ¶æ€å˜é‡çš„å€¼åŸŸ-A-a-ij-p-i-t-1-q-j-i-t-q-i-çŠ¶æ€è½¬ç§»çŸ©é˜µ-B-b-j-k-p-o-t-v-k-i-t-q-j-å‘å°„çŸ©é˜µ"><a href="#https-www-yuque-com-bystander-wg876-yc5f72-dvgo5bæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥ä»é¢‘ç‡æ´¾å’Œè´å¶æ–¯æ´¾é¢‘ç‡æ´¾çš„æ–¹æ³•ä¸­çš„æ ¸å¿ƒæ˜¯ä¼˜åŒ–é—®é¢˜ï¼Œè€Œåœ¨è´å¶æ–¯æ´¾çš„æ–¹æ³•ä¸­ï¼Œæ ¸å¿ƒæ˜¯ç§¯åˆ†é—®é¢˜ï¼Œä¹Ÿå‘å±•å‡ºæ¥äº†ä¸€ç³»åˆ—çš„ç§¯åˆ†æ–¹æ³•å¦‚å˜åˆ†æ¨æ–­ï¼ŒMCMC-ç­‰-Def-lambda-pi-A-B-pi-is-the-initial-state-distribution-o-t-æ¥è¡¨ç¤ºè§‚æµ‹å˜é‡ï¼Œ-O-ä¸ºè§‚æµ‹åºåˆ—ï¼Œ-V-v-1-v-2-cdots-v-M-è¡¨ç¤ºè§‚æµ‹çš„å€¼åŸŸ-i-t-è¡¨ç¤ºçŠ¶æ€å˜é‡ï¼Œ-I-ä¸ºçŠ¶æ€åºåˆ—ï¼Œ-Q-q-1-q-2-cdots-q-N-è¡¨ç¤ºçŠ¶æ€å˜é‡çš„å€¼åŸŸ-A-a-ij-p-i-t-1-q-j-i-t-q-i-çŠ¶æ€è½¬ç§»çŸ©é˜µ-B-b-j-k-p-o-t-v-k-i-t-q-j-å‘å°„çŸ©é˜µ" class="headerlink" title="https://www.yuque.com/bystander-wg876/yc5f72/dvgo5bæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥ä»é¢‘ç‡æ´¾å’Œè´å¶æ–¯æ´¾é¢‘ç‡æ´¾çš„æ–¹æ³•ä¸­çš„æ ¸å¿ƒæ˜¯ä¼˜åŒ–é—®é¢˜ï¼Œè€Œåœ¨è´å¶æ–¯æ´¾çš„æ–¹æ³•ä¸­ï¼Œæ ¸å¿ƒæ˜¯ç§¯åˆ†é—®é¢˜ï¼Œä¹Ÿå‘å±•å‡ºæ¥äº†ä¸€ç³»åˆ—çš„ç§¯åˆ†æ–¹æ³•å¦‚å˜åˆ†æ¨æ–­ï¼ŒMCMC ç­‰### Def$\lambda&#x3D;(\pi,A,B)$- $\pi$ is the initial state distribution- $o_t$ æ¥è¡¨ç¤ºè§‚æµ‹å˜é‡ï¼Œ$O$ ä¸ºè§‚æµ‹åºåˆ—ï¼Œ$V&#x3D;{v_1,v_2,\cdots,v_M}$ è¡¨ç¤ºè§‚æµ‹çš„å€¼åŸŸ- $i_t$ è¡¨ç¤ºçŠ¶æ€å˜é‡ï¼Œ$I$ ä¸ºçŠ¶æ€åºåˆ—ï¼Œ$Q&#x3D;{q_1,q_2,\cdots,q_N}$ è¡¨ç¤ºçŠ¶æ€å˜é‡çš„å€¼åŸŸ- $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$çŠ¶æ€è½¬ç§»çŸ©é˜µ- $B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ å‘å°„çŸ©é˜µ"></a><a target="_blank" rel="noopener" href="https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b">https://www.yuque.com/bystander-wg876/yc5f72/dvgo5b</a><br>æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥ä»é¢‘ç‡æ´¾å’Œè´å¶æ–¯æ´¾<br>é¢‘ç‡æ´¾çš„æ–¹æ³•ä¸­çš„æ ¸å¿ƒæ˜¯ä¼˜åŒ–é—®é¢˜ï¼Œè€Œåœ¨è´å¶æ–¯æ´¾çš„æ–¹æ³•ä¸­ï¼Œæ ¸å¿ƒæ˜¯ç§¯åˆ†é—®é¢˜ï¼Œä¹Ÿå‘å±•å‡ºæ¥äº†ä¸€ç³»åˆ—çš„ç§¯åˆ†æ–¹æ³•å¦‚å˜åˆ†æ¨æ–­ï¼ŒMCMC ç­‰<br>### Def<br>$\lambda&#x3D;(\pi,A,B)$<br><br>- $\pi$ is the initial state distribution<br>- $o_t$ æ¥è¡¨ç¤ºè§‚æµ‹å˜é‡ï¼Œ$O$ ä¸ºè§‚æµ‹åºåˆ—ï¼Œ$V&#x3D;{v_1,v_2,\cdots,v_M}$ è¡¨ç¤ºè§‚æµ‹çš„å€¼åŸŸ<br>- $i_t$ è¡¨ç¤ºçŠ¶æ€å˜é‡ï¼Œ$I$ ä¸ºçŠ¶æ€åºåˆ—ï¼Œ$Q&#x3D;{q_1,q_2,\cdots,q_N}$ è¡¨ç¤ºçŠ¶æ€å˜é‡çš„å€¼åŸŸ<br>- $A&#x3D;(a_{ij}&#x3D;p(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i))$çŠ¶æ€è½¬ç§»çŸ©é˜µ<br>- $B&#x3D;(b_j(k)&#x3D;p(o_t&#x3D;v_k|i_t&#x3D;q_j))$ å‘å°„çŸ©é˜µ</h2><h4 id="ä¸¤ä¸ªåŸºæœ¬å‡è®¾"><a href="#ä¸¤ä¸ªåŸºæœ¬å‡è®¾" class="headerlink" title="ä¸¤ä¸ªåŸºæœ¬å‡è®¾"></a>ä¸¤ä¸ªåŸºæœ¬å‡è®¾</h4><ol>
<li>é½æ¬¡ Markov å‡è®¾ï¼ˆæœªæ¥åªä¾èµ–äºå½“å‰ï¼‰ï¼š<br>$p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)&#x3D;p(i_{t+1}|i_t)$</li>
<li>è§‚æµ‹ç‹¬ç«‹å‡è®¾ï¼š<br>$p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)&#x3D;p(o_t|i_t)$</li>
</ol>
<h4 id="ä¸‰ä¸ªåŸºæœ¬é—®é¢˜"><a href="#ä¸‰ä¸ªåŸºæœ¬é—®é¢˜" class="headerlink" title="ä¸‰ä¸ªåŸºæœ¬é—®é¢˜"></a>ä¸‰ä¸ªåŸºæœ¬é—®é¢˜</h4><ol>
<li>Evaluationï¼š$p(O|\lambda)$ï¼ŒForward-Backward </li>
<li>Learningï¼š$\lambda&#x3D;\mathop{argmax}\limits_{\lambda}p(O|\lambda)$ï¼ŒEM ï¼ˆBaum-Welchï¼‰</li>
<li>Decodingï¼š$I&#x3D;\mathop{argmax}\limits_{I}p(I|O,\lambda)$ï¼ŒVierbi ç®—æ³•<br>  a. é¢„æµ‹é—®é¢˜ï¼š$p(i_{t+1}|o_1,o_2,\cdots,o_t)$<br>  b. æ»¤æ³¢é—®é¢˜ï¼š$p(i_t|o_1,o_2,\cdots,o_t)$</li>
</ol>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h2 id="-2"><a href="#-2" class="headerlink" title=""></a></h2><h2 id="æœ‰å…³MCMC"><a href="#æœ‰å…³MCMC" class="headerlink" title="æœ‰å…³MCMC"></a>æœ‰å…³MCMC</h2><h2 id="ä¸€ä¸ªæ¯”è¾ƒåŸºç¡€çš„ä»‹ç»ï¼š-https-zhuanlan-zhihu-com-p-420214359-Abstract-è´å¶æ–¯æ¨æ–­ä¼°è®¡å‚æ•°çš„æ–¹æ³•æ˜¯ï¼šæˆ‘ä»¬å¯ä»¥ç®—å‡ºå‚æ•°-Theta-çš„åˆ†å¸ƒå‡½æ•°-P-Theta-ï¼Œæˆ‘ä»¬ç”¨å‚æ•°åˆ†å¸ƒçš„æ•°å­¦æœŸæœ›ä½œä¸ºå¯¹å‚æ•°çš„ä¼°è®¡å€¼-MCMCçš„ä½œç”¨æ˜¯ï¼šå¯ä»¥å¸®æˆ‘ä»¬ä»ä»»æ„ï¼ˆæ— è®ºæœ‰æ²¡æœ‰è§£æå½¢å¼çš„ï¼‰åˆ†å¸ƒä¸ŠæŠ½æ ·ä¸€æ‰¹æ•°æ®ï¼Œç„¶åç”¨è¿™å †æŠ½æ ·æ•°æ®çš„å‡å€¼ä½œä¸ºå¯¹è¿™ä¸ªåˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡-æˆ‘ä»¬ç”¨MCMCè¿™ç§æ±‚æœŸæœ›çš„æ–¹æ³•æ±‚å‚æ•°åˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡å€¼ï¼Œä»¥æ­¤æ±‚å‡ºå‚æ•°çš„ä¼°è®¡å€¼"><a href="#ä¸€ä¸ªæ¯”è¾ƒåŸºç¡€çš„ä»‹ç»ï¼š-https-zhuanlan-zhihu-com-p-420214359-Abstract-è´å¶æ–¯æ¨æ–­ä¼°è®¡å‚æ•°çš„æ–¹æ³•æ˜¯ï¼šæˆ‘ä»¬å¯ä»¥ç®—å‡ºå‚æ•°-Theta-çš„åˆ†å¸ƒå‡½æ•°-P-Theta-ï¼Œæˆ‘ä»¬ç”¨å‚æ•°åˆ†å¸ƒçš„æ•°å­¦æœŸæœ›ä½œä¸ºå¯¹å‚æ•°çš„ä¼°è®¡å€¼-MCMCçš„ä½œç”¨æ˜¯ï¼šå¯ä»¥å¸®æˆ‘ä»¬ä»ä»»æ„ï¼ˆæ— è®ºæœ‰æ²¡æœ‰è§£æå½¢å¼çš„ï¼‰åˆ†å¸ƒä¸ŠæŠ½æ ·ä¸€æ‰¹æ•°æ®ï¼Œç„¶åç”¨è¿™å †æŠ½æ ·æ•°æ®çš„å‡å€¼ä½œä¸ºå¯¹è¿™ä¸ªåˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡-æˆ‘ä»¬ç”¨MCMCè¿™ç§æ±‚æœŸæœ›çš„æ–¹æ³•æ±‚å‚æ•°åˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡å€¼ï¼Œä»¥æ­¤æ±‚å‡ºå‚æ•°çš„ä¼°è®¡å€¼" class="headerlink" title="ä¸€ä¸ªæ¯”è¾ƒåŸºç¡€çš„ä»‹ç»ï¼š https://zhuanlan.zhihu.com/p/420214359- Abstract:  - è´å¶æ–¯æ¨æ–­ä¼°è®¡å‚æ•°çš„æ–¹æ³•æ˜¯ï¼šæˆ‘ä»¬å¯ä»¥ç®—å‡ºå‚æ•°$\Theta$çš„åˆ†å¸ƒå‡½æ•°$P(\Theta)$ï¼Œæˆ‘ä»¬ç”¨å‚æ•°åˆ†å¸ƒçš„æ•°å­¦æœŸæœ›ä½œä¸ºå¯¹å‚æ•°çš„ä¼°è®¡å€¼  - MCMCçš„ä½œç”¨æ˜¯ï¼šå¯ä»¥å¸®æˆ‘ä»¬ä»ä»»æ„ï¼ˆæ— è®ºæœ‰æ²¡æœ‰è§£æå½¢å¼çš„ï¼‰åˆ†å¸ƒä¸ŠæŠ½æ ·ä¸€æ‰¹æ•°æ®ï¼Œç„¶åç”¨è¿™å †æŠ½æ ·æ•°æ®çš„å‡å€¼ä½œä¸ºå¯¹è¿™ä¸ªåˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡  - æˆ‘ä»¬ç”¨MCMCè¿™ç§æ±‚æœŸæœ›çš„æ–¹æ³•æ±‚å‚æ•°åˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡å€¼ï¼Œä»¥æ­¤æ±‚å‡ºå‚æ•°çš„ä¼°è®¡å€¼"></a>ä¸€ä¸ªæ¯”è¾ƒåŸºç¡€çš„ä»‹ç»ï¼š <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420214359">https://zhuanlan.zhihu.com/p/420214359</a><br>- Abstract:<br>  - è´å¶æ–¯æ¨æ–­ä¼°è®¡å‚æ•°çš„æ–¹æ³•æ˜¯ï¼šæˆ‘ä»¬å¯ä»¥ç®—å‡ºå‚æ•°$\Theta$çš„åˆ†å¸ƒå‡½æ•°$P(\Theta)$ï¼Œæˆ‘ä»¬ç”¨å‚æ•°åˆ†å¸ƒçš„æ•°å­¦æœŸæœ›ä½œä¸ºå¯¹å‚æ•°çš„ä¼°è®¡å€¼<br>  - MCMCçš„ä½œç”¨æ˜¯ï¼šå¯ä»¥å¸®æˆ‘ä»¬ä»ä»»æ„ï¼ˆæ— è®ºæœ‰æ²¡æœ‰è§£æå½¢å¼çš„ï¼‰åˆ†å¸ƒä¸ŠæŠ½æ ·ä¸€æ‰¹æ•°æ®ï¼Œç„¶åç”¨è¿™å †æŠ½æ ·æ•°æ®çš„å‡å€¼ä½œä¸ºå¯¹è¿™ä¸ªåˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡<br>  - æˆ‘ä»¬ç”¨MCMCè¿™ç§æ±‚æœŸæœ›çš„æ–¹æ³•æ±‚å‚æ•°åˆ†å¸ƒæœŸæœ›çš„ä¼°è®¡å€¼ï¼Œä»¥æ­¤æ±‚å‡ºå‚æ•°çš„ä¼°è®¡å€¼</h2><h3 id="1-Monte-Carlo-Sampling"><a href="#1-Monte-Carlo-Sampling" class="headerlink" title="1 Monte Carlo Sampling"></a>1 Monte Carlo Sampling</h3><p>å¦‚æœ$X$æœä»$f(x)$è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘æ€ä¹ˆè·å¾—$E(X)$<br>æœ€å¸¸è§çš„ä¸€ç§Monte Carloæ–¹æ³•çš„ä½¿ç”¨åœºæ™¯å°±æ˜¯ï¼šå¯¹éšæœºå˜é‡è¿›è¡Œå……åˆ†å¤šçš„é‡‡æ ·åï¼Œä½¿ç”¨è¿™äº›é‡‡æ ·çš„å‡å€¼æ¥ä¼°è®¡æ€»ä½“çš„æœŸæœ›</p>
<p>å¯¹äºéšæœºå˜é‡$X$ï¼Œå®ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º$p(x)$ï¼Œå› æ­¤å®ƒçš„æ•°å­¦æœŸæœ›ä¸º<br>$E(x)&#x3D;\int_{-\infty}^{+\infty}xp(x)dx$<br>æˆ‘ä»¬å¯¹äºè¿™ä¸ªéšæœºå˜é‡éšæœºé‡‡æ ·å¾—åˆ°$n$ä¸ªé‡‡æ ·å€¼$x_i$ï¼Œæ ¹æ®å¤§æ•°å®šç†ï¼Œæœ‰<br>$\lim_{n\rightarrow+\infty}{\frac1n\sum_i^n{x_i}}&#x3D;E(X)$</p>
<h3 id="2-Bayes-MCMC"><a href="#2-Bayes-MCMC" class="headerlink" title="2 Bayes &amp; MCMC"></a>2 Bayes &amp; MCMC</h3><h4 id="2-1-Bayes-Model-å‚æ•°-Theta-ï¼ŒObserved-data-D"><a href="#2-1-Bayes-Model-å‚æ•°-Theta-ï¼ŒObserved-data-D" class="headerlink" title="2.1 Bayes Model: å‚æ•°$\Theta$ ï¼ŒObserved data: $D$"></a>2.1 Bayes Model: å‚æ•°$\Theta$ ï¼ŒObserved data: $D$</h4><p>è´å¶æ–¯å…¬å¼ï¼š$P(\Theta|D)&#x3D;\frac1{P(D)}P(D|\Theta)P(\Theta)$</p>
<p>ç”±äº$P(D)$æ˜¯ä¸€ä¸ªæ— å…³ç´§è¦çš„å¸¸æ•°ï¼Œå› æ­¤ä¸Šå¼å¾€å¾€ç›´æ¥å†™æˆä¸€ä¸ªæ­£æ¯”å…³ç³»å¼ï¼š<br>$P(\Theta|D)\propto P(D|\Theta)P(\Theta)$</p>
<p>åœ¨è´å¶æ–¯æ¨æ–­é‡Œï¼š</p>
<ol>
<li>é€šè¿‡$P(\Theta|D)$æ¥å¾—åˆ°$\Theta$çš„ä¼°è®¡å€¼</li>
<li>æ¨¡å‹ç»™å‡º$P(D|\Theta)$ï¼Œ å³likelihood$P(D|\Theta)$</li>
<li>è¿˜å¯ä»¥é€šè¿‡$P(\Theta)$æ¥å¯¹å‚æ•°çš„åˆ†å¸ƒæƒ…å†µåšä¸€äº›å…ˆéªŒçš„çŒœæµ‹ã€‚ï¼ˆå¦‚æœä½ ä»€ä¹ˆéƒ½ä¸çŸ¥é“ï¼Œ$P(\Theta)$è‡ªç„¶å¯ä»¥çŒœä¸€ä¸ªå‡åŒ€åˆ†å¸ƒï¼‰</li>
</ol>
<h4 id="2-2-é€šè¿‡åéªŒæ¦‚ç‡-P-Theta-D-è·å–å‚æ•°-Theta-çš„ä¼°è®¡å€¼"><a href="#2-2-é€šè¿‡åéªŒæ¦‚ç‡-P-Theta-D-è·å–å‚æ•°-Theta-çš„ä¼°è®¡å€¼" class="headerlink" title="2.2 é€šè¿‡åéªŒæ¦‚ç‡$P(\Theta|D)$è·å–å‚æ•°$\Theta$çš„ä¼°è®¡å€¼"></a>2.2 é€šè¿‡åéªŒæ¦‚ç‡$P(\Theta|D)$è·å–å‚æ•°$\Theta$çš„ä¼°è®¡å€¼</h4><p>æƒ³æ³•ï¼šä¼—æ•°æˆ–è€…æœŸæœ›ä½œä¸º<br>ä¼°è®¡å€¼</p>
<ol>
<li>ä¼—æ•°ï¼š$\hat\Theta&#x3D;\arg\max_\Theta{P(\Theta|D)}$</li>
<li>æœŸæœ›ï¼š$\hat\Theta&#x3D;\int_\Theta\Theta P(\Theta|D)d\Theta$</li>
</ol>
<p>MCMCå°±æ˜¯æ•™æˆ‘ä»¬æ€ä¹ˆåœ¨ä¸€ä¸ªæ²¡æœ‰è§£æå½¢å¼çš„æ•°æ®ä¸Šã€ŒæŠ½æ ·å‡ ä¸ªæ•°æ®ç®—å¹³å‡å€¼ã€çš„æ–¹æ³•</p>
<h3 id="3-Sampling-é‡‡æ ·"><a href="#3-Sampling-é‡‡æ ·" class="headerlink" title="3 Sampling é‡‡æ ·"></a>3 Sampling é‡‡æ ·</h3><ol>
<li>Uniform</li>
<li>Gaussian:<br>Given $U_1,U_2$<br>  $$<br>   Z_0 &#x3D; \sqrt{-2 \ln U_1} \cos(2\pi U_2)<br>   $$<br>   $$<br>   Z_1 &#x3D; \sqrt{-2 \ln U_1} \sin(2\pi U_2)<br>   $$</li>
<li>Reject-Accept: ç”¨äºå¯¹å¾ˆä¸è§„åˆ™çš„$f(x)$é‡‡æ ·ã€‚å…·ä½“ç»†èŠ‚æ²¡çœ‹</li>
<li></li>
</ol>
<h3 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov Chain"></a>Markov Chain</h3><p>ä¸€ä¸ªå¯¹é©¬å°”å¯å¤«çŠ¶æ€è®²çš„æ¯”è¾ƒè¯¦ç»†çš„æ–‡ç« ï¼š<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250146007">https://zhuanlan.zhihu.com/p/250146007</a></p>
<h4 id="Def"><a href="#Def" class="headerlink" title="Def"></a>Def</h4><p>è½¬ç§»æ¦‚ç‡çŸ©é˜µï¼š</p>
<p>$P&#x3D;\begin{bmatrix}p_{11} &amp; p_{12} &amp;p_{13} \ p_{21} &amp; p_{22} &amp;p_{23} \ p_{31} &amp; p_{32} &amp;p_{33}\end{bmatrix}$ </p>
<p>å…¶ä¸­ $p_{ij}&#x3D;P(X_{t}&#x3D;i|X_{t-1}&#x3D;j)$ ã€‚</p>
<p>å®šä¹‰ï¼šé©¬å°”ç§‘å¤«é“¾åœ¨ $t$ æ—¶åˆ»çš„æ¦‚ç‡åˆ†å¸ƒç§°ä¸º $t$ æ—¶åˆ»çš„çŠ¶æ€åˆ†å¸ƒï¼š</p>
<p>$\pi (t)&#x3D;\begin{bmatrix}\pi_{1}(t) \ \pi_{2}(t) \ \pi_{3}(t)\end{bmatrix}$ </p>
<p>å…¶ä¸­  $\pi_{i} (t)&#x3D;P(X_{t}&#x3D;i),i&#x3D;1,2,â€¦$ ã€‚</p>
<h4 id="æ€§è´¨"><a href="#æ€§è´¨" class="headerlink" title="æ€§è´¨"></a>æ€§è´¨</h4><ol>
<li><p>å®šç†ï¼šç»™å®šä¸€ä¸ªé©¬å°”ç§‘å¤«é“¾ $X&#x3D;\left{ X_0,X_1,â€¦,X_t,â€¦ \right}$ ï¼Œ $t$ æ—¶åˆ»çš„çŠ¶æ€åˆ†å¸ƒï¼š<br> $\pi&#x3D;(\pi_1,\pi_2,â€¦)$ æ˜¯ $X$ çš„å¹³ç¨³åˆ†å¸ƒçš„æ¡ä»¶æ˜¯ $\pi&#x3D;(\pi_1,\pi_2,â€¦)$ æ˜¯ä¸‹åˆ—æ–¹ç¨‹ç»„çš„è§£ï¼š<br> $x_{i}&#x3D;\sum_{j}{p_{ij}x_j},i&#x3D;1,2,â€¦$<br> $x_i\geq0,i&#x3D;1,2,â€¦$<br> $\sum_{i}{x_{i}&#x3D;1}$</p>
</li>
<li></li>
</ol>
<h3 id="MCMCå…·ä½“ç»†èŠ‚"><a href="#MCMCå…·ä½“ç»†èŠ‚" class="headerlink" title="MCMCå…·ä½“ç»†èŠ‚"></a>MCMCå…·ä½“ç»†èŠ‚</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/253784711">https://zhuanlan.zhihu.com/p/253784711</a></p>
<h2 id="Conditional-Random-Field-CRF"><a href="#Conditional-Random-Field-CRF" class="headerlink" title="Conditional Random Field(CRF)"></a>Conditional Random Field(CRF)</h2><ul>
<li>HMM ç”Ÿæˆæ¨¡å‹</li>
<li>MEMM Maximum Entropy Markov Model</li>
</ul>
<p><img src="/2024-05-29-Casual-Inference/image-4.png" alt="alt text"></p>
<ul>
<li>HMM:<br>$$ P(\mathbf{X}, \mathbf{Y} | \lambda) &#x3D; P(\mathbf{Y} | \lambda) P(\mathbf{X} | \mathbf{Y}, \lambda) $$</li>
<li>MEMM:<br>$$ P(y_t | y_{t-1}, x_t) $$</li>
<li>CRF:<br>$$ P(\mathbf{Y} | \mathbf{X}, \lambda) &#x3D; \frac{1}{Z(\mathbf{X})} \exp \left( \sum_{t&#x3D;1}^{T} \lambda_t f(y_t, y_{t-1}, \mathbf{X}, t) \right) $$</li>
</ul>
<h2 id="æ¦‚ç‡å›¾æ¨¡å‹"><a href="#æ¦‚ç‡å›¾æ¨¡å‹" class="headerlink" title="æ¦‚ç‡å›¾æ¨¡å‹"></a>æ¦‚ç‡å›¾æ¨¡å‹</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1BW41117xo/?spm_id_from=333.999.0.0&vd_source=441679270dda23308fe16f3c5602b058">https://www.bilibili.com/video/BV1BW41117xo/?spm_id_from=333.999.0.0&amp;vd_source=441679270dda23308fe16f3c5602b058</a></p>
<h3 id="æ¦‚ç‡å’Œå›¾"><a href="#æ¦‚ç‡å’Œå›¾" class="headerlink" title="æ¦‚ç‡å’Œå›¾"></a>æ¦‚ç‡å’Œå›¾</h3><p>æ¦‚ç‡å›¾</p>
<ul>
<li>è¡¨ç¤º Representation<ul>
<li>æœ‰å‘å›¾ Beyesian Netowrk</li>
<li>æ— å‘å›¾</li>
<li>é«˜æ–¯å›¾ï¼ˆè¿ç»­çš„éšæœºå˜é‡ï¼‰</li>
</ul>
</li>
<li>æ¨æ–­ Inference<ul>
<li>ç²¾ç¡®æ¨æ–­</li>
<li>è¿‘ä¼¼æ¨æ–­<ul>
<li>ç¡®å®šæ€§æ¨æ–­ï¼ˆå˜åˆ†ï¼‰</li>
<li>éšæœºè¿‘ä¼¼ MCMC</li>
</ul>
</li>
</ul>
</li>
<li>å­¦ä¹ <ul>
<li>å‚æ•°å­¦ä¹ <ul>
<li>å®Œå¤‡æ•°æ®</li>
<li>éšå˜é‡</li>
</ul>
</li>
<li>ç»“æ„å­¦ä¹ </li>
</ul>
</li>
</ul>
<p>é«˜ç»´éšæœºå˜é‡</p>
<ul>
<li>sum: $P(x_1) &#x3D; \int P(x_1, x_2)dx_2$</li>
<li>product: $P(x_1|x_2) &#x3D; P(x_1|x_2)P(x_2)&#x3D;P(x_2|x_1)P(x_1)$</li>
</ul>
<p>é“¾å¼æ³•åˆ™</p>
<ul>
<li>$$P(X_1,X_2,â€¦,X_n)&#x3D;P(X_1)P(X_2|X_1)P(X_3|X_2,X_1)Â·Â·Â·P(X_n|X_{n-1},X_{n-2},â€¦,X_1)$$</li>
</ul>
<p>å…¨æ¦‚ç‡å…¬å¼</p>
<ul>
<li>$P(X_i)&#x3D;\sum_{j}{P(X_i,X_j)}&#x3D;\sum_{j}{P(X_i|X_j)P(X_j)}$</li>
</ul>
<p>è´å¶æ–¯å…¬å¼</p>
<ul>
<li>$P(X_i|X_j)&#x3D;\frac{P(X_i,X_j)}{P(X_j)}&#x3D;\frac{P(X_i|X_j)P(X_j)}{P(X_j)}$</li>
</ul>
<p>å›°å¢ƒï¼š<br>ç»´åº¦é«˜$P(X_1,X_2,â€¦,X_n)$è®¡ç®—å¤æ‚</p>
<ul>
<li>1.å‡è®¾$X_i$ç›¸äº’ç‹¬ç«‹:<ul>
<li>$P(X_1,X_2,â€¦,X_n)&#x3D;\prod_{i}{P(X_i)}$</li>
</ul>
</li>
<li>2.Markov Property(HMMé½æ¬¡é©¬å°”å¯å¤«):<ul>
<li>$x_j\perp x_i+1|x_i, j&lt;i$ </li>
<li>$P(X_1, X_2,â€¦,X_n)&#x3D;P(X_1)P(X_2|X_1)P(X_3|X_2,X_1)Â·Â·Â·P(X_n|X_{n-1},X_{n-2},â€¦,X_1)$</li>
</ul>
</li>
<li>3.å‡è®¾$X_i$æ¡ä»¶ç‹¬ç«‹: <ul>
<li>$x_a\perp x_b|x_c$</li>
<li>$P(X_1,X_2,â€¦,X_n)&#x3D;\prod_{i}{P(X_i|X_{i-1})}$</li>
</ul>
</li>
</ul>
<h4 id="æ¦‚ç‡è¡¥å……çŸ¥è¯†"><a href="#æ¦‚ç‡è¡¥å……çŸ¥è¯†" class="headerlink" title="æ¦‚ç‡è¡¥å……çŸ¥è¯†"></a>æ¦‚ç‡è¡¥å……çŸ¥è¯†</h4><p>æŒ‡æ•°æ—åˆ†å¸ƒ</p>
<ul>
<li>å……åˆ†ç»Ÿè®¡é‡$\phi(x)$</li>
<li>å…±è½­</li>
<li>æœ€å¤§ç†µ</li>
<li>å¹¿ä¹‰çº¿æ€§æ¨¡å‹</li>
<li>æ¦‚ç‡å›¾æ¨¡å‹</li>
<li>å˜åˆ†æ¨æ–­</li>
</ul>
<p>å……åˆ†ç»Ÿè®¡é‡</p>
<ul>
<li>$P(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))$<ul>
<li>$h(x)$: base measure</li>
<li>$\eta$: parameter å‚æ•°å‘é‡</li>
<li>$\phi(x)$: feature function</li>
<li>$A(\eta)$ï¼š log partition function é…åˆ†å‡½æ•°</li>
</ul>
</li>
<li>$P(x|\theta)&#x3D;\frac{1}{z}\hat P(x|\theta)$<ul>
<li>$z&#x3D;\int \hat P(x|\theta)dx$ å½’ä¸€åŒ–å› å­</li>
</ul>
</li>
<li>$P(x|\eta)&#x3D;h(x)\exp(\eta^T\phi(x)-A(\eta))$<ul>
<li>$&#x3D;\frac{1}{exp(A(\eta))}h(x)exp(\eta^T\phi(x))$</li>
<li>$&#x3D;\frac{1}{z}\hat P(x|\eta)$<ul>
<li>$\hat P(x|\eta)&#x3D;h(x)exp(\eta^T\phi(x))$</li>
<li>$z&#x3D;exp(A(\eta))$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>å…±è½­, å…ˆåéªŒåˆ†å¸ƒåŒç»„æ–¹ä¾¿è®¡ç®—</p>
<ul>
<li>$P(\theta|x)&#x3D;\frac{P(x|\theta)P(\theta)}{P(x)}$</li>
<li>$P(\theta|x)$å’Œ$P(x|\theta)$å±äºåŒä¸€ä¸ªæŒ‡æ•°æ—</li>
<li>$P(\theta|x)$çš„å‚æ•°æ˜¯$P(\theta|x)$çš„å‚æ•°çš„å‡½æ•°</li>
</ul>
<p>å…ˆéªŒ</p>
<ul>
<li>å…±è½­ - è®¡ç®—æ–¹ä¾¿</li>
<li>æœ€å¤§ç†µ æ— ä¿¡æ¯å…ˆéªŒ</li>
<li>Jerrif</li>
</ul>
<p>å¹¿ä¹‰çº¿æ€§æ¨¡å‹</p>
<ul>
<li>çº¿æ€§ç»„åˆ $w^Tx$</li>
<li>Link funciton -&gt;aactivation function</li>
<li>æŒ‡æ•°åˆ†å¸ƒ $y|x\sim$æŒ‡æ•°ç»„åˆ†å¸ƒï¼ˆ$Bernulli, Poisson, N(\mu, \Sigma)$ï¼‰</li>
</ul>
<h5 id="Gaussian"><a href="#Gaussian" class="headerlink" title="Gaussian"></a>Gaussian</h5><ul>
<li>$P(x|\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}|\Sigma|^{1&#x2F;2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$</li>
<li>$\eta&#x3D;<br>\left(!<br>  \begin{array}{c}<br>\eta_1 &#x3D;\frac{\mu}{\sigma^2}\<br>\eta_2&#x3D;-\frac{1}{\sigma^2}<br>  \end{array}<br>  !\right)$</li>
<li>$\phi(x)&#x3D;\left(!<br>  \begin{array}{c}<br>x\<br>x^2<br>  \end{array}<br>  !\right)$</li>
<li>$A(\eta)&#x3D;-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\ln(-\frac{\pi}{\eta_2})$</li>
</ul>
<h4 id="-3"><a href="#-3" class="headerlink" title=""></a></h4><ul>
<li>$P(x | \eta) &#x3D; h(x) \exp (\eta^T \phi(x) - A(\eta))$<ul>
<li>$\eta$: å‚æ•° (parameter)</li>
<li>$\phi(x)$: å……åˆ†ç»Ÿè®¡é‡ (sufficient statistics)</li>
<li>$A(\eta)$: å¯¹æ•°é…åˆ†å‡½æ•° (log partition function)</li>
</ul>
</li>
</ul>
<ol>
<li>$Aâ€™(\eta) &#x3D; \mathbb{E}_{P(x|\eta)}[\phi(x)]$</li>
<li>$Aâ€™â€™(\eta) &#x3D; \text{Var}[\phi(x)]$</li>
<li>$A(\eta)$ æ˜¯å‡¸å‡½æ•° (convex function)</li>
</ol>
<p>å¯¹è¿™ä¸ªå‡½æ•°æ±‚å¯¼</p>
<ul>
<li>$\exp(A(\eta)) &#x3D; \int h(x) \exp(\eta^T \phi(x)) , dx$</li>
<li>$Aâ€™(\eta) &#x3D; \frac{\partial}{\partial \eta} \log \left( \int h(x) \exp(\eta^T \phi(x)) , dx \right)$</li>
<li>$Aâ€™(\eta) &#x3D; \int \frac{h(x) \exp(\eta^T \phi(x)) \phi(x) , dx}{\exp(A(\eta))}$</li>
<li>$Aâ€™(\eta) &#x3D; \int P(x | \eta) \phi(x) , dx &#x3D; \mathbb{E}_{P(x|\eta)}[\phi(x)]$</li>
</ul>
<h5 id="MLE"><a href="#MLE" class="headerlink" title="MLE"></a>MLE</h5><p>$D&#x3D;{x_1, \cdots, x_N}$</p>
<p>$\eta_{MLE}&#x3D;\text{argmax  } log(P(D|\eta))$</p>
<ul>
<li>$&#x3D;\sum_{i&#x3D;1}^N\text{argmax  }log\cdot h(x_i)+(\eta^T \phi(x_i) - A(\eta))$</li>
<li><ul>
<li>$&#x3D;\sum_{i&#x3D;1}^N\text{argmax  }\eta^T \phi(x_i) - A(\eta)$</li>
</ul>
</li>
</ul>
<p>set $\frac{\partial \eta_{MLE}}{\partial \eta}&#x3D;0$</p>
<ul>
<li>$\sum \phi(x_i)-NAâ€™(\eta)&#x3D;0$</li>
</ul>
<h5 id="Entorpy-æœ€å¤§ç†µ"><a href="#Entorpy-æœ€å¤§ç†µ" class="headerlink" title="Entorpy æœ€å¤§ç†µ"></a>Entorpy æœ€å¤§ç†µ</h5><p>ä¿¡æ¯ç†µ $-log\ p$<br>ç†µï¼Œï¼ˆå¯¹å¯èƒ½æ€§çš„è¡¡é‡ï¼‰</p>
<ul>
<li><p>$E_{p(x)}[-log\ p]&#x3D;\int -p(x)log\ p(x)dx&#x3D; \sum -p(x)log\ p(x)$</p>
</li>
<li><p>$\hat p_i&#x3D;\text{argmax } H(p)$ </p>
</li>
<li><p>æ‹‰æ ¼æœ—æ—¥$\mathcal{L}(p, \lambda)&#x3D;\sum p_ilog\ p_i$</p>
<ul>
<li>$\frac{\partial \mathcal{L}}{\partial p_i}&#x3D;log\ p_i+1-\lambda$</li>
<li>$\hat p_i&#x3D;exp(\lambda-1)&#x3D;1&#x2F;k$</li>
</ul>
</li>
</ul>
<h5 id="ç»éªŒåˆ†å¸ƒ"><a href="#ç»éªŒåˆ†å¸ƒ" class="headerlink" title="ç»éªŒåˆ†å¸ƒ"></a>ç»éªŒåˆ†å¸ƒ</h5><p>$Data &#x3D; {x_1, \cdots, x_N}$</p>
<ul>
<li><strong>é¢‘ç‡åˆ†å¸ƒ</strong>ï¼š $P(x_1, x_2, \ldots, x_n) \approx \hat{P}(x) &#x3D; \frac{\text{count}(x)}{N}$</li>
<li><strong>ç»éªŒæœŸæœ›</strong>ï¼š $\mathbb{E}<em>p[f(x)] &#x3D; \Delta \approx \frac{1}{N} \sum</em>{i&#x3D;1}^N f(x_i)$</li>
</ul>
<p>æœ€å¤§ç†µé—®é¢˜çš„æ±‚è§£<br>$$<br>\begin{aligned}<br>\min_{p(x)} &amp; \sum_x p(x) \log p(x) \<br>\text{subject to} &amp; \sum_x p(x) &#x3D; 1 \<br>&amp; \mathbb{E}<em>p[f(x)] &#x3D; \mathbb{E}</em>{\hat{p}}[f(x)] &#x3D; \Delta<br>\end{aligned}<br>$$</p>
<p>æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ±‚è§£<br>$$<br>\begin{aligned}<br>L(p, \lambda, \eta) &amp;&#x3D; \sum_x p(x) \log p(x) + \lambda (1 - \sum_x p(x)) + \eta (\Delta - \sum_x p(x) f(x)) \<br>\frac{\partial}{\partial p(x)} &amp;&#x3D; \log p(x) + 1 - \lambda_0 - \lambda f(x) &#x3D; 0 \<br>p(x) &amp;&#x3D; \exp(\lambda_1 f(x) + \lambda_0 - 1) \<br>&amp;&#x3D; \frac{\exp(\eta^T f(x))}{Z(\eta)}<br>\end{aligned}<br>$$</p>
<h3 id="è´å¶æ–¯ç½‘ç»œ"><a href="#è´å¶æ–¯ç½‘ç»œ" class="headerlink" title="è´å¶æ–¯ç½‘ç»œ"></a>è´å¶æ–¯ç½‘ç»œ</h3><p>æœ‰å‘æ— ç¯å›¾<br>å› å­åˆ†è§£</p>
<ul>
<li>$P(X_1,X_2,â€¦,X_n)&#x3D;\prod_{i}{P(X_i|Pa(X_i))}$</li>
<li>$P(X_i|Pa(X_i))$æ˜¯å±€éƒ¨æ¦‚ç‡åˆ†å¸ƒ</li>
<li>$Pa(X_i)$æ˜¯$X_i$çš„çˆ¶èŠ‚ç‚¹é›†åˆ</li>
</ul>
<h4 id="ä¸‰ç§æ¨¡å‹"><a href="#ä¸‰ç§æ¨¡å‹" class="headerlink" title="ä¸‰ç§æ¨¡å‹"></a>ä¸‰ç§æ¨¡å‹</h4><h5 id="tail-to-tail"><a href="#tail-to-tail" class="headerlink" title="tail-to-tail"></a>tail-to-tail</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a--&gt; b</span><br><span class="line">  a--&gt; c</span><br></pre></td></tr></table></figure>
<p>å› å­åˆ†è§£ï¼š</p>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c|a)$</li>
</ul>
<p>é“¾å¼æ³•åˆ™ï¼š</p>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c|a, b)$</li>
</ul>
<p>$\implies P(c|a)&#x3D;P(c|a, b)\implies c\perp b |a$<br>è‹¥$b$ è¢«è§‚æµ‹åˆ™è·¯å¾„è¢«é˜»å¡ï¼š </p>
<h5 id="head-to-tail"><a href="#head-to-tail" class="headerlink" title="head-to-tail"></a>head-to-tail</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a --&gt;b </span><br><span class="line">  b --&gt;c </span><br></pre></td></tr></table></figure>
<p>$a\perp c |b$<br>è‹¥$b$ è¢«è§‚æµ‹åˆ™è·¯å¾„è¢«é˜»å¡ï¼š </p>
<h5 id="head-to-head"><a href="#head-to-head" class="headerlink" title="head-to-head"></a>head-to-head</h5>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  b--&gt; c</span><br><span class="line">  a--&gt; c</span><br></pre></td></tr></table></figure>
<ul>
<li>$P(a, b, c)&#x3D;P(a)P(b)P(c|a, b)$</li>
<li>$P(a, b, c)&#x3D;P(a)P(b|a)P(c | a, b)$</li>
</ul>
<p>$\implies P(b)&#x3D;P(b|a)\implies a\perp b$<br>è‹¥$b$ è¢«è§‚æµ‹åˆ™è·¯å¾„è¢«è¿é€šï¼š</p>
<h4 id="D-seperation"><a href="#D-seperation" class="headerlink" title="D-seperation"></a>D-seperation</h4>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  a  --&gt; b1</span><br><span class="line">  a  --&gt; b2</span><br><span class="line">  a  --&gt; b*</span><br><span class="line">  b1 --&gt; c</span><br><span class="line">  b2 --&gt; c</span><br><span class="line">  c  --&gt; b*</span><br></pre></td></tr></table></figure>
<ul>
<li>å¦‚æœ$b1, b2\in B$è¢«è§‚æµ‹äº†<br>ï¼Œé‚£ä¹ˆ$a$å’Œ$c$è¢«é˜»æ–­ï¼Œ</li>
<li>ä½†æ˜¯$b*$æ²¡æœ‰è¢«è§‚æµ‹åˆ°,ä¸”$b*$çš„åç»­èŠ‚ç‚¹éƒ½ä¸åœ¨$b$ä¸­<br>$a\perp c|b$</li>
</ul>
<p><img src="/2024-05-29-Casual-Inference/image-2.png" alt="alt text"><br>é©¬å°”å¯å¤«æ¯¯(Markov Blanket)</p>
<ul>
<li>$x_{pa(i)}$ï¼š$x_i$çš„çˆ¶èŠ‚ç‚¹</li>
<li>$x_{child(i)}$: childs of $x_i$</li>
<li>$x_{pa(child(i))}$: parent of $x_{child(i)}$</li>
<li>$x_{-i}&#x3D;x&#x2F;x_i$ è¡¨ç¤ºé™¤äº† $x_i$ ä»¥å¤–çš„æ‰€æœ‰å˜é‡ã€‚<ul>
<li>å’Œ$x$æœ‰å…³:$\Delta$<ul>
<li>$P(x_i|x_{Pa(i)})&#x3D;f(\bar \Delta)$</li>
</ul>
</li>
<li>å’Œ$x$æ— å…³:$\bar \Delta$</li>
</ul>
</li>
</ul>
<p>é©¬å°”å¯å¤«æ¯¯çš„ä½œç”¨æ˜¯åœ¨ç»™å®šé©¬å°”å¯å¤«æ¯¯å†…æ‰€æœ‰èŠ‚ç‚¹çš„æƒ…å†µä¸‹ï¼Œ$x_i$ ä¸ç½‘ç»œä¸­å…¶ä»–èŠ‚ç‚¹æ¡ä»¶ç‹¬ç«‹ã€‚</p>
<p>$$<br>P(x_i | x_{-i}) &#x3D; \frac{P(x_i, x_{-i})}{P(x_{-i})} &#x3D; \frac{P(x)}{\int_{x_i} P(x_{-i})} &#x3D; \frac{ \int_{x_i} P(x) , dx_i }{ \int_{x_i} P(x_j | x_{\text{pa}(j)}) , dx_i }<br>$$</p>
<p>$$<br>P(x_{\text{child}(i)} | x_i, x_{\text{Parent}(\text{Child}(i))})<br>$$</p>
<h4 id="è´å¶æ–¯ç½‘ç»œ-Beyesian-Network"><a href="#è´å¶æ–¯ç½‘ç»œ-Beyesian-Network" class="headerlink" title="è´å¶æ–¯ç½‘ç»œ Beyesian Network"></a>è´å¶æ–¯ç½‘ç»œ Beyesian Network</h4><p>NB<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  y--&gt; x1 </span><br><span class="line">  y--&gt; xp</span><br></pre></td></tr></table></figure></p>
<p>GMM<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  z--&gt; x</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>Beyesian Network</p>
<ul>
<li>å•ä¸€ï¼š Naive Bayes<ul>
<li>pç»´ï¼š $P(x|y)&#x3D;\prod^p_{i&#x3D;1}P(x_i|y&#x3D;1)$</li>
<li>$x_1\perp x_2|y$</li>
</ul>
</li>
<li>æ··åˆï¼šGMM<ul>
<li>Z discrete, z&#x3D;1,2,3,4</li>
</ul>
</li>
<li>æ—¶é—´<ul>
<li>Markov Chain</li>
<li>Gaussian Process(æ— é™ç»´åˆ†å¸ƒ)</li>
</ul>
</li>
<li>è¿ç»­ï¼š Gaussian Network</li>
</ul>
<p>åŠ¨æ€æ¨¡å‹</p>
<ul>
<li>HMM ç¦»æ•£</li>
<li>LDS Kalman Filter è¿ç»­çº¿æ€§</li>
<li>Particle filter éçº¿æ€§éé«˜æ–¯</li>
</ul>
<h3 id="Markov-Network"><a href="#Markov-Network" class="headerlink" title="Markov Network"></a>Markov Network</h3><p>æ¡ä»¶ç‹¬ç«‹æ€§</p>
<ul>
<li>å…¨å±€ Global Markov Property<ul>
<li>$X_A \perp X_C \mid X_B$</li>
<li>å¦‚æœé›†åˆ $X_A$ å’Œ $X_C$ è¢«é›†åˆ $X_B$ åˆ†éš”å¼€ï¼Œé‚£ä¹ˆ $X_A$ å’Œ $X_C$ æ˜¯æ¡ä»¶ç‹¬ç«‹çš„</li>
</ul>
</li>
<li>å±€éƒ¨ Local Markov Property<ul>
<li>$a \perp {Non-Neighbour} \mid {Neighbour}$</li>
<li>{ }ï¼šé›†åˆ</li>
</ul>
</li>
<li>æˆå¯¹ Pairwise Markov Property<ul>
<li>$x_i \perp x_j \mid x_{-ij}$</li>
<li>å¦‚æœèŠ‚ç‚¹ $x_i$ å’Œ $x_j$ ç›´æ¥ç›¸è¿ï¼Œé‚£ä¹ˆåœ¨ç»™å®šå…¶ä»–æ‰€æœ‰èŠ‚ç‚¹çš„æƒ…å†µä¸‹ï¼Œ$x_i$ å’Œ $x_j$ æ˜¯æ¡ä»¶ç‹¬ç«‹çš„</li>
</ul>
</li>
</ul>
<h4 id="Factorization"><a href="#Factorization" class="headerlink" title="Factorization"></a>Factorization</h4><p>å›¢: Clique<br>æœ€å¤§å›¢: Maximal Clique<br>$$<br>P(X) &#x3D; \frac{1}{Z} \prod_{i&#x3D;1}^K \psi(X_{C_i})<br>$$</p>
<ul>
<li>$c_i$ æœ€å¤§å›¢</li>
<li>$x_{c_i}$: æœ€å¤§å›¢éšæœºå˜é‡é›†åˆ</li>
<li>$\psi(x_{c_i})$: åŠ¿å‡½æ•°</li>
<li>$Z$:<ul>
<li>$Z &#x3D; \sum_{X} \prod_{i&#x3D;1}^K \psi(X_{C_i})$</li>
</ul>
</li>
<li>$\psi(X_{C_i})$ æ˜¯å®šä¹‰åœ¨æœ€å¤§å›¢ $C_i$ ä¸Šçš„å› å­å‡½æ•°</li>
</ul>
<p>ä½•å¼ƒç–—<br><img src="/2024-05-29-Casual-Inference/image-3.png" alt="alt text"></p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li><p>è”åˆæ¦‚ç‡ Joint Probability</p>
<ul>
<li>$P(X) &#x3D; P(x_1, x_2, \ldots, x_p)$</li>
</ul>
</li>
<li><p>è¾¹ç¼˜æ¦‚ç‡ Marginal Probability</p>
<ul>
<li>$P(x) &#x3D; \sum_{x_j} P(x_j)$</li>
</ul>
</li>
<li><p>æ¡ä»¶æ¦‚ç‡ Conditional Probability</p>
<ul>
<li>$P(x_i | x_j)$, å…¶ä¸­$x_j &#x3D; {x \backslash x_i}$</li>
</ul>
</li>
<li><p>æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡ MAP Inference</p>
<ul>
<li>$\hat{z} &#x3D; \arg \max_z P(z | x) \propto \arg \max_z P(z, x)$</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>ç²¾ç¡®æ¨æ–­ Exact Inference</p>
<ul>
<li>Variable Elimination (VE)</li>
<li>Belief Propagation (BP) â†’ Sum-Product Algorithm (æ±‚å’Œ-ä¹˜ç§¯ç®—æ³•)</li>
<li>Junction Tree Algorithm (æ ‘å½¢ç®—æ³•)</li>
</ul>
</li>
<li><p>è¿‘ä¼¼æ¨æ–­ Approximate Inference</p>
<ul>
<li>Loop Belief Propagation (å¾ªç¯ä¿¡å¿µä¼ æ’­)</li>
<li>Monte Carlo Inference: Importance Sampling, MCMC (è’™ç‰¹å¡ç½—æ¨æ–­ï¼šé‡è¦æ€§é‡‡æ ·ï¼ŒMCMC)</li>
<li>Variational Inference (å˜åˆ†æ¨æ–­)</li>
</ul>
</li>
</ul>
<h4 id="Variable-Elimination"><a href="#Variable-Elimination" class="headerlink" title="Variable Elimination"></a>Variable Elimination</h4><p>$P(x) &#x3D; \prod_{i} \phi_i(x_i)$</p>
<h4 id="ç¤ºä¾‹"><a href="#ç¤ºä¾‹" class="headerlink" title="ç¤ºä¾‹"></a>ç¤ºä¾‹</h4><p>å‡è®¾æˆ‘ä»¬æœ‰å››ä¸ªäºŒå€¼éšæœºå˜é‡ $a, b, c, d\in {0,1}$ã€‚æˆ‘ä»¬æƒ³è®¡ç®—è¾¹ç¼˜æ¦‚ç‡ $P(d)$ã€‚<br>$a\rightarrow b \rightarrow c \rightarrow d$</p>
<ol>
<li><p>å±•å¼€è”åˆåˆ†å¸ƒï¼š</p>
<ul>
<li>$P(d) &#x3D; \sum_{a, b, c} P(a, b, c, d)$</li>
</ul>
</li>
<li><p>Chain rule</p>
<ul>
<li>$P(a, b, c, d) &#x3D; P(a) P(b | a) P(c | b) P(d | c)$</li>
</ul>
</li>
<li><p>1-&gt;2</p>
<ul>
<li>$P(d) &#x3D; \sum_{a, b, c} P(a) P(b | a) P(c | b) P(d | c)$<ul>
<li>$&#x3D; \sum_{b, c} P(d | c) \left( \sum_{a} P(a) P(b | a) \right) P(c | b)$</li>
</ul>
</li>
</ul>
</li>
<li><p>å®šä¹‰æ–°çš„å› å­å‡½æ•°ï¼š</p>
<ul>
<li>$\phi_1(b, c) &#x3D; \sum_{a} P(a) P(b | a)$</li>
</ul>
</li>
<li><p>æœ€ç»ˆå¾—åˆ°ï¼š</p>
<ul>
<li>$P(d) &#x3D; \sum_{c} P(d | c) \left( \sum_{b} \phi_1(b, c) P(c | b) \right)$</li>
</ul>
</li>
<li><p>å¾—åˆ°å¦ä¸€ä¸ªå› å­å‡½æ•° $\phi_2(c, d)$ï¼š</p>
<ul>
<li>$P(d) &#x3D; \sum_{c} \phi_2(c, d)$</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/29/2024-05-29-Casual-Inference/" data-id="cm9bnagsv001nzc3d2zxgc2mm" data-title="Casual Inference" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-05-19-CS224W-notes" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/19/2024-05-19-CS224W-notes/" class="article-date">
  <time class="dt-published" datetime="2024-05-19T17:01:34.000Z" itemprop="datePublished">2024-05-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/19/2024-05-19-CS224W-notes/">CS224W_notes</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-Introductionï¼ŒMachine-learning-for-graphs"><a href="#1-Introductionï¼ŒMachine-learning-for-graphs" class="headerlink" title="1 Introductionï¼ŒMachine learning for graphs"></a>1 Introductionï¼ŒMachine learning for graphs</h1><h2 id="å¤§çº²"><a href="#å¤§çº²" class="headerlink" title="å¤§çº²"></a>å¤§çº²</h2><p>å¤§çº²</p>
<ol>
<li>Traditional methods: Graphlets, Graph Kernels</li>
<li>Methods for node embeddings: DeepWalk, Node2Vec</li>
<li>Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs</li>
<li>Knowledge graphs and reasoning: TransE, BetaE</li>
<li>Deep generative models for graphs</li>
<li>Applications to Biomedicine, Science, Industry</li>
</ol>
<h2 id="Defs"><a href="#Defs" class="headerlink" title="Defs"></a>Defs</h2><ol start="0">
<li>$G&#x3D;(V, E, F)$ or $G(V, E)$</li>
<li>Directed&#x2F; undirected</li>
<li>Degree<br>Directed $\bar{k} &#x3D; \langle k \rangle &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} k_i &#x3D; \frac{2E}{N}$<br>Undirected in-degree + out-degree &#x3D; (total) degree, $\bar{k}&#x3D; \frac{E}{N}$ </li>
<li>Bipartite Graph</li>
<li>Folded&#x2F;Projected Bipartite graph</li>
<li>Representing graphs: Adjacency matrix Density of matrix $\frac{E}{N^2}$<ol>
<li>adjacency matrix</li>
<li>Edge List</li>
<li>Adjacency List</li>
</ol>
</li>
<li>Attribute of edges</li>
<li>Weighted&#x2F;Unweighted</li>
<li>Self-edges</li>
<li>Connectivity: (Un)Directed, Strong Connected Components (in Undirected)</li>
</ol>
<h1 id="2-Traditional-methods-for-ML-on-graph"><a href="#2-Traditional-methods-for-ML-on-graph" class="headerlink" title="2 Traditional methods for ML on graph"></a>2 Traditional methods for ML on graph</h1><p>Structural Feature&#x2F; Node features<br>Train on Random Forest, SVM, Neural Network; Apply on new graph. </p>
<h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><h3 id="1-Node-Centrality"><a href="#1-Node-Centrality" class="headerlink" title="(1) Node Centrality"></a>(1) Node Centrality</h3><p>233</p>
<ol>
<li>Eigenvector centrality: $c_v&#x3D;\frac{1}{\lambda}\sum_{u\in N(v)}$</li>
<li>Betweenness centrality<br>Here is the improved version of the formulas in the format you provided, while keeping the same style and explanation:</li>
</ol>
<hr>
<ol>
<li><p><strong>Betweenness Centrality</strong><br>Betweenness centrality ( c_v ) measures the extent to which a node ( v ) lies on the shortest paths between other nodes.  </p>
<p>$$<br>c_v &#x3D; \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}<br>$$</p>
<ul>
<li>( \sigma_{st} ): The number of shortest paths between nodes ( s ) and ( t ).  </li>
<li>( \sigma_{st}(v) ): The number of shortest paths between ( s ) and ( t ) that pass through ( v ).</li>
</ul>
</li>
</ol>
<hr>
<ol start="2">
<li><p><strong>Closeness Centrality</strong><br>Closeness centrality ( c_v ) quantifies how close a node ( v ) is to all other nodes in the network.  </p>
<p>$$<br>c_v &#x3D; \frac{1}{\sum_{u \neq v} d(u, v)}<br>$$</p>
<ul>
<li>( d(u, v) ): The shortest path length between node ( u ) and node ( v ).</li>
</ul>
</li>
</ol>
<h3 id="2-Clustering-Coefficient"><a href="#2-Clustering-Coefficient" class="headerlink" title="(2) Clustering Coefficient"></a>(2) Clustering Coefficient</h3><p>[<br>e_v &#x3D; \frac{\text{Number of edges between neighbors of } v}{\binom{k_v}{2}}, \quad \binom{k_v}{2} &#x3D; \frac{k_v (k_v - 1)}{2}<br>] </p>
<h3 id="3-Graphlet-Rooted-connected-non-isomorphic-subgraphs"><a href="#3-Graphlet-Rooted-connected-non-isomorphic-subgraphs" class="headerlink" title="(3) Graphlet: Rooted connected non-isomorphic subgraphs"></a>(3) Graphlet: Rooted connected non-isomorphic subgraphs</h3><p>Graphlet degree vector<br>Clustering coefficient.<br>Feature-based&#x2F; structure-based features.</p>
<h2 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h2><h3 id="1-Link-prediction"><a href="#1-Link-prediction" class="headerlink" title="(1) Link prediction"></a>(1) Link prediction</h3><ol>
<li>Links missing at random: Remove a random set of links and then aim to predict them</li>
<li>Links over time: Given $G[t_0, t_0â€™]$ a graph on edges up to time $t_0â€™$, output a ranked list $L$ of links (not in $G[t_0, t_0â€™]$) that are predicted to appear in $G[t_1, t_1â€™]$</li>
</ol>
<h3 id="2-Local-Neighborhood-Overlap"><a href="#2-Local-Neighborhood-Overlap" class="headerlink" title="(2) Local Neighborhood Overlap"></a>(2) Local Neighborhood Overlap</h3><ul>
<li>Common neighbors: $|N(v_1) \cap N(v_2)|$</li>
<li>Jaccardâ€™s coefficient: $\frac{|N(v_1) \cap N(v_2)|}{|N(v_1) \cup N(v_2)|}$,<br>Normalize common neighbor, assuming having the same number of neighbors</li>
<li>Adamic-Adar index: $\sum_{u \in N(v_1) \cap N(v_2)} \frac{1}{\log k_u}$,<br>Penalize those who have many neighbors</li>
</ul>
<h3 id="3-Global-Neighborhood-Overlap"><a href="#3-Global-Neighborhood-Overlap" class="headerlink" title="(3) Global Neighborhood Overlap"></a>(3) Global Neighborhood Overlap</h3><ul>
<li>Katz index: $S_{uv}&#x3D; \sum_{l&#x3D;1}^{\infty} \beta^l A^l_{uv}$,<br>$A^l_{uv}$: number of paths of length $l$ between $u$ and $v$,<br>$\beta$: discount factor, the contribution of long paths  </li>
<li>Katz index matrix: $S&#x3D; \sum_{i&#x3D;1}^\infty \beta^iA^i &#x3D; (I-\beta A)^{-1}-I$</li>
</ul>
<h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><h3 id="Kernel-method"><a href="#Kernel-method" class="headerlink" title="Kernel method:"></a>Kernel method:</h3><ol>
<li><p><strong>Kernel function</strong>:<br>$K(G, Gâ€™)$ measures the similarity between two graphs $G$ and $Gâ€™$.<br>Maps the graphs into a higher-dimensional space where linear methods can be applied to perform complex, non-linear tasks in the original space.</p>
</li>
<li><p><strong>Kernel Matrix</strong>: $\mathbf{K} &#x3D; \left( K(G, Gâ€™) \right)_{G, Gâ€™}$ is a symmetric matrix.<br>It is positive semidefinite, meaning all its eigenvalues are non-negative.</p>
</li>
<li><p><strong>Feature Representation</strong>:<br>Feature mapping $\phi(\cdot)$ kernel function is expressed as dot product in feature space: $K(G, Gâ€™) &#x3D; \phi(G)^\top \phi(Gâ€™)$.</p>
</li>
</ol>
<p>cite: <a target="_blank" rel="noopener" href="https://blog.csdn.net/PolarisRisingWar/article/details/115598815">CSDN Blog</a></p>
<h3 id="Design-graph-feature-vector-phi-G"><a href="#Design-graph-feature-vector-phi-G" class="headerlink" title="Design graph feature vector $\phi(G)$"></a>Design graph feature vector $\phi(G)$</h3><p><strong>Bag-of-Words: BoW</strong><br>Key idea: use the word counts as features (#nodes as features, #degree, #graphlet, #color)</p>
<h3 id="Graphlet-features"><a href="#Graphlet-features" class="headerlink" title="Graphlet features"></a>Graphlet features</h3><p>Key idea: Count the number of different graphlets in a graph.<br>$G_k&#x3D;(g_1, g_2, \cdots, g_{n_k})$</p>
<p>Graphlet count vector $(f_G)&#x3D;#(g_i \subseteq G), ; i \leq n_k$</p>
<p>$h_G&#x3D; \frac{f_G}{\text{Sum}(F_G)}$,<br>$K(G, Gâ€™)&#x3D;H_G^\top H_Gâ€™$</p>
<p>NP-hard $O(nd^{k-1})$, expensive to calculate </p>
<h3 id="Weisfeiler-Lehman-Kernel"><a href="#Weisfeiler-Lehman-Kernel" class="headerlink" title="Weisfeiler-Lehman Kernel"></a>Weisfeiler-Lehman Kernel</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/PolarisRisingWar/article/details/117336622">Weisfeiler-Lehman Kernel Blog</a></p>
<h1 id="3-Node-Embedding"><a href="#3-Node-Embedding" class="headerlink" title="3 Node Embedding"></a>3 Node Embedding</h1><h2 id="3-1-Intro"><a href="#3-1-Intro" class="headerlink" title="3.1 Intro"></a>3.1 Intro</h2><p>Key: How to define node similarity  </p>
<!-- Avoid direct Feature Engineering, and also reflect structural features -->
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Feature representation â†” feature embedding  </p>
<!-- If two nodes are similar in structure, they should be similar in embedding space -->
<!-- Use dot product to measure similarity -->
<p>Similarity $(u, v) \approx z_u^T z_v$,<br>$ENC(u) &#x3D; z_u$, $ENC(v) &#x3D; z_v$, <!--ENC: Encoder--><br>$z_u, z_v$ are $d$-dimensional in embedding space, $d$ usually 64-1000<br>$ENC(v)$ node in the input graph</p>
<p>$ENC(v) &#x3D; z_v &#x3D; Z \cdot v$<br>Encoder is a lookup, embedding matrix $Z \in \mathbb{R}^{d \times |V|}$, $v \in I^{|V|}$</p>
<h4 id="ENC"><a href="#ENC" class="headerlink" title="ENC"></a>ENC</h4><p>Shallow encoder: $d \times |V|$<br>Some ways for ENC: DeepWalk, Node2Vec</p>
<h2 id="3-2-Random-walk-for-Node-Embedding"><a href="#3-2-Random-walk-for-Node-Embedding" class="headerlink" title="3.2 Random walk for Node Embedding"></a>3.2 Random walk for Node Embedding</h2><h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul>
<li>Vector $z_u$: embedding vector of node $u$ (what we aim to find)</li>
<li>$P(v|z_u)$: probability of visiting node $v$ on random walk starting from $u$. Used to measure similarity. </li>
<li>Softmax: $\sigma(z)_i &#x3D; \frac{e^{z_i}}{\sum e^{z_j}}$</li>
<li>Sigmoid: $S(x) &#x3D; \frac{1}{1 + e^{-x}}$</li>
</ul>
<h3 id="Random-walk-embedding"><a href="#Random-walk-embedding" class="headerlink" title="Random walk embedding"></a>Random walk embedding</h3><p>Using random strategy $R$: $P_R(u|v)$<br>Given<br>$G&#x3D;(V, E)$<br>Goal: Learn a mapping $f: u \rightarrow \mathbb{R}^d$<br>$$<br>\mathcal{L} &#x3D; \sum_{u \in V} \sum_{v \in N_R(u)} -\log\left(\frac{\exp(z_u^\top z_v)}{\sum_{n \in V} \exp(z_u^\top z_n)}\right)<br>$$<br><strong>Negative sampling</strong><br>$$<br>\log\left(\frac{\exp(z_u^\top z_v)}{\sum_{n \in V} \exp(z_u^\top z_n)}\right) \approx \log\left(\sigma(z_u^\top z_v)\right) - \sum_{i&#x3D;1}^{k} \log\left(\sigma(z_u^\top z_{n_i})\right), \quad n_i \sim P_V<br>$$ â€“&gt;</p>
<h3 id="Random-walk-strategy"><a href="#Random-walk-strategy" class="headerlink" title="Random walk strategy"></a>Random walk strategy</h3><h4 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h4><p><a target="_blank" rel="noopener" href="https://www.vldb.org/pvldb/vol10/p13-wu.pdf">DeepWalk Paper</a></p>
<h4 id="Node2Vec"><a href="#Node2Vec" class="headerlink" title="Node2Vec"></a>Node2Vec</h4><p>Node2Vec</p>
<p><strong>Hyperparameters:</strong></p>
<ul>
<li>$p$: Return parameter</li>
<li>$q$: In-out parameter</li>
</ul>
<h2 id="3-3-Embedding-Entire-Graph"><a href="#3-3-Embedding-Entire-Graph" class="headerlink" title="3.3 Embedding Entire Graph"></a>3.3 Embedding Entire Graph</h2><h3 id="Approach-1"><a href="#Approach-1" class="headerlink" title="Approach 1"></a>Approach 1</h3><p>Sum&#x2F;mean $z_G &#x3D; \sum_{v \in G} z_v$<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1509.09292">Embedding Entire Graph</a></p>
<h3 id="Approach-2"><a href="#Approach-2" class="headerlink" title="Approach 2"></a>Approach 2</h3><p>Virtual node<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.05493">Virtual Node Paper</a></p>
<h3 id="Approach-3"><a href="#Approach-3" class="headerlink" title="Approach 3"></a>Approach 3</h3><p>Anonymous walk:</p>
<h4 id="Sampling-Anonymous-walks"><a href="#Sampling-Anonymous-walks" class="headerlink" title="Sampling Anonymous walks"></a>Sampling Anonymous walks</h4><p>Distribution have error less than $\epsilon$ with probability, less than $\delta$<br>$m &#x3D; f(\epsilon, \sigma, \delta)$</p>
<h4 id="Walk-Embedding"><a href="#Walk-Embedding" class="headerlink" title="Walk Embedding"></a>Walk Embedding</h4><p>$\Delta$</p>
<h1 id="4-Node-Embedding-using-Random-walk-PageRank"><a href="#4-Node-Embedding-using-Random-walk-PageRank" class="headerlink" title="4 Node Embedding using Random walk - PageRank"></a>4 Node Embedding using Random walk - PageRank</h1><h2 id="4-1-Intro"><a href="#4-1-Intro" class="headerlink" title="4.1 Intro"></a>4.1 Intro</h2><p>PageRank: $r_v &#x3D; \sum_{u \in N_R(v)} r_u \frac{A_{u,v}}{d_u}$<br>$d_u$: degree of node $u$<br>$A_{u,v}$: adjacency matrix<br>$r_v$: rank of node $v$<br>$N_R(v)$: neighbors of node $v$</p>
<h2 id="4-2-PageRank-for-Graph"><a href="#4-2-PageRank-for-Graph" class="headerlink" title="4.2 PageRank for Graph"></a>4.2 PageRank for Graph</h2><p>$r_v &#x3D; \sum_{u \in N_R(v)} r_u \frac{A_{u,v}}{d_u}$<br>$d_u$: degree of node $u$<br>$A_{u,v}$: adjacency matrix<br>$r_v$: rank of node $v$<br>$N_R(v)$: neighbors of node $v$</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>$r_v &#x3D; \sum_{u \in N_R(v)} r_u \frac{A_{u,v}}{d_u}$</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><ul>
<li>Personalized PageRank (Topic specific PageRank)<br>Rank proximity of nodes to the teleport nodes $S$,<br>Proximity on graphs: </li>
<li>PageRank with restarts:</li>
</ul>
<h3 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h3><p>Frobenius norm: $\min_z ||A - Z^\top Z||$<br><img src="2024-05-19-CS224W-notes/image.png" alt="Alt text" width="400" ></p>
<h1 id="5-Message-passing-Node-Classification"><a href="#5-Message-passing-Node-Classification" class="headerlink" title="5 Message passing &amp; Node Classification"></a>5 Message passing &amp; Node Classification</h1><p>Classical methods</p>
<p><strong>Correlation:</strong> nearby nodes have the same color<br>$A_{n \times n}$: Adjacency matrix<br>$Y &#x3D; {0, 1}^n$</p>
<h2 id="Collective-Classification"><a href="#Collective-Classification" class="headerlink" title="Collective Classification:"></a>Collective Classification:</h2><ol>
<li>Local classifier </li>
<li>Relational Classifier </li>
<li>Collective Inference<br>$1^{st}$ order Markov assumption: $P(Y_v) &#x3D; P(Y_v | N_v)$</li>
</ol>
<ul>
<li>Relational classification</li>
<li>Iterative classification </li>
<li>Belief propagation</li>
</ul>
<h1 id="6-GNN-Model"><a href="#6-GNN-Model" class="headerlink" title="6 GNN Model"></a>6 GNN Model</h1><h1 id="7-GNN-Design-Space"><a href="#7-GNN-Design-Space" class="headerlink" title="7 GNN Design Space"></a>7 GNN Design Space</h1><h1 id="8-Training-GNN"><a href="#8-Training-GNN" class="headerlink" title="8 Training GNN"></a>8 Training GNN</h1><h2 id="8-1-Data-augmentation"><a href="#8-1-Data-augmentation" class="headerlink" title="8.1 Data augmentation"></a>8.1 Data augmentation</h2><h3 id="Feature-based"><a href="#Feature-based" class="headerlink" title="Feature based"></a>Feature based</h3><h3 id="Structure-based"><a href="#Structure-based" class="headerlink" title="Structure based"></a>Structure based</h3><h2 id="8-2"><a href="#8-2" class="headerlink" title="8.2"></a>8.2</h2><h2 id="8-3"><a href="#8-3" class="headerlink" title="8.3"></a>8.3</h2><p><img src="/2024-05-19-CS224W-notes/image-1.png" alt="alt text"></p>
<p><strong>Node Prediction</strong>  </p>
<ul>
<li>Transductive setting</li>
<li>Inductive setting</li>
</ul>
<p><strong>Training</strong><br>Validation (tuning hyperparameters)<br>Test set</p>
<p><strong>Graph Prediction</strong>  </p>
<ul>
<li>Link Prediction â€“&gt;</li>
</ul>
<h1 id="9-Theory-of-GNN"><a href="#9-Theory-of-GNN" class="headerlink" title="9 Theory of GNN"></a>9 Theory of GNN</h1><p>GCN, GAT, GraphSAGE, design space </p>
<h2 id="9-1"><a href="#9-1" class="headerlink" title="9.1"></a>9.1</h2><h2 id="9-2"><a href="#9-2" class="headerlink" title="9.2"></a>9.2</h2><p>GCN Mean pooling fails<br>GraphSAGE mean-pool </p>
<p>Injective Multiset function: $\Phi(\cdot)$: a non-linear function:<br>$\Phi(\sum_{x \in S} f(x))$:<br>Multi-layer Perceptron<br><strong>Theorem:</strong> Universal approximation theorem<br>A neural network can model any injective multiset function:<br>$MLP_{\Phi}(\sum_{x \in S} MLP_{f}(x))$</p>
<p><strong>Graph Isomorphism Network (GIN) Xue 2019</strong> </p>
<p><strong>WL Graph Kernel</strong><br>Hash<br>$$<br>\left( c^{(k)}(v), { c^{(k)}(u) }_{u \in N(v)} \right)<br>$$</p>
<p>$$<br>\text{MLP}<em>{\Phi} \left( (1 + \epsilon) \cdot \text{MLP}</em>{f}(c^{(k)}(v)) + \sum_{u \in N(v)} \text{MLP}_{f}(c^{(k)}(u)) \right)<br>$$</p>
<p>where $\epsilon$ is a learnable scalar</p>
<p>$$<br>c^{(k+1)}(v) &#x3D; \text{HASH} \left( c^{(k)}(v), { c^{(k)}(u) }_{u \in N(v)} \right)<br>$$</p>
<p>$$<br>\text{GINConv} \left( c^{(k)}(v), { c^{(k)}(u) }<em>{u \in N(v)} \right) &#x3D; \text{MLP}</em>{\Phi} \left( (1 + \epsilon) \cdot c^{(k)}(v) + \sum_{u \in N(v)} c^{(k)}(u) \right)<br>$$</p>
<h1 id="10-Heterogeneous-Graphs-and-Knowledge-Graph-Embeddings"><a href="#10-Heterogeneous-Graphs-and-Knowledge-Graph-Embeddings" class="headerlink" title="10 Heterogeneous Graphs and Knowledge Graph Embeddings"></a>10 Heterogeneous Graphs and Knowledge Graph Embeddings</h1><h2 id="10-1"><a href="#10-1" class="headerlink" title="10.1"></a>10.1</h2><h3 id="Heterogeneous-Graphs"><a href="#Heterogeneous-Graphs" class="headerlink" title="Heterogeneous Graphs"></a>Heterogeneous Graphs</h3><p>$G&#x3D;(V, E, R, T)$</p>
<h2 id="RGCN"><a href="#RGCN" class="headerlink" title="RGCN"></a>RGCN</h2><h1 id="VGAE"><a href="#VGAE" class="headerlink" title="VGAE"></a>VGAE</h1><p>è®²çš„æ¯”è¾ƒå¥½çš„GAEå’ŒVGAE</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.atyun.com/17976.html">AtYun Article</a></li>
<li><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/5253#%E7%BB%88%E7%82%B9%E7%AB%99">Spaces Article</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_16763983/article/details/120403055?spm=1001.2101.3001.6650.7&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-120403055-blog-119531815.235%5Ev43%5Epc_blog_bottom_relevance_base8&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-7-120403055-blog-119531815.235%5Ev43%5Epc_blog_bottom_relevance_base8&utm_relevant_index=13">CSDN Blog</a></li>
</ol>
<h3 id="CNN-code"><a href="#CNN-code" class="headerlink" title="CNN code"></a>CNN code</h3><ol>
<li>CNN ç½‘ç»œç»“æ„ä¸éƒ¨åˆ† PyTorch: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/wpx123/p/17616156.html">CNBlogs 1</a>, <a target="_blank" rel="noopener" href="https://www.cnblogs.com/wpx123/p/17621303.html">CNBlogs 2</a></li>
</ol>
<h4 id="Optimizerï¼š"><a href="#Optimizerï¼š" class="headerlink" title="Optimizerï¼š"></a>Optimizerï¼š</h4><p>SGD, GD, Adam éƒ½æ˜¯ Optimizer çš„ç§ç±»</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346205754">PyTorch æºä»£ç è§£è¯»</a>, ä»¥åŠå„ç§å‚æ•° lr, gamma çš„å½±å“</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/xian0710830114/article/details/126551268">ç®€å•è®²è§£äº† SGDï¼Œ Adam çš„åŸç†</a></li>
</ol>
<h3 id="ä¸€ä¸ªæ¯”è¾ƒæœ‰ç”¨çš„-Casual-Inference-ç»¼è¿°çš„åšå®¢ï¼š"><a href="#ä¸€ä¸ªæ¯”è¾ƒæœ‰ç”¨çš„-Casual-Inference-ç»¼è¿°çš„åšå®¢ï¼š" class="headerlink" title="ä¸€ä¸ªæ¯”è¾ƒæœ‰ç”¨çš„ Casual Inference ç»¼è¿°çš„åšå®¢ï¼š"></a>ä¸€ä¸ªæ¯”è¾ƒæœ‰ç”¨çš„ Casual Inference ç»¼è¿°çš„åšå®¢ï¼š</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/caoyusang/p/13518354.html">Casual Inference ç»¼è¿°</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/19/2024-05-19-CS224W-notes/" data-id="cm9bnagps000yzc3d7hmqckbt" data-title="CS224W_notes" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-05-08-Meetings-log" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/08/2024-05-08-Meetings-log/" class="article-date">
  <time class="dt-published" datetime="2024-05-09T00:11:06.000Z" itemprop="datePublished">2024-05-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/08/2024-05-08-Meetings-log/">papers</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-Counterfactual-fairness"><a href="#1-Counterfactual-fairness" class="headerlink" title="1. Counterfactual fairness"></a>1. Counterfactual fairness</h1><p>Counterfactual fairness<br>link: <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf</a></p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>Definitions:</p>
<h4 id="defs"><a href="#defs" class="headerlink" title="defs"></a>defs</h4><p>$A$: Protected attributes, sensitive features<br>$X$: features of individuals, excluding A<br>$U$: latent features not observed, represented<br>$Y$: predictor    </p>
<h4 id="Fairness-through-unawareness-FTU"><a href="#Fairness-through-unawareness-FTU" class="headerlink" title="Fairness through unawareness (FTU):"></a>Fairness through unawareness (FTU):</h4><p><em>An algorithm is fair so long as any protected attributes $A$ are not explicitly used in the decision-making process.</em><br>Shortcoming: $X$ might intersects $A$</p>
<h4 id="Individual-Fairness-IF"><a href="#Individual-Fairness-IF" class="headerlink" title="Individual Fairness (IF)."></a>Individual Fairness (IF).</h4><p>For distance metric(should be carefully choosen), $d(\cdot , \cdot)$, if $d(i, j)$ is small, then $\hat Y(X^{(i)}, A^{(i)}) \approx \hat Y(X^{(j)}, A^{(j)})$</p>
<h4 id="Demographic-Parity-DP-äººå£ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„å¹³ç­‰"><a href="#Demographic-Parity-DP-äººå£ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„å¹³ç­‰" class="headerlink" title="Demographic Parity (DP)(äººå£ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„å¹³ç­‰)"></a>Demographic Parity (DP)(äººå£ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„å¹³ç­‰)</h4><p>Predictor $\hat Y$ satisfies demographic partiy if $P(\hat Y|A&#x3D;0)&#x3D;P(\hat Y|A&#x3D;1)$ </p>
<h4 id="Equality-of-Opportunity"><a href="#Equality-of-Opportunity" class="headerlink" title="Equality of Opportunity"></a>Equality of Opportunity</h4><p>$P(\hat Y|A&#x3D;0, Y&#x3D;1)&#x3D;P(\hat Y|A&#x3D;1, Y&#x3D;1)$ </p>
<h3 id="Causal-Models-å› æœæ¨æ–­-Counterfacutalã€"><a href="#Causal-Models-å› æœæ¨æ–­-Counterfacutalã€" class="headerlink" title="Causal Models(å› æœæ¨æ–­), Counterfacutalã€"></a>Causal Models(å› æœæ¨æ–­), Counterfacutalã€</h3><p>Casual Model $(U, V, F)$,<br>$U$: latent background variables,<br>$V$: observed variables, <br>$F&#x3D;{f_1. f_2, \cdots, f_n}$, for each $V_i&#x3D;f_i(pa_i, U_{pa_i})\in V, pa_i \subseteq V \backslash {V_i}$ </p>
<p><strong>Three Steps of Inference</strong>\</p>
<ul>
<li>Abductionï¼šfor a given prior on $U$, compute the posterior distribution of $U$ given the evidence $W &#x3D; w$</li>
<li>Actionï¼šsubstitute the equations for $Z$ with the interventional values $z$, resulting in the modified set of equations $F_z$</li>
<li>Prediction:</li>
</ul>
<h2 id="é¢˜å¤–è¯"><a href="#é¢˜å¤–è¯" class="headerlink" title="é¢˜å¤–è¯"></a>é¢˜å¤–è¯</h2><h3 id="Casual-Models-å› æœæ¨æ–­"><a href="#Casual-Models-å› æœæ¨æ–­" class="headerlink" title="Casual Models (å› æœæ¨æ–­)"></a>Casual Models (å› æœæ¨æ–­)</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1217887302124773376">https://www.zhihu.com/column/c_1217887302124773376</a></p>
<h4 id="Three-levels"><a href="#Three-levels" class="headerlink" title="Three levels:"></a>Three levels:</h4><ol>
<li>Association: $A-B$ </li>
<li>Interventionï¼š$A&#x2F;Aâ€™ \rightarrow B?$</li>
<li>Counterfactual $ want\ Bâ€™, how A\rightarrow Aâ€™$</li>
</ol>
<h4 id="Beyasian-Network"><a href="#Beyasian-Network" class="headerlink" title="Beyasian Network"></a>Beyasian Network</h4><p>In Directed acyclic Graph (DAG):<br><img src="/papers/image.png" alt="alt text"></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/mantch/p/11179933.html">https://www.cnblogs.com/mantch/p/11179933.html</a><br>Component:</p>
<ol>
<li>head-to-head $a\rightarrow c\leftarrow b$ <br>$P(a,b,c) &#x3D; P(a)P(b)P(c|a,b)$,<br>unknown $c$, $a, b$ are blocked thus independent</li>
<li>tail-to-tail $a\leftarrow c\rightarrow b$</li>
</ol>
<ul>
<li>$c$ unknown, $P(a,b,c)&#x3D;P(c)P(a|c)P(b|c)$, $a, b$, not independent</li>
<li>$c$ known, $P(a,b,c)&#x3D;P(c)P(a|c)P(b|c)$, $P(a,b|c)&#x3D;P(a,b,c)&#x2F;P(c)&#x3D;P(a|c)*P(b|c)$, $a, b $independent</li>
</ul>
<ol start="3">
<li>head-to-tail (Markov Chain) $A\rightarrow C\rightarrow B$</li>
</ol>
<ul>
<li>$c$ unknown, $a, b$, not independent</li>
<li>$c$ known, $a, b$ independent</li>
</ul>
<p><strong>Factor Graph</strong></p>
<h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><p>Confounder</p>
<h2 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><p>æˆ‘ä»ä¸€å¤©å‰å¼€å§‹çœ‹è®ºæ–‡ï¼Œè¢«casual modelçš„æ¦‚å¿µå¸å¼•äº†ã€‚æˆ‘è®¤ä¸ºæ˜¯å¾ˆå¥½çš„ä¸€ä¸ªç†è§£æ–¹å¼ã€‚ä»æ—©ä¸Šäº”ç‚¹å‡†å¤‡åˆ°åä¸€ç‚¹ã€‚<br>ä»Šå¤©åšäº†preï¼Œæ•ˆæœå¾ˆå·®ã€‚</p>
<ol>
<li>å¯¹æ¦‚ç‡çš„å„ç§å…¬å¼å¾ˆä¸å¤ªäº†è§£ã€‚å¯¹è´å¶æ–¯å’ŒMCMCä¸ä¼šã€‚</li>
<li>æ²¡æœ‰å»æƒ³è¿‡$U ,A, X$çš„å…³ç³»ã€‚æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„è§£é‡Šè®ºæ–‡ä¸­çš„é€»è¾‘å…³ç³»ã€‚</li>
</ol>
<p>åœ¨å¼€ä¼šçš„æ—¶å€™æ•™æˆè¯´é‡è¦ä¸œè¥¿ï¼š</p>
<ol>
<li>Counterfactual <br>è¿™ç¯‡æœ€é‡è¦çš„æ˜¯ï¼š <strong>Definition 5:</strong> $P(\hat Y_{A\leftarrow a}(U)|X&#x3D;x, A&#x3D;a)&#x3D;P(\hat Y_{A\leftarrow aâ€™}(U)|X&#x3D;x, A&#x3D;a)$ <br>å¾ˆå¤šâ€˜æ¦‚ç‡â€™åªæ˜¯è¡¨ç¤ºæ–¹æ³•ã€‚ï¼ˆä½†æ˜¯ç¡®å®ä¸å¾ˆç†è§£æ¦‚ç‡ï¼‰<br>ç®—æ³•çš„æ€æƒ³åœ¨äºï¼š1. å¼•å…¥å› æœå›¾ã€‚2.å¯»æ‰¾Uï¼ˆ17å¹´MCMCï¼Œç°åœ¨å¯ä»¥GANï¼Œæˆ–å…¶ä»–ç”Ÿæˆå¼å­¦ä¹ æ–¹æ³•ï¼‰ã€‚</li>
<li>$U \rightarrow Xï¼ŒA$<br>åœ¨è®¡ç®—ä¸­ç”¨$X, A \rightarrow U$ æœ‰ä¸€äº›ç±»ä¼¼Adversarial learning. å¯ä»¥ç ”ç©¶æ€ä¹ˆå¥—ç”¨ã€‚</li>
<li>GAD</li>
<li>æœ‰ç‚¹æƒ³åštransfer learning çš„é‚£ç§</li>
</ol>
<h1 id="FairGAD"><a href="#FairGAD" class="headerlink" title="FairGAD"></a>FairGAD</h1><p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=3cE6NKYy8x">https://openreview.net/forum?id=3cE6NKYy8x</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.04937">https://arxiv.org/abs/2307.04937</a></p>
<h2 id="Fair-GAD-problem"><a href="#Fair-GAD-problem" class="headerlink" title="Fair GAD problem"></a>Fair GAD problem</h2><p><strong>GAD</strong><br>$G&#x3D;(V, E, X)$, <br>node feature matrix $X\in \R^{n\times d}$, <br>Adjacency matrix $A\in {0,1}^{n\times n}$, <br>Anomaly labels $Y\in {0, 1}^n$, predicted $\hat Y$, <br><strong>Fair GAD</strong><br>sensitive attributes $S\in {0, 1}^n$, a binary feature $X$.<br>Performance matrix: accuracy and <em>AUCROC</em>: Area under the ROC Curve <br>Unfairness Mextrics, Statistic Parity(SP):$SP &#x3D; |P(\hat Y&#x3D;1|S&#x3D;0)âˆ’P(\hat Y &#x3D;1|S&#x3D;1)|$, <br>Equality of Odds <em>(EOO)</em>: $SP &#x3D; |P(\hat Y&#x3D;1|S&#x3D;0, Y&#x3D;1)âˆ’P(\hat Y &#x3D;1|S&#x3D;1, Y&#x3D;1)|$</p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><ul>
<li>Reddit:<br>graph structureï¼š linking two user posted the name subreddit within 24h.<br>Node feature: Embedding from post histories.</li>
<li>Twitter:<br>graph structure:: A follows B.<br>Node feature: demographic infromation using M3 system, multimodal, multilingual, multi attirbute demographix inderence framework.</li>
</ul>
<!-- ## GAD Methods
### DOMINANT (Ding et al., 2019a)
### CONAD (Xu et al., 2022)
### COLA (Liu et al., 2021)
### VGOD (Huang et al., 2023)

## Non-Graph AD methods
- DONE (Bandyopadhyay et al., 2020)
- AdONE (Bandyopadhyay et al., 2020)
- ECOD (Li et al., 2022)
- VAE (Kingma & Welling, 2014)
- ONE (Bandyopadhyay et al., 2019)
- LOF (Breunig et al., 2000)
- F (Liu et al., 2008)

## Fainess Method:
### FAIROD (Shekhar et al., 2021)
### CORRELATION (Shekhar et al., 2021)
### HIN (Zeng et al., 2021)
### EDITS (Dong et al., 2022)
### FAIRWALK (Rahman et al., 2019)

## Distance 
### Wasserstein Distance
### Minkowski distance -->



<h1 id="2024-05-23-Meeting-summary"><a href="#2024-05-23-Meeting-summary" class="headerlink" title="2024.05.23 Meeting summary"></a>2024.05.23 Meeting summary</h1><ol>
<li>è®¨è®ºäº†FairGADã€‚å¦‚æœä¸€ä¸ªæ–‡ç« çš„è´¡çŒ®æ˜¯æ•°æ®é›†ï¼Œé‚£ä¹ˆéœ€è¦è¯¦ç»†çš„Benchmarking: æœ‰ä¸€ç¯‡surveyçš„æ€§è´¨ï¼Œæ˜ç™½å„ç§æ–¹æ³•åœ¨æ•°æ®é›†ä¸Šè¡¨ç°æ€ä¹ˆæ ·ï¼Œæå‡ºä¸€ä¸ªè¯„åˆ¤æ ‡å‡†ï¼Œåªç”¨EOOä½œä¸ºfairçš„åˆ¤æ–­å¤ªç®€çŸ­äº†ã€‚</li>
<li>åŸºäºsentivityçš„Counterfactual fairnessçš„è¯„åˆ¤æ ‡å‡†ï¼Œæˆ‘ä»¬ç”¨ä»€ä¹ˆæ ·çš„è¯„åˆ¤æ ‡å‡†å’Œ<br>2.1 æœ€ç®€å•çš„æ„é€ æ–¹æ³• anomaly datasetï¼šclassification with y&#x3D;1,2,3,4,5ã€‚æ‹¿å¾ˆå¤š1ï¼Œsampleè¾ƒå°‘2345.<br>2.2 æ‰¾ä¸€äº›graphä¸Šæ•°æ®é›†ï¼Œç”¨GADçš„æ–¹æ³•ï¼Œå˜æˆfairGADçš„æ•°æ®é›†ã€‚ä½†æ˜¯FairGADï¼Œæ˜¯GADæ•°æ®é›†inject fairnessï¼Œå¯èƒ½ä¸å¤ªå¥½ã€‚<br>2.3</li>
</ol>
<h2 id="Task-of-this-week"><a href="#Task-of-this-week" class="headerlink" title="Task of this week"></a>Task of this week</h2><p>create synthetic data for fair GAD</p>
<ol>
<li>Note this paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.01391">https://arxiv.org/pdf/2304.01391</a> for a survey on graph counterfactual. To create a synthetic dataset, see their Section 3.5.1, where the data creation method is detailed in <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.03662">https://arxiv.org/abs/2201.03662</a>.</li>
<li>See pygod <a target="_blank" rel="noopener" href="https://github.com/pygod-team/pygod">https://github.com/pygod-team/pygod</a> for outlier injection method to the graph dataset. Also, see Jingâ€™s paper <a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v231/gu24a/gu24a.pdf">https://proceedings.mlr.press/v231/gu24a/gu24a.pdf</a> for improvement.</li>
<li>Next Friday, you can try to talk about how to generate the synthetic data and how this falls into counterfactual category.</li>
</ol>
<p>æ‰€ä»¥å°±æ˜¯è¦å€Ÿé‰´åˆ›å»ºæ•°æ®é›†çš„æ–¹æ³•ã€‚è¿˜æœ‰å­¦ä¹ ä¸€äº›counterfactualã€‚ </p>
<h2 id="2024-Counterfactual-Learning-on-Graphs-A-Survey"><a href="#2024-Counterfactual-Learning-on-Graphs-A-Survey" class="headerlink" title="2024 Counterfactual Learning on Graphs: A Survey"></a>2024 Counterfactual Learning on Graphs: A Survey</h2><p>3.5.1 How to create synthetic dataset </p>
<h2 id="2022-Learning-Fair-Node-Representations-with-Graph-Counter-factual-Fairness"><a href="#2022-Learning-Fair-Node-Representations-with-Graph-Counter-factual-Fairness" class="headerlink" title="2022 Learning Fair Node Representations with Graph Counter factual Fairness"></a>2022 Learning Fair Node Representations with Graph Counter factual Fairness</h2><p>Two limitation on existing CF on graph:</p>
<ol>
<li>$S_i$ affect the predetection. Red</li>
<li>$S_i$ affect $A, X_i$ Green</li>
</ol>
<p>GEAR: Graph Counterfactually Fair Node Representation</p>
<ol>
<li>subgraph generation<br>Node <strong>Importance Score</strong> by prune range of casualmodel to <strong>ego-centric subgraph</strong>( node and its neighbour)</li>
<li>Counterfactual Data Argmentation:<br>Graph Auto encodder and fair contrains: <strong>self-pertubation</strong>(flip its $S_i$), <strong>neighbour pertubatiob</strong></li>
<li>Node Representation Learning  :<br>Siamese network to minimize discrepancy</li>
</ol>
<p><strong>Def, Graph conterfactual fairness:</strong><br>An encoder $\Phi(\cdot)$ satisfies graph counterfactual fairness if for any node $i$:<br>$$<br>P((Z_i)<em>{S \leftarrow sâ€™} | X &#x3D; \mathbf{X}, A &#x3D; \mathbf{A}) &#x3D; P((Z_i)</em>{S \leftarrow sâ€™â€™} | X &#x3D; \mathbf{X}, A &#x3D; \mathbf{A}),<br>$$<br>for all $sâ€™ \neq sâ€™â€™$, where $sâ€™, sâ€™â€™ \in {0, 1}^n$ are arbitrary sensitive attribute values of all nodes, $Z_i &#x3D; (\Phi(\mathbf{X}, \mathbf{A}))_i$ denotes the node representations.</p>
<p>$\Phi$, minimize the discrepancy between representation $\Phi(X_{S\leftarrow sâ€™}, A_{S\leftarrow sâ€™})$ and $\Phi(X_{S\leftarrow sâ€™â€™}, A_{S\leftarrow sâ€™â€™})$</p>
<h3 id="GEAR"><a href="#GEAR" class="headerlink" title="GEAR"></a>GEAR</h3><h3 id="1-subgraph-generation"><a href="#1-subgraph-generation" class="headerlink" title="1) subgraph generation"></a>1) subgraph generation</h3><p>Personalized Pagerank algorithm:<br>Importance score $\mathbf R&#x3D;\alpha (\mathbf I-(1-\alpha \mathbf {\bar A}))$, $\mathbf I$, identity<br>$R_{i,j}$ How node $j$ is important for node $i$, $\alpha \in [0,1]$</p>
<p>$\mathbf {\bar A}&#x3D;\mathbf A \mathbf D^{-1} $ column-normalized adjacency matric, $\mathbf D: \mathbf D_{i, i}&#x3D;\sum_j A{i, j}$</p>
<p>$\mathcal{G}^{(i)}&#x3D;Sub(i, \mathcal{G}, k)$ :, subgraph generation</p>
<ul>
<li><p>$\mathcal{G}^{(i)} &#x3D; { \mathcal{V}^{(i)}, \mathcal{E}^{(i)}, \mathbf{X}^{(i)} } &#x3D; { \mathbf{A}^{(i)}, \mathbf{X}^{(i)} },<br>$ Vertive, Edge, Features with $S&#x3D;{s_i}_{i&#x3D;1}^n $ includes in $X$, and $X^{\neg s} &#x3D; { x_1^{\neg s}, â€¦, x_n^{\neg s} } $, where $ x_i^{\neg s} &#x3D; x_i \setminus s_i$</p>
</li>
<li><p>$\mathcal{V}^{(i)} &#x3D; \text{TOP}(\mathbf{R}_{i,:}, k),$</p>
</li>
<li><p>$\mathbf{A}^{(i)} &#x3D; \mathbf{A}<em>{\mathcal{V}^{(i)}, \mathcal{V}^{(i)}}, \quad \mathbf{X}^{(i)} &#x3D; \mathbf{X}</em>{\mathcal{V}^{(i)}, :},<br>$,</p>
</li>
</ul>
<h3 id="2ï¼‰Counterfactual-Data-Augmentation"><a href="#2ï¼‰Counterfactual-Data-Augmentation" class="headerlink" title="2ï¼‰Counterfactual Data Augmentation"></a>2ï¼‰Counterfactual Data Augmentation</h3><p><strong>GraphVAG</strong>: graph variational auto-encoder<br>latent embedding $H&#x3D;{h_1, h_2, \cdots, h_k}$  $H$ is sampled from $q(H|X, A)$,  $p(ğ»)$ is a standard Normal prior distribution<br>$\mathcal{L}&#x3D;$</p>
<p>$\tilde{s}_i$: summary of neighbor info, aggregationof all nodes in subgarph $\mathcal{G}^{(i)}$<br>$\tilde{s}<em>i &#x3D; \frac{1}{|\mathcal{V}^{(i)}|} \sum</em>{j \in \mathcal{V}^{(i)}} s_j$</p>
<p>Discriminator,$D(\cdot)$<br>$D(\mathbf{H}, b)$  predicts the probability of whether the summary of sensitive attribute values is in range $b$</p>
<p>Fairness Constraint<br>$L_d &#x3D; \sum_{b \in B} \mathbb{E} [\log(D(\mathbf{H}, b))]$<br>$L_d$ is a regularizer to minimize the mutual information between the summary of sensitive attribute values and the<br>embeddings</p>
<p><strong>Final Loss</strong> for Counterfactual Data Augmentation<br>$L_a &#x3D; L_r + \beta L_d$<br>$\beta$ is a hyperparameter for the weight of fairness constraint<br>Use alternating SGD for optimization: </p>
<ol>
<li>minimize $L_{a}$ by fixing the discriminator and updating parameters in other parts; </li>
<li>minimize $âˆ’L_{a}$ with respect to the discriminator while other parts fixed.</li>
</ol>
<h4 id="Self-Perturbation"><a href="#Self-Perturbation" class="headerlink" title="Self-Perturbation"></a>Self-Perturbation</h4><p>$\overline{\mathcal{G}}^{(i)} &#x3D; { \mathcal{G}^{(i)}_{S_i \leftarrow 1-s_i} }$ (flipping sensitive feature)</p>
<h4 id="Neighbor-Perturbation"><a href="#Neighbor-Perturbation" class="headerlink" title="Neighbor-Perturbation"></a>Neighbor-Perturbation</h4><p>$\underline{\mathcal{G}}^{(i)} &#x3D; \left{ \mathcal{G}^{(i)}<em>{S^{(i)}</em>{\setminus i} \leftarrow \text{SMP}(S^{(i)}<em>{\mathcal{V}^{(i)}</em>{\setminus i}})} \right}$</p>
<p>subgraph $\mathcal{G}^{(i)}$ ego($i$)-center subgraph with noes $\mathcal{V}^{(i)}$, exclude node $i$: $\mathcal{V}^{(i)}<em>{\setminus i}$, randomly preterbe the sentsitice value of other nodes: $SMP(\mathcal{V}^{(i)}</em>{\setminus i})$</p>
<p>Reconstruction Loss (GraphVAE Module)<br>$L_r &#x3D; \mathbb{E}_{q(\mathbf{H}|X, A)} \left[ -\log(p(X, A | \mathbf{H}, S)) \right] + \text{KL}[q(\mathbf{H} | X, A) | p(\mathbf{H})]$</p>
<h3 id="3-Fair-Representation-learning"><a href="#3-Fair-Representation-learning" class="headerlink" title="3) Fair Representation learning"></a>3) Fair Representation learning</h3><p><strong>Fairness Loss</strong><br>$<br>L_f &#x3D; \frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}} \left( (1 - \lambda_s) d(z_i, \bar{z}_i) + \lambda_s d(z_i, \underline{z}_i) \right),<br>$<br>$\lambda_s$ hyperparam control neig-preturbation weight</p>
<p><strong>Node Representations</strong></p>
<ul>
<li>$<br>z_i &#x3D; (\phi(\mathbf{X}^{(i)}, \mathbf{A}^{(i)}))_i,<br>$</li>
<li>$<br>\bar{z}<em>i &#x3D; \text{AGG} \left( \left{ (\phi(\mathbf{X}^{(i)}</em>{S_i \leftarrow 1-s_i}, \mathbf{A}^{(i)}_{S_i \leftarrow 1-s_i}))_i \right} \right),<br>$</li>
<li>$<br>\underline{z}<em>i &#x3D; \text{AGG} \left( \left{ (\phi(\mathbf{X}^{(i)}</em>{S_i \leftarrow \text{SMP}(S^{(i)}<em>{\mathcal{V}^{(i)}</em>{\setminus i}})}, \mathbf{A}^{(i)}<em>{S_i \leftarrow \text{SMP}(S^{(i)}</em>{\mathcal{V}^{(i)}_{\setminus i}})})_i \right} \right),<br>$</li>
</ul>
<p>Prediction Loss<br>$L_p &#x3D; \frac{1}{n} \sum_{i \in [n]} l(f(z_i), y_i),$ $l$: could be CE(Cross entropy), $f(\cdot)$ makes predictions for downstream tasks with the representations, i.e.$ \hat y_i&#x3D;f(z_i)$</p>
<p>Overall Loss<br>$<br>L &#x3D; L_p + \lambda L_f + \mu | \theta |^2,<br>$</p>
<h3 id="Dataset-creation"><a href="#Dataset-creation" class="headerlink" title="Dataset creation"></a>Dataset creation</h3><p>Sensitive Attributes<br>$S_i \sim \text{Bernoulli}(p),$ $p&#x3D;0.4$ percent $S_i&#x3D;1$</p>
<p>Latent Embeddings<br>$Z_i \sim \mathcal{N}(0, \mathbf{I}),$ <br>$\mathbf{I}$ identity, dimension of $Z_i$: $d_s&#x3D;50$</p>
<p>Node Features<br>$X_i &#x3D; \mathcal{S}(Z_i) + S_i \mathbf{v},$<br>sampling operation $S(\cdot)$ select 25 dims from $Z_i$, $\mathbf{v} \sim \mathcal{N}(0, \mathbf{I})$</p>
<p>Graph Structure<br>$P(A_{i,j} &#x3D; 1) &#x3D; \sigma(\text{cos}(Z_i, Z_j) + a \mathbf{1}(S_i &#x3D; S_j)),$<br>$\sigma$ sigmoid function, $\mathbf{1}(S_i &#x3D; S_j)&#x3D;&#x3D;S_i &#x3D; S_j. \alpha&#x3D;0.01$</p>
<p>Node Labels<br>$Y_i &#x3D; \mathcal{B}(w Z_i + w_s \frac{\sum_{j \in \mathcal{N}_i} S_j}{|\mathcal{N}_i|}),$<br>$\mathcal{B}$ Bernulli distribution,$\mathcal{N}_i$ set of neighbors of node i $w, w_i$ weight vector</p>
<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><p>Using Synthetic dataset, Bail, Credit</p>
<h2 id="24-Three-Revisits-to-Node-Level-Graph-Anomaly-Detection"><a href="#24-Three-Revisits-to-Node-Level-Graph-Anomaly-Detection" class="headerlink" title="24 Three Revisits to Node-Level Graph Anomaly Detection"></a>24 Three Revisits to Node-Level Graph Anomaly Detection</h2><p>Outliers, Message Passing and Hyperbolic Neural Networks</p>
<h3 id="Previous-Outlier-injection-method"><a href="#Previous-Outlier-injection-method" class="headerlink" title="Previous Outlier injection method"></a>Previous Outlier injection method</h3><p>$\mathcal{G}&#x3D;(\mathcal{V}, \mathcal{E}, X, y)$: vertice set, edge set, attibute matrix, label of class</p>
<ul>
<li><p><strong>Contextual(cntxt.) outlier injection</strong><br>Normalize features $x_iâ€™&#x3D;\frac{x_i}{||x_i||_1}$<br>Sample $o$ nodes from $\mathcal{V}$ as $\mathcal{V}_c$. without replacement<br>For node $i$ in $\mathcal{V}_c$, sample $q$ nodes from $\mathcal{V}_r&#x3D;\mathcal{V}- \mathcal{V}_c$, among them choose the farthest one $j &#x3D; \text{argmax}_k(||x_iâ€™-x_kâ€™||_2)$ to replace $x_i$ with $x_j$.</p>
</li>
<li><p><strong>Strctural(stct.) outlier injection</strong><br>create $t$ groups sized $s$ with anomalous nodes.<br>sample $o&#x3D;t\times s$ from $\mathcal{V}$ without replacement<br>Then randoms partition into $t$ groups.<br>Add edges to make them a clique(fully connected), then drop edges with $p$ probability</p>
</li>
</ul>
<h4 id="Score-function"><a href="#Score-function" class="headerlink" title="Score function"></a>Score function</h4><p>The farthest node will have large $||\tilde{\mathbf x}_i||_2$ <br>A structural outlier node $i$ will have many neighbors leads to large $||\tilde{\mathbf a}_i||_1$ </p>
<p>Score function: $score_{norm}(i)&#x3D;\alpha||\tilde{\mathbf x}_i||_2+(1-\alpha)||\tilde {\mathbf a}_i||_1$,  $\tilde{\mathbf x}_i$: $x_i$ after outlier injection, $\tilde{\mathbf a}<em>i$: $a_i$ after outlier injection, $A</em>{ii}&#x3D;1$<br>where cntxt OD, $\alpha&#x3D;1$, stct OD, $\alpha&#x3D;0$ :  $\alpha$ ratio of two methods </p>
<p>test 1: ROC-AUC<br>For each dataset, use original dataset v.s. l2-nrom for each $x_i$<br>do anomaly injection. apply GAD Method to get  $score_{norm}$</p>
<h3 id="Novel-Anomaly-injection-method"><a href="#Novel-Anomaly-injection-method" class="headerlink" title="Novel Anomaly injection method"></a>Novel Anomaly injection method</h3><h2 id="Sum-in-terms-of-Dataset"><a href="#Sum-in-terms-of-Dataset" class="headerlink" title="Sum in terms of Dataset"></a>Sum in terms of Dataset</h2><p>ä»æ•°æ®é›†çš„è§’åº¦æ¥è¯´ï¼š</p>
<h3 id="FairGAD-1"><a href="#FairGAD-1" class="headerlink" title="FairGAD:"></a>FairGAD:</h3><p>Reddit:</p>
<ul>
<li>æ•°æ®æ¥æºï¼šPost on politic related subReddit</li>
<li>Labelling Y: based on FACTOID(Sakketou et al., 2022), use the num of posted link(left or right)</li>
<li>Graph construciton:</li>
</ul>
<p><br><br><br><br><br><br><br><br><br><br><br><br>\</p>
<h1 id="2024-05-31-Meeting"><a href="#2024-05-31-Meeting" class="headerlink" title="2024.05.31 Meeting"></a>2024.05.31 Meeting</h1><p>Preparation: </p>
<ol>
<li>è®¨è®ºå¯¹äºSynthetic dataset æ€ä¹ˆåˆ›å»ºçš„ç†è§£ã€‚</li>
<li>å¯¹outlier datasetæ€ä¹ˆåˆ›å»ºçš„ç†è§£ã€‚</li>
<li>fair + outlier (å‚è€ƒFairGADé‚£ç¯‡çš„åˆ›å»º)</li>
</ol>
<!-- è¿™ä¸€å‘¨èŠ±äº†ä¸‰å››å¤©åœ¨ä¿¡ä¸€çš„èº«ä¸Šï¼Œä¸€ç§åƒ­è¶Šçš„å¿«ä¹ã€‚
ä½“æ‚Ÿæ˜¯ï¼Œ 
1. å­¦ä¸œè¥¿çš„ç›®çš„æ€§è¿˜æ˜¯ä¸å¤Ÿæ˜æ˜¾ã€‚
2. è¾¹å¬è¯¾è¾¹çœ‹è®ºæ–‡ä¼šå²·å¿æé«˜ç›®çš„æ€§å’Œæé«˜æ•ˆç‡ã€‚
3. å‡å°‘è¿‡åº¦åŠŸåˆ©çš„éœ€æ±‚ï¼Œå­¦ä¸€äº›æœ‰è¶£çš„ä¸œè¥¿ï¼Œå°½é‡é¿å¼€äººã€‚
4. èƒŒå•è¯ã€‚GREè¦å¯„äº†ã€‚ -->


<p><strong>Meeting</strong></p>
<ol>
<li><p>Plan for subgroups:<br>Mo, We 1-2 p.m.</p>
</li>
<li><p>intro to all projects<br>HNN: Convolution $\rightarrow$ HNN<br>CNN(T(x)) Paralell Translation equivalence</p>
</li>
</ol>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p><img src="/2024-05-08-papers/image-1.png" alt="alt text"> from FairGAD(2024)<br>##<br>Pokec: </p>
<ul>
<li>source paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.01454">https://arxiv.org/pdf/2009.01454</a></li>
<li>repo: FairGNN  <a target="_blank" rel="noopener" href="https://github.com/EnyanDai/FairGNN">https://github.com/EnyanDai/FairGNN</a><br>sampled from <a target="_blank" rel="noopener" href="https://snap.stanford.edu/data/soc-Pokec.html">https://snap.stanford.edu/data/soc-Pokec.html</a></li>
</ul>
<p>Bail, Credit, German:</p>
<ul>
<li>source paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.05233">https://arxiv.org/pdf/2108.05233</a> (Dong et al. 2022)<br>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1102.2166">https://arxiv.org/pdf/1102.2166</a> (2012)</li>
<li>repo: EDITS <a target="_blank" rel="noopener" href="https://github.com/yushundong/EDITS">https://github.com/yushundong/EDITS</a></li>
</ul>
<p>(æ„Ÿè§‰è®ºæ–‡éƒ¨åˆ†å¼•ç”¨åäº†)</p>
<p>German</p>
<ul>
<li>source paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.13186">https://arxiv.org/pdf/2102.13186</a> (2021)</li>
<li>repo: NIFTY <a target="_blank" rel="noopener" href="https://github.com/HongduanTian/NIFTY">https://github.com/HongduanTian/NIFTY</a></li>
</ul>
<p>UCSD34:</p>
<ul>
<li>repo: <a target="_blank" rel="noopener" href="https://networkrepository.com/socfb-UCSD34.php">https://networkrepository.com/socfb-UCSD34.php</a></li>
</ul>
<h1 id="2024-06-03-Meeting"><a href="#2024-06-03-Meeting" class="headerlink" title="2024.06.03 Meeting"></a>2024.06.03 Meeting</h1><ol>
<li>Gujingå­¦å§çš„è®ºæ–‡æ˜¯ unsupervised learningï¼ŒæŒ‰ç…§å¥¹åœ¨pygodé‡Œé¢çš„æ–¹æ³•ï¼ŒæŠŠäºŒåˆ†ç±»çš„ä»»åŠ¡ç”¨fiarness metrixï¼Œç”¨counterfacutalé‡Œçš„è¯„åˆ¤æ ‡å‡†ã€‚EOO, SP, CF(åªåœ¨Syntheticé‡Œæœ‰)</li>
</ol>
<p>æ‰€ä»¥è¦å†™çš„æ˜¯ï¼š </p>
<ol>
<li>Fairness metrix çš„è®¡ç®—ï¼Œå¤šç§</li>
<li>ä½¿ç”¨å„ç§æ–¹æ³•è·‘ä¸€ä¸‹æ•°æ®é›†ã€‚å¾—åˆ°fairå’Œaccuracyï¼Œå‚è€ƒåˆ«çš„è®ºæ–‡ã€‚</li>
</ol>
<p>é•¿æœŸä»»åŠ¡ï¼š</p>
<ol>
<li><p>WSDM 22â€™ çš„åšcounterfactual Data argumentation å’ŒGADçš„æ–¹æ³•æ— å…³ã€‚ï¼Œæ€»çš„æ¥è¯´æ˜¯åœ¨ä¸åŒGAD æ–¹æ³•ä¸Šconsistently improve fairness. WSDM æ˜¯åœ¨æ•°æ®é›†çš„encodingå’Œencodingä¸Šç”¨çš„fairnessã€‚<br>Detection ä¹Ÿæ˜¯ç”¨en&#x2F;decodingåšçš„ï¼Ÿæœ‰çš„ç”¨GNNä¹Ÿå°±å¯ä»¥predictionäº†ã€‚å¯ä»¥è¯•ç€ç”»ä¸€ä¸ªå›¾ã€‚ </p>
</li>
<li><p>224Wå¯ä»¥çœ‹17-19ï¼Œ 21å’Œå‰é¢encodingéƒ¨åˆ†åœ¨å­¦ä¸€ä¸‹ã€‚</p>
</li>
<li><p>å› æœæ¨æ–­çš„Counterfactualéƒ¨åˆ†çš„å…¬å¼</p>
</li>
</ol>
<h2 id="Execute"><a href="#Execute" class="headerlink" title="Execute"></a>Execute</h2><p>6.3: è§£å†³</p>
<ol>
<li>Synthetic dataset have about $\frac{|V|^2}{2}$ edges when v&#x3D;2000(paper), edge should be about 4000?<br>solved by Finding source code of paper in GEAR repo</li>
<li>Threading problem with python not shoot<br>solved by commenting the 22th line in loader.py # from ogb.nodeproppred import PygNodePropPredDataset</li>
</ol>
<p>6.4</p>
<ol>
<li>å¯ä»¥ä½¿ç”¨ä¸€äº›æ–¹æ³•ï¼Œ<br>WSDM 22 GEAR çš„è®ºæ–‡é‡Œç”¨GCN, GraphSAGE, GIN, C-ENC, FairGNN, NIFTY-GCN, NIFTY-SAGE, and GEAR<br>Gu 24 HNN çš„è®ºæ–‡ç”¨pygodçš„GADçš„åº“<br>ä½†æ˜¯éƒ½æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä»£ç </li>
</ol>
<p>Gear&#x2F;src</p>
<ul>
<li>utils.py: <ol>
<li>load_dataset, sub function</li>
<li>accuracy</li>
</ol>
</li>
<li>Preprocessing.py:<ol>
<li>load_data() deal with params</li>
<li>generate cf subgraph(æ— å…³)</li>
<li>generate_synthetic_data</li>
</ol>
</li>
<li>models.py:<ol>
<li>GCN, GIN, JK, SAGE, Encoder_DGI, GraphInMax, Encoder, Classifier,<br>  GraphCF,</li>
</ol>
</li>
<li>main.py<ol>
<li>parser.argment()</li>
<li>evaluate: acc, fairness</li>
<li>compute loss, evaluate sf</li>
<li>train test</li>
</ol>
</li>
</ul>
<p>HNN_GAD<br>æ ¹æ®æˆ‘çš„è§‚å¯Ÿï¼Œè¿™ç¯‡é‡Œé¢åªå†™äº†è‡ªå·±çš„æ–¹æ³•çš„ä»£ç ã€‚</p>
<p>6.5 Meeting<br>å†³å®šç”¨ray tuneæ¥è°ƒå‚<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html</a></p>
<p>6.7<br>æ­£åœ¨å†™scratch_main.py<br>ç–‘é—®ï¼š<br>    1. è¿™ä¸ªtrain, val, test æ˜¯æ€ä¹ˆåˆ†çš„,å­å›¾è¿˜æ˜¯ï¼Ÿ<br>    ???<br>        evaluateå’Œtestæœ‰ä»€ä¹ˆåŒºåˆ«<br>    2. evaluateé‡Œçš„counterfactual metrixæ˜¯æ€ä¹ˆç®—çš„ï¼Ÿ<br>    3. Injectionçš„å‚æ•°<br>    4. githubæ€ä¹ˆä¸Šä¼ <br>    5. </p>
<p>6.11<br>    Paulå¼ºè°ƒSRSæ˜¯ä¸ºäº†ä¸°å¯Œç®€å†çš„ï¼Œè¦å¹²å¾ˆå¤šè·Ÿç”³ç ”ç›¸å…³çš„äº‹æƒ…ã€‚<br>    çœ‹çœ‹æ•™æˆç°åœ¨åœ¨å¹²ä»€ä¹ˆï¼Œfellowshipæ˜¯å•¥ï¼Œï¼Œï¼Ÿï¼Ÿï¼Ÿï¼Ÿpracticing interviewã€‚<br>    å‡å¤šå°‘å­¦æ ¡ï¼Ÿï¼Ÿï¼Ÿæ²¡å¬æ‡‚<br>    5-8<br>    16ï¼Ÿï¼Ÿï¼Ÿ</p>
<h1 id="2024-06-12-Meeting"><a href="#2024-06-12-Meeting" class="headerlink" title="2024.06.12 Meeting"></a>2024.06.12 Meeting</h1><ol>
<li>CF + guå­¦å§çš„ä¸‰ä¸ªæ–¹æ³•</li>
<li>gpuçš„é—®é¢˜è¿˜æ²¡æœ‰è§£å†³</li>
<li>injectçš„å¥½åƒä¸æ˜¯ç‰¹åˆ«å½±å“fairness</li>
</ol>
<p>æ•°æ®é›†çš„æ„é€ æ–¹é¢åœ¨sensitivity groupå’Œæ˜¯ä¸æ˜¯outlierä¹‹é—´åŠ ä¸Šcasualityã€‚FairGADç”¨äº†debiaserçš„æ–¹æ³•ä½¿fairnessé«˜äº†ä¸€ç‚¹<br>run Jingâ€™s method for GAD: shengenåœ¨åš<br>Check with Yifei for GPUï¼šcheckäº†ï¼Œç°åœ¨ä¸€äº›modelåœ¨å¤§çš„æ•°æ®é›†ä¸Šè¿˜è¦åˆ†batchã€‚<br>Check CF scoresï¼šè£…äº†ä¸¤å¤©ç¯å¢ƒï¼Œ<br>Complete remaining experimentsï¼šæ²¡æœ‰<br>brainstorm so that outlier injection contains sensitivityï¼šè®¤ä¸º<br>CF using DA</p>
<p>Motivationï¼š outlier detectionï¼Œ<br>Fairnesæœ‰æ•ˆçš„æ•°æ®é›†ï¼š<br>Outlierçš„æ³¨å…¥ï¼š</p>
<h2 id="6-12-é—®é¢˜"><a href="#6-12-é—®é¢˜" class="headerlink" title="6.12 é—®é¢˜"></a>6.12 é—®é¢˜</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><p>ä¸€ä¸ªä¸‹åˆä¸»è¦éƒ½åœ¨è§£å†³gpuçš„é—®é¢˜ï¼Œ</p>
<h4 id="1"><a href="#1" class="headerlink" title="1"></a>1</h4><p>é¦–å…ˆç›®å‰æœ€å¤§çš„è°œå›¢æ˜¯Pygodä¸­AdONE(gpu&#x3D;0)è¿™é‡Œçš„å…‰è°±ä¸ºå•¥åªèƒ½æ˜¯0<br>æˆ‘å»æ‰¾äº†æºä»£ç ï¼Œåº”è¯¥å¯ä»¥æ˜¯int cudaçš„idï¼Œæ‰€ä»¥ç†è®ºä¸Šåº”è¯¥æ˜¯0-7 éƒ½å¯ä»¥çš„ï¼Œä½†æ˜¯åªæœ‰0å¯è¡Œï¼Œ<br>ä¸»è¦ä»£ç <br>pygod&#x2F;pygod&#x2F;detector&#x2F;base <br>pygod&#x2F;utils&#x2F;utility.py çš„<code>validate_device(gpu_id)</code>å‡½æ•°<code>gpu_id</code>å°±æ˜¯<code>DOMINANT(gpu=0)</code>é‡Œçš„<code>gpu</code></p>
<h4 id="2"><a href="#2" class="headerlink" title="2"></a>2</h4><p>è¿˜æœ‰ä¸€ä¸ªå¾ˆè ¢å¾—å·²ç»è¢«è§£å†³çš„é—®é¢˜æ˜¯<br>ä¸ºä»€ä¹ˆ.shæ–‡ä»¶ä¼šæŠ¥ã€‚ ä¹‹å‰ä¸€ç›´ä¸æ˜ç™½ä¸ºä»€ä¹ˆå‘½ä»¤è¡Œå°±æ²¡é—®é¢˜ï¼Œä½†æ˜¯.sh å°±ä¸å¯ä»¥ï¼Œåæ¥å‘ä¿¡å•Šæ˜¯æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«<br>    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.35 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.01 GiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 16.79 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>è¿™ç§é”™ï¼Œ<br>ä¿®æ”¹çš„æ–¹å¼æ˜¯</p>
<ol>
<li><p>åœ¨ray tune é‡ŒæŠŠç½‘ç»œå¾—å¤§å°ä¿®æ”¹å°ä¸€ç‚¹ï¼Œå¹¶ä¸”åˆ†batchï¼Œé€šè¿‡åœ¨trainæœ€åé‡Šæ”¾å†…å­˜</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">defv train():</span><br><span class="line">    ... ...</span><br><span class="line">    torch.cuda.empty_cache() </span><br><span class="line">    return</span><br></pre></td></tr></table></figure></li>
<li><p>åœ¨ray tune åˆ†batchã€‚åœ¨mainå¾—ç¬¬ä¸€å¥åŠ ä¸Š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[&#x27;PYTORCH_CUDA_ALLOC_CONF&#x27;] = &#x27;max_split_size_mb:128&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>æœ¬èº«å› ä¸ºdatasetå’Œmodelçš„å¤§å°ä¸åŒï¼Œæ‰€ä»¥æœ‰çš„æ¨¡å‹è¢«è·‘å‡ºæ¥çš„å¯è¡Œæ€§å°±æ˜¯è¦å°ä¸€ç‚¹<br>æ¯”å¦‚ä»synthetic &lt; german &lt; bail &lt; credit &lt; pokec<br>å‰ä¸‰ä¸ªæ˜¯å¯ä»¥è·‘æ‰€æœ‰æ¨¡å‹çš„ï¼Œ<br>ä½†æ˜¯creditä¸å¯ä»¥è·‘gaan, ä¼šç«™600GiBçš„å†…å­˜ï¼Œguideä¹Ÿéå¸¸æ…¢ï¼Œ credit+guideæ ¹æœ¬æ²¡ä¸Šgpuï¼Ÿï¼Ÿï¼Ÿ<br>ç„å­¦</p>
</li>
</ol>
<h3 id="6-14-CF"><a href="#6-14-CF" class="headerlink" title="6.14 CF"></a>6.14 CF</h3><p>cf_eoo, cf_dp, df, eoo, dp åœ¨è®ºæ–‡ä¸­åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆ<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.03662">https://arxiv.org/pdf/2201.03662</a></p>
<p>sens rate </p>
<p>è®ºæ–‡ç®—cfçš„æ–¹æ³•æ˜¯:<br>å¯¹æ¯”åŸå›¾å’Œç»è¿‡ä¿®æ”¹sens featureï¼ˆç±»ä¼¼äºperturbeçš„æ‰‹æ³•ï¼‰ï¼Œé€šè¿‡$hat y$ä¹‹é—´çš„æ¥ç®—cf</p>
<p>é‡ç‚¹æ˜¯å¦‚ä½•å¾—åˆ° modified dataï¼Œ ä¹Ÿå°±æ˜¯ evaluate ä¸­çš„ data_cf</p>
<p>éšæœºå– sens_rate * N ä¸ªèŠ‚ç‚¹ï¼Œä½¿$S_i$ä¸º1ï¼Œå‰©ä¸‹ä¸º0.</p>
<h3 id="GEARé…ç¯å¢ƒè¸©å‘"><a href="#GEARé…ç¯å¢ƒè¸©å‘" class="headerlink" title="GEARé…ç¯å¢ƒè¸©å‘"></a>GEARé…ç¯å¢ƒè¸©å‘</h3><p>pygå¾ˆçƒ¦äºº<br>æˆ‘æ˜¯å…ˆè£…äº†torch1.6.0 + cu10.2<br>ç„¶åå‘ç°pyg&#x3D;1.3.0 æ˜¯æœ€è€ç‰ˆæœ¬çš„ï¼Œå°±googleåˆ°äº†pygçš„çš„source code ï¼š <a target="_blank" rel="noopener" href="https://github.com/pyg-team/pytorch_geometric/releases/tag/1.3.0">https://github.com/pyg-team/pytorch_geometric/releases/tag/1.3.0</a><br>ç„¶åå°±åº”è¯¥python setup.py install,ä½†æ˜¯<strong>ç½‘å¾ˆæ…¢</strong>ï¼Œ æ‰€ä»¥è¦å¤šç­‰ä¸€ä¼š<br>ç„¶åçœ‹åˆ°readmeä¹‹åæ‰‹åŠ¨è£…äº†ä¸ªtorch-sparseä¸€ç±»çš„whlï¼š <a target="_blank" rel="noopener" href="https://data.pyg.org/whl/">https://data.pyg.org/whl/</a><br>åæ¥å¾ˆå‚»çš„æ‰å‘ç°python setup.py installï¼Œç­‰äº†3åˆ†é’Ÿä¹‹åæŠ¥é”™ï¼Œæ— pytest-runnerï¼Œ äºæ˜¯è¿›setup.pyçœ‹äº†ä¸€ä¸‹ä¹‹åæ‰‹åŠ¨pip install pytest-runner pytest pytest-cov mock,<br>ç„¶åpython setup.py installä¸€ä¸‹å­å°±å¥½äº†ï¼Œäºæ˜¯åˆæ‰‹åŠ¨ pip install pandas matplotlib Cpython cytoolz aif360</p>
<p>è£…åˆ°aif360 æŠ¥é”™Failed building wheel for llvmliteï¼Œåº”è¯¥æ˜¯æ²¡æœ‰llvmï¼Œäºæ˜¯æ‰‹åŠ¨æœ¬åœ°è£…<br>è£…äº†9.0.0çš„ç‰ˆæœ¬</p>
<pre><code>å¦‚æœ `llvmlite` çš„é¢„æ„å»ºäºŒè¿›åˆ¶æ–‡ä»¶å’Œ `conda` æ–¹æ³•éƒ½æ— æ³•è§£å†³é—®é¢˜ï¼Œæ™®é€šç”¨æˆ·å¯ä»¥åœ¨ç”¨æˆ·ç›®å½•ä¸­å®‰è£… LLVMï¼Œè€Œä¸éœ€è¦ `sudo` æƒé™ã€‚



<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¸‹è½½å¹¶è§£å‹ LLVM</span></span><br><span class="line">wget https://github.com/llvm/llvm-project/releases/download/llvmorg-11.1.0/llvm-11.1.0.src.tar.xz</span><br><span class="line">tar -xf llvm-11.1.0.src.tar.xz</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»ºæ„å»ºç›®å½•</span></span><br><span class="line"><span class="built_in">mkdir</span> llvm-11.1.0.build</span><br><span class="line"><span class="built_in">cd</span> llvm-11.1.0.build</span><br><span class="line"></span><br><span class="line"><span class="comment"># é…ç½®ç¼–è¯‘ï¼ˆå®‰è£…åœ¨ç”¨æˆ·ç›®å½•ï¼‰</span></span><br><span class="line">cmake -G <span class="string">&quot;Unix Makefiles&quot;</span> -DCMAKE_INSTALL_PREFIX=<span class="variable">$HOME</span>/llvm ../llvm-11.1.0.src</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¼–è¯‘å’Œå®‰è£…</span></span><br><span class="line">make -j$(<span class="built_in">nproc</span>)</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>

ç„¶åè®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨æœ¬åœ°å®‰è£…çš„ LLVMï¼š

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HOME</span>/llvm/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$HOME</span>/llvm/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure>



<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install llvmlite --no-binary llvmlite</span><br></pre></td></tr></table></figure>

ä¹‹åå†å®‰è£… `aif360`ï¼š

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install aif360</span><br></pre></td></tr></table></figure>
</code></pre>
<p>ä½†æ˜¯è¿˜æ˜¯ä¼šæŠ¥wheel build å¤±è´¥çš„é”™è¯¯ã€‚<br>äºæ˜¯å°±ç›´æ¥ç»•è¿‡aif360,å› ä¸ºåªç”¨äº†ä¸¤ä¸ªå‡½æ•°ï¼Œæ‰€ä»¥ç›´æ¥å¤åˆ¶å‡½æ•°å’Œå¿…è¦çš„utilè¿‡æ¥äº†ï¼Œç»•è¿‡å®‰è£…aif360çš„é—®é¢˜äº†ã€‚<br>å®é™…ä¸Šaif360å¯¹python 3.8ä¹‹åæ‰æ¯”è¾ƒå…¼å®¹ï¼Œæ‰€ä»¥ä»¥åç”¨æ–°ä¸€ç‚¹çš„ç¯å¢ƒã€‚</p>
<p>ç„¶åé‡åˆ°äº†AttributeError: Canâ€™t get attribute â€˜DataEdgeAttrâ€™<br>import torch_geometric.transforms as T<br>from ogb.nodeproppred import PygNodePropPredDataset<br>shoujuå­¦å§æé†’å¯ä»¥csdnï¼Œï¼ˆè¿™æ¬¡æä¾›çš„è§£å†³æ–¹æ¡ˆç¡®å®å’Œgptä¸ä¸€æ ·ï¼‰<a target="_blank" rel="noopener" href="https://blog.csdn.net/oqqENvY12/article/details/129786928">https://blog.csdn.net/oqqENvY12/article/details/129786928</a> ä¹Ÿæœ‰ä¸€éƒ¨åˆ†ç‰ˆæœ¬è¿‡è€çš„é—®é¢˜<br>ä½†æ˜¯é€šè¿‡è§‚å¯Ÿï¼Œæ˜¯è·¯å¾„é—®é¢˜ï¼ŒæŠŠä¸€ä¸ªç›¸å¯¹main.py line 541çš„ç›¸å¯¹è·¯å¾„æ”¹æˆç»å¯¹è·¯å¾„å°±æˆåŠŸäº†ï¼Œ<br>æˆ‘è§‰å¾—<strong>13.23çš„æœåŠ¡å™¨åœ¨è·¯å¾„ä¸Šç¡®å®æœ‰äº›ç„ä¹</strong></p>
<ul>
<li>è¿è¡Œç»“æŸä¹‹åæ²¡æœ‰åŠæ³•è‡ªåŠ¨å…³é—­</li>
<li>german æ•°æ®é›†æ— æ³•æ­£å¸¸ç”Ÿæˆ</li>
</ul>
<h3 id="Guå­¦å§çš„ä¸‰ä¸ªmodel"><a href="#Guå­¦å§çš„ä¸‰ä¸ªmodel" class="headerlink" title="Guå­¦å§çš„ä¸‰ä¸ªmodel"></a>Guå­¦å§çš„ä¸‰ä¸ªmodel</h3><p>ä¸‡èƒ½çš„CSDN<br>Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(546): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.<em>, but conda is ignoring the .</em> and treating it as 1.7.1<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/ermmtt/article/details/132628639">https://blog.csdn.net/ermmtt/article/details/132628639</a></p>
<h3 id="Batché—®é¢˜"><a href="#Batché—®é¢˜" class="headerlink" title="Batché—®é¢˜"></a>Batché—®é¢˜</h3><h3 id="Python-æ’ä»¶"><a href="#Python-æ’ä»¶" class="headerlink" title="Python æ’ä»¶"></a>Python æ’ä»¶</h3><p>è¿™ä¸ªæ˜¯æœ€å‚»çš„ï¼Œä¸‹è½½äº† vsix ä¹‹åå‘ç° vscode ç‰ˆæœ¬å¯¹ä¸ä¸Šï¼Œ ç„¶åæ›´æ–°äº†ä¸€ä¸‹ vscode å°±å¥½äº†ã€‚ã€‚ã€‚</p>
<h1 id="2024-06-24-Meeting"><a href="#2024-06-24-Meeting" class="headerlink" title="2024.06.24 Meeting"></a>2024.06.24 Meeting</h1><h2 id="2024-06-19"><a href="#2024-06-19" class="headerlink" title="2024.06.19"></a>2024.06.19</h2><p>(åˆšåˆšæ‰äº†è§£ Encoder å’Œ Decoder ä¹Ÿç®—æ˜¯ GNNï¼Œç„¶åçœ‹äº†ä¸€äº›ä¸œè¥¿<br>(OIä½¬å†™çš„GEARä»£ç ï¼Œçœ‹ä¸æ‡‚ï¼Œ</p>
<h1 id="2024-07-14"><a href="#2024-07-14" class="headerlink" title="2024.07.14"></a>2024.07.14</h1><!-- ç«Ÿç„¶è¿‡äº†ä¸€ä¸ªæœˆäº†ã€‚ -->

<h2 id="2024-06-10å¼€ä¼š"><a href="#2024-06-10å¼€ä¼š" class="headerlink" title="2024.06.10å¼€ä¼š"></a>2024.06.10å¼€ä¼š</h2><ol>
<li>åšbenchmark. + Jingå­¦å§çš„ä¸‰ä¸ªHNNå·²ç»æ”¹æˆäº†Classä½†æ˜¯AUCè¿˜æ˜¯å¤ªä½äº†ã€‚ã€‚ã€‚</li>
<li>æ‰‹å†™encoderå’Œdecoderã€‚ç»“æ„æœªçŸ¥ï¼Œä½†æ˜¯ä¸»è¦æ˜¯ä¿®æ”¹Loss Function???åŸºäºCFçš„ï¼Œå¯ä»¥åœ¨dominantä¸Šä¿®æ”¹</li>
<li>è§£é‡Šä¸ºä»€ä¹ˆCFæ˜¯ä½çš„ï¼Œçœ‹decoderå‡ºæ¥çš„sensâ€™ï¼Œæ˜¯è¿˜åŸäº†sensè¿˜æ˜¯éƒ½æ˜¯1&#x2F;2.è¿™ä¸¤è€…éƒ½æ˜¯å¯ä»¥è§£é‡Šçš„</li>
<li>åœ¨å­¦ä¸€ä¸‹CFä¹‹ç±»çš„ç†è®ºã€‚</li>
<li>GNNNNNNNNNN</li>
</ol>
<h1 id="2024-08-15"><a href="#2024-08-15" class="headerlink" title="2024.08.15"></a>2024.08.15</h1><!-- ç«Ÿç„¶åˆè¿‡äº†ä¸€ä¸ªæœˆäº†ã€‚ -->
<!-- æŠ½è±¡ï¼Œå®Œå…¨ä¸çŸ¥é“è‡ªå·±åœ¨å¹²å•¥ã€‚ã€‚ã€‚ -->

<ol>
<li>DOIMINANT 19, DONE 21, gadnr 24, ada-gad 234. (CFGN denied)</li>
<li>benchmarkå’Œä¸€äº›sens reconstruct çš„å€¼å¯¹æ¯”ï¼ˆä¹‹å‰è²Œä¼¼åªä½œäº†guide 21çš„ï¼‰</li>
<li>è®ºæ–‡ä¹Ÿçœ‹ä¸å‡ºæ¥åœ¨å†™å•¥</li>
<li></li>
</ol>
<p>08.18 äº¤srsæŠ¥å‘Š<br>08.25 GRE è€ƒè¯•ï¼Œå¯„<br>æ‰¾solæ•™æˆè¯¢é—®music techæ–¹å‘çš„é—®é¢˜<br>è®ºæ–‡è®ºæ–‡è®ºæ–‡</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/08/2024-05-08-Meetings-log/" data-id="cm9bnagq00018zc3dddgmbccv" data-title="papers" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-03-08-LHY-ML" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/08/2024-03-08-LHY-ML/" class="article-date">
  <time class="dt-published" datetime="2024-03-08T15:36:24.000Z" itemprop="datePublished">2024-03-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/08/2024-03-08-LHY-ML/">LHY ML</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php">https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php</a></p>
<h2 id="2-18"><a href="#2-18" class="headerlink" title="2&#x2F;18"></a>2&#x2F;18</h2><h3 id="Video-2"><a href="#Video-2" class="headerlink" title="Video 2"></a>Video 2</h3><p>Piecewise Linear</p>
<p>$y &#x3D; c * Sigmoid(b+wx_1)$, w, b, c,</p>
<p>$\theta$: A vector of all unknown variable<br>Gradient $ g &#x3D;\nabla L(\theta^{0}) $<br>$\eta$: learning rate<br>Batch, update, Epoch</p>
<p>Activation function:<br>Sigmoid function<br>Rectified Linear Unit (ReLU) max(0, )</p>
<h3 id="Pytorch-1-2"><a href="#Pytorch-1-2" class="headerlink" title="Pytorch 1&#x2F;2"></a>Pytorch 1&#x2F;2</h3><p>Mainly introduce some practical advice for coding. </p>
<h3 id="Background-propagation"><a href="#Background-propagation" class="headerlink" title="Background propagation"></a>Background propagation</h3><p>Back Propagation: an efficient way to calculate Gradient Descent:<br>forward pass, backward pass</p>
<p>æ²¡å¤ªæ‡‚</p>
<h3 id="Predicting-Pokemon-CP"><a href="#Predicting-Pokemon-CP" class="headerlink" title="Predicting PokÃ©mon CP"></a>Predicting PokÃ©mon CP</h3><p>Regression: difference in origin $x_{cp}$, and species<br>Gradient descent<br>Overfitting Regularization</p>
<h3 id="Pokemon-classification"><a href="#Pokemon-classification" class="headerlink" title="Pokemon classification"></a>Pokemon classification</h3><h4 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h4><p>2-D Gaussian distribution:<br>$f_{\mu^1,\Sigma^1}(x) &#x3D; \frac{1}{(2\pi)^{D&#x2F;2}|\Sigma^1|^{1&#x2F;2}} \exp\left(-\frac{1}{2}(x - \mu^1)^T(\Sigma^1)^{-1}(x - \mu^1)\right)$</p>
<p>$\mu$ mean $\sum$ covariance<br>$\mu^1 &#x3D; \begin{bmatrix}<br>75.0 \<br>71.3 \<br>\end{bmatrix}<br>\quad<br>\Sigma^1 &#x3D; \begin{bmatrix}<br>874 &amp; 327 \<br>327 &amp; 929 \<br>\end{bmatrix}$</p>
<p>$P(C_1|x) &#x3D; \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$</p>
<p>Simplify the function, substitute Gaussian into probability<br>$P(C_1|x)&#x3D;\sigma(z) &#x3D; \sigma(wx+b)$<br>$w&#x3D;(\mu^1-\mu^2)^T\sum^{-1}, b&#x3D;â€¦(scalar)$<br>So the Boundary for shared $\sum$ is linear.</p>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>ï¼ˆæ•°å­¦æ¨å¯¼æ¯”è¾ƒå¤šï¼‰</p>
<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>Cross Entropy for a Bernoulli distribution<br>$H(p,q)&#x3D;âˆ’[p\log(q)+(1âˆ’p)\log(1âˆ’q)]$<br>which is better than Square Error. </p>
<p>Discriminative: Logistic Regression: Directly find $w$ and $b$, which generally have better performance<br>Generative: Gaussian description: Have assumptions (Naive Bayes, or â€¦) of model and find $\mu^1$, $\mu^2$, $\sum$ </p>
<h4 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h4><p>è·³è¿‡äº†ï¼Œæƒ³åšhwå†å¬ï¼Œè§‰å¾—ç°åœ¨å¯¹$f_{w, b}(x)$çš„ç†è§£è¿˜ä¸æ·±</p>
<h2 id="2-25"><a href="#2-25" class="headerlink" title="2&#x2F;25"></a>2&#x2F;25</h2><h3 id="Video-1"><a href="#Video-1" class="headerlink" title="Video 1"></a>Video 1</h3><p>Loss on training data large: Model Bias (need a more complex model) or Optimization<br>Loss on testing data large: Overfitting or mismatch -&gt; more data </p>
<h3 id="Video-2-1"><a href="#Video-2-1" class="headerlink" title="Video 2"></a>Video 2</h3><p>How to Optimize:<br>$<br>L(\theta) \approx L(\thetaâ€™) + (\theta - \thetaâ€™)^T \vec{g} + \frac{1}{2} (\theta - \thetaâ€™)^T H (\theta - \thetaâ€™)<br>$<br>Gradient $\vec{g}$:<br>$\vec{g} &#x3D; \nabla L(\thetaâ€™)$<br>$g_i &#x3D; \frac{\partial L(\thetaâ€™)}{\partial \theta_i}$<br>$\vec{g} &#x3D;<br>\begin{bmatrix}<br>\frac{\partial L}{\partial \theta_1} \<br>\frac{\partial L}{\partial \theta_2} \<br>\vdots \<br>\frac{\partial L}{\partial \theta_n}<br>\end{bmatrix}$<br>Hessian $H$ is a matrix $H_{ij} &#x3D; \frac{\partial^2 L(\thetaâ€™)}{\partial \theta_i \partial \theta_j}$<br>For all $v$:</p>
<ol>
<li>$v^T Hv &gt; 0$: $H$ is positive definite, $L(\theta) &gt; L(\thetaâ€™)$: Local minima</li>
<li>$v^T Hv &lt; 0$: $H$ is negative definite, $L(\theta) &lt; L(\thetaâ€™)$: Local maxima</li>
<li>Some eigenvalues are $+$, some are $-$: Saddle point<br>Empirical learning:</li>
</ol>
<h3 id="Video-3"><a href="#Video-3" class="headerlink" title="Video 3"></a>Video 3</h3><p>Batch: large batch $N$ not necessarily need longer time for gradient computing (parallel computing)</p>
<h3 id="Video-4"><a href="#Video-4" class="headerlink" title="Video 4"></a>Video 4</h3><p>Adaptive $\eta$ (learning rate):<br>Error surface<br>Critical points (local minima, saddle point):</p>
<ol>
<li>Adagrad </li>
<li>RMSProp</li>
<li>Adam: RMSProp + Momentum</li>
</ol>
<p>Learning Rate Scheduling:<br>Learning rate Decay<br>Warm up (Residual Network, Transformer Classification)</p>
<h3 id="Video-5"><a href="#Video-5" class="headerlink" title="Video 5"></a>Video 5</h3><p>Regression:<br>Right answer: $\hat{y} \leftrightarrow y$<br>Classification: class: one-hot vector: $\hat{y} \leftrightarrow yâ€™ &#x3D; \text{softmax}(y)$<br>Soft-max (Normalize): $n \geq 3$: $y_iâ€™ &#x3D; \frac{\exp(y_i)}{\sum_j \exp(y_j)}$<br>$n&#x3D;2$ $yâ€™ &#x3D; \text{sigmoid}(y)$</p>
<p>Distance $e$:<br>Mean Square Error (MSE): $e &#x3D; \sum (\hat{y_i} - y_iâ€™)^2$<br>Cross-entropy: $e &#x3D; -\sum \hat{y_i} \ln y_iâ€™$<br>Minimize Cross-entropy $\leftrightarrow$ Maximize likelihood</p>
<h3 id="Basic-Theory"><a href="#Basic-Theory" class="headerlink" title="Basic Theory"></a>Basic Theory</h3><p>We want $L(h_{\text{train}}, D_{\text{all}}) - L(h_{\text{all}}, D_{\text{all}}) \leq \delta$<br>$\forall h \in \mathcal{H}, |L(h, D_{\text{train}}) - L(h, D_{\text{all}})| \leq \frac{\delta}{2}$</p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><h3 id="Beyond-Adam-1"><a href="#Beyond-Adam-1" class="headerlink" title="Beyond Adam 1"></a>Beyond Adam 1</h3><h3 id="Beyond-Adam-2"><a href="#Beyond-Adam-2" class="headerlink" title="Beyond Adam 2"></a>Beyond Adam 2</h3><h2 id="3-04-CNN"><a href="#3-04-CNN" class="headerlink" title="3&#x2F;04 CNN"></a>3&#x2F;04 CNN</h2><h3 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h3><p>Image as input</p>
<h4 id="V1"><a href="#V1" class="headerlink" title="V1"></a>V1</h4><p>Tensor: a Matrix &gt;&#x3D; 3 dimensional  </p>
<ol>
<li>Observation 1:<br>Receptive Field: Kernel Size (3x3), Stride (1 or 2, padding 0, hope receptive field are intersecting)  </li>
<li>Observation 2<br>Shared parameters: filter<br>1 + 2 -&gt; Convolution Layer -&gt; CNN (designed for image)</li>
</ol>
<h4 id="V2"><a href="#V2" class="headerlink" title="V2"></a>V2</h4><p>Each filter detects a small pattern (3 * 3 * channel_num, which is a tensor)<br>Feature Map<br>3. Observation 3<br>Max Pooling: Operator<br>Convolutional Layer + Pooling  </p>
<h3 id="Spatial-Transformer-Layer"><a href="#Spatial-Transformer-Layer" class="headerlink" title="Spatial Transformer Layer"></a>Spatial Transformer Layer</h3><p>CNN is not invariant to scaling and rotation<br>Interpolation.</p>
<h2 id="3-11-Self-attention"><a href="#3-11-Self-attention" class="headerlink" title="3&#x2F;11 Self-attention"></a>3&#x2F;11 Self-attention</h2><h3 id="Video-1-1"><a href="#Video-1-1" class="headerlink" title="Video 1"></a>Video 1</h3><p>Sequence Labeling<br>Self-Attention: dot product additive </p>
<h3 id="Video-2-2"><a href="#Video-2-2" class="headerlink" title="Video 2"></a>Video 2</h3><p>Self-attention<br>Multihead self-attention<br>Truncated self-attention<br>CNN is a simplified self-attention (limited to receptive field)<br>RNN, GNN (Graph Neural Network)</p>
<h3 id="GNN-1"><a href="#GNN-1" class="headerlink" title="GNN 1"></a>GNN 1</h3><p>Convolution (spatial-based&#x2F;Spectral-based)</p>
<h4 id="Spatial-based"><a href="#Spatial-based" class="headerlink" title="Spatial-based"></a>Spatial-based</h4><p>Terminology:<br>Aggregate: use neighbor features to update the next hidden state<br>Readout: use all nodesâ€™ features to represent the whole graph<br>NN4G<br>DCNN<br>GAT (Graph Attention Network)<br>Graph Isomorphism Network</p>
<h3 id="GNN-2"><a href="#GNN-2" class="headerlink" title="GNN 2"></a>GNN 2</h3><p>Deep Graph Library</p>
<h4 id="Graph-Signal-Processing"><a href="#Graph-Signal-Processing" class="headerlink" title="Graph Signal Processing"></a>Graph Signal Processing</h4><p>Graph Laplacian:<br>Degree Matrix $D$, Adjacency Matrix: $A$, $L$ is an operation on graph<br>$L &#x3D; D - A &#x3D; U \Lambda U^T$<br>Discrete time Fourier basis $\lambda$ wave length<br>$(Lf)(v_i) &#x3D; \sum_{v_j \in V} w_{i,j}(f(v_i) - f(v_j))$<br>$\begin{aligned}<br>f^T L f &amp;&#x3D; \sum_{v_i \in V} f(v_i) \sum_{v_j \in V} w_{i,j}(f(v_i) - f(v_j))\<br>&amp;&#x3D; \frac{1}{2} \sum_{v_i \in V} \sum_{v_j \in V} w_{i,j}(f(v_i) - f(v_j))^2<br>\end{aligned}<br>$</p>
<p>Graph Fourier Transform of signal $\hat{x}$: $\hat{x} &#x3D; U ^T x, \hat{x}_i &#x3D; u_i \cdot x$<br>Inverse Graph Fourier Transform of signal $\hat{x}$: $x &#x3D; U ^T \hat{x}$</p>
<p>Filtering: Convolution in time domain is multiplication in frequency domain</p>
<p>ChebNet<br>å¬ä¸æ‡‚åœ¨å¹²ä»€ä¹ˆ</p>
<h4 id="Spectral-based"><a href="#Spectral-based" class="headerlink" title="Spectral-based"></a>Spectral-based</h4><h2 id="3-18"><a href="#3-18" class="headerlink" title="3&#x2F;18"></a>3&#x2F;18</h2><h3 id="Video-1-Batch-Normalization"><a href="#Video-1-Batch-Normalization" class="headerlink" title="Video 1 Batch Normalization"></a>Video 1 Batch Normalization</h3><p>Batch Normalization<br>Internal Covariate Shift</p>
<h3 id="Video-2-Seq2seq"><a href="#Video-2-Seq2seq" class="headerlink" title="Video 2 Seq2seq"></a>Video 2 Seq2seq</h3><p>Transformer<br>Seq2seq:<br>Chatbox,<br>NLP</p>
<h2 id="Video-3-Decoder"><a href="#Video-3-Decoder" class="headerlink" title="Video 3 Decoder"></a>Video 3 Decoder</h2><p>Autoregressive<br>Masked Self-attention</p>
<h2 id="NAT-Non-autoregressive-translation"><a href="#NAT-Non-autoregressive-translation" class="headerlink" title="NAT Non autoregressive translation"></a>NAT Non autoregressive translation</h2><p>åƒä¸€ä¸ªNATå‘å±•çš„è®ºæ–‡ç»¼è¿°<br>Naive approach,<br>autoregressive,<br>GAN, </p>
<p>Improvement</p>
<ol>
<li>Fertility</li>
<li>Sequence-level knowledge distillation</li>
<li>Noisy Parallel Decoding NPD</li>
</ol>
<p>Vanilla NAT, Iterative Refinement, Insertion-based, Insertion+Deletion, CTC-based, Masked-predict, Kermit, CTC, LAS, Imputer (CTC+Mask-Predict)</p>
<h2 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h2><h2 id="3-25"><a href="#3-25" class="headerlink" title="3&#x2F;25"></a>3&#x2F;25</h2><h3 id="Video-1-GAN"><a href="#Video-1-GAN" class="headerlink" title="Video 1 GAN"></a>Video 1 GAN</h3><p>Discriminator</p>
<h3 id="Video-2-GAN"><a href="#Video-2-GAN" class="headerlink" title="Video 2 GAN"></a>Video 2 GAN</h3><p>JS Divergence<br>$G^*&#x3D; \arg \min(G) \max(D) \mathcal{V}(G, D)$<br><img src="/LHY-ML/image-1.png" alt="alt text"><br>$JS(P \parallel Q) &#x3D; \frac{1}{2} KL(P \parallel M) + \frac{1}{2} KL(Q \parallel M)$<br>$KL(P \parallel Q) &#x3D; \sum_{x} P(x) \log\left(\frac{P(x)}{Q(x)}\right)$</p>
<p>WGAN\<br>Wasserstein distance: improve JS divergence: $JS(P_G, P_{\text{data}}) \rightarrow W(P_G, P_{\text{data}})$<br>$\max_{D \in 1-\text{Lipschitz}} \left{ \mathbb{E}<em>{x \sim P</em>{\text{data}}} [D(x)] - \mathbb{E}<em>{x \sim P</em>{G}} [D(x)] \right}$<br>the $D(x)$ should be smooth enough</p>
<h3 id="Video-3-BERT-anecdote"><a href="#Video-3-BERT-anecdote" class="headerlink" title="Video 3 BERT anecdote"></a>Video 3 BERT anecdote</h3><p>CBOW (2 transforms): word embedding<br>contextualized word embedding<br>Multi BERT: Zero-shot Reading Comprehension, alignment</p>
<h3 id="Video-4-Cycle-GAN"><a href="#Video-4-Cycle-GAN" class="headerlink" title="Video 4 Cycle GAN"></a>Video 4 Cycle GAN</h3><p>Cycle&#x2F;Dual&#x2F;Disco GAN: $G_{x\rightarrow y}$, $G_{y\rightarrow x}$</p>
<h3 id="The-theory-of-GAN-1"><a href="#The-theory-of-GAN-1" class="headerlink" title="The theory of GAN (1)"></a>The theory of GAN (1)</h3><p>$\max_{D} \mathcal{V}(G, D)$ maximize the discriminator D in GAN  </p>
<p>$\mathcal{V}(G, D) &#x3D; \mathbb{E}<em>{x \sim P</em>{\text{data}}} [\log D(x)] + \mathbb{E}<em>{x \sim P</em>{G}} [\log(1 - D(x))]$<br>$\mathcal{V}(G, D) &#x3D; \int_{x} P_{\text{data}}(x)\log D(x) , dx + \int_{x} P_{G}(x)\log(1 - D(x)) , dx$</p>
<p>$\mathcal{V}(G, D) &#x3D; P_{\text{data}}(x)\log D(x) + P_{G}(x)\log(1 - D(x))$ </p>
<p>$D^*(x) &#x3D; \frac{P_{\text{data}}(x)}{P_{\text{data}}(x) + P_{G}(x)}$</p>
<h2 id="4-01"><a href="#4-01" class="headerlink" title="4&#x2F;01"></a>4&#x2F;01</h2><h3 id="Video-1-2"><a href="#Video-1-2" class="headerlink" title="Video 1"></a>Video 1</h3><p>Self-supervised Learning</p>
<h3 id="Video-2-BERT-intro"><a href="#Video-2-BERT-intro" class="headerlink" title="Video 2 BERT intro"></a>Video 2 BERT intro</h3><p>Masking Input: Mask<br>Next Sentence Prediction: [CLS] sentence 1. [SEP] sentence 2.<br>Pre-trained Fine-tune for Downstream Tasks:<br>GLUE: General Language Understanding Evaluation<br>in seq, out class: sentiment analysis<br>in seq ((n)), out seq (n): POG tagging<br>in 2 seqs, out class: NLI Natural language inference<br>in seqs, out seqs: QA Extract-based Question Answer</p>
<p>MASS\ BART T5, C4 (open sourced resource)</p>
<h3 id="Video-3-BERT-anecdote-1"><a href="#Video-3-BERT-anecdote-1" class="headerlink" title="Video 3 BERT anecdote"></a>Video 3 BERT anecdote</h3><p>Same with above</p>
<h3 id="Video-4-GPT-outlook"><a href="#Video-4-GPT-outlook" class="headerlink" title="Video 4 GPT outlook"></a>Video 4 GPT outlook</h3><p>Linear Transform -&gt; Softmax -&gt; distribution<br>Few-shot learning, one-shot, zero-shot learning<br>SimCLR, BYOL,<br>Speech GLUE - SUPERB</p>
<h2 id="4-15"><a href="#4-15" class="headerlink" title="4&#x2F;15"></a>4&#x2F;15</h2><p>????</p>
<h2 id="4-22"><a href="#4-22" class="headerlink" title="4&#x2F;22"></a>4&#x2F;22</h2><p>Auto encoder</p>
<h3 id="Video-1-basic-idea"><a href="#Video-1-basic-idea" class="headerlink" title="Video 1 basic idea"></a>Video 1 basic idea</h3><p>same idea with Cycle GAN, embedding, representation, code<br>Dimension reduction: not deep learning based PCA, t-SNE<br>De-noising Auto-encoder</p>
<p>Video 2-8 are all anomaly detection</p>
<h3 id="Video-2-3"><a href="#Video-2-3" class="headerlink" title="Video 2"></a>Video 2</h3><p>Feature disentanglement: know the content of embedding: Voice Conversion<br>Discrete Representation: VQVAE</p>
<h3 id="Video-3-1"><a href="#Video-3-1" class="headerlink" title="Video 3"></a>Video 3</h3><p>Anomaly detection: other methods outlier, novelty, exception<br>one class classifier: Approach: Auto-encoder</p>
<h3 id="Video-4-1"><a href="#Video-4-1" class="headerlink" title="Video 4"></a>Video 4</h3><p>A confidence score $c$, a threshold $\lambda$, smaller than, anomaly.</p>
<h3 id="Video-5-1"><a href="#Video-5-1" class="headerlink" title="Video 5"></a>Video 5</h3><p>Generating anomaly data</p>
<h3 id="Video-6"><a href="#Video-6" class="headerlink" title="Video 6"></a>Video 6</h3><p>Without Labels<br><a target="_blank" rel="noopener" href="https://github.com/ahaque/twitch-troll-detection">https://github.com/ahaque/twitch-troll-detection</a> </p>
<h3 id="Video-7"><a href="#Video-7" class="headerlink" title="Video 7"></a>Video 7</h3><p>Gaussian Distribution<br>Assume the data points are samples from a probability density function $f_{\theta}(x)$<br>$\theta$ determine the shape of $f_{\theta}(x)$<br>$L(\theta)&#x3D;f_{\theta}(x^1)f_{\theta}(x^2)â€¦f_{\theta}(x^N)$<br>$\theta^* &#x3D; \arg \max_\theta L(\theta), \theta&#x3D;(\mu, \Sigma)$<br>$f_{\mu,\Sigma}(x) &#x3D; \frac{1}{(2\pi)^{D&#x2F;2}}\frac{1}{|\Sigma|^{1&#x2F;2}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$</p>
<h3 id="Video-8"><a href="#Video-8" class="headerlink" title="Video 8"></a>Video 8</h3><p>Auto-encoder</p>
<h2 id="4-29"><a href="#4-29" class="headerlink" title="4&#x2F;29"></a>4&#x2F;29</h2><h3 id="Video-1-Explainable-ML-Local"><a href="#Video-1-Explainable-ML-Local" class="headerlink" title="Video 1 Explainable ML Local"></a>Video 1 Explainable ML Local</h3><p>Loss of an example: Gradient: Saliency Map<br>Limitation: Noisy Gradient, SmoothGrad<br>MFCC<br>Attention is Explainable<br>Probing: CNN BLSTM</p>
<h3 id="Video-2-Explainable-ML-Global"><a href="#Video-2-Explainable-ML-Global" class="headerlink" title="Video 2 Explainable ML Global"></a>Video 2 Explainable ML Global</h3><h2 id="5-06"><a href="#5-06" class="headerlink" title="5&#x2F;06"></a>5&#x2F;06</h2><h3 id="Video-1-3"><a href="#Video-1-3" class="headerlink" title="Video 1"></a>Video 1</h3><p>Attack<br>$x^* &#x3D; \arg_{d(x^0, x)&lt;\epsilon} \min L(x)$</p>
<h4 id="Non-targeted"><a href="#Non-targeted" class="headerlink" title="Non-targeted"></a>Non-targeted</h4><p>$e(,)$ cross entropy<br>$L(x) &#x3D; -e(y, \hat y)$</p>
<h4 id="Targeted"><a href="#Targeted" class="headerlink" title="Targeted"></a>Targeted</h4><p>$L(x) &#x3D; -e(y, \hat y) + e(y, y_{\text{target}})$<br>$\hat y$: real case<br>$y_{\text{target}}$: what you wish to be perceived</p>
<h4 id="Non-perceivable"><a href="#Non-perceivable" class="headerlink" title="Non-perceivable"></a>Non-perceivable</h4><p>$d(x^0, x) &lt; \epsilon$,<br>L2-norm, L-infinity</p>
<h3 id="Video-2-4"><a href="#Video-2-4" class="headerlink" title="Video 2"></a>Video 2</h3><p>Black box attack: Proxy Network<br>Ensemble Network, one-pixel, universal adversarial attack<br>Beyond Images, speech processing, Natural<br>Adversarial reprogramming<br>Filter</p>
<h2 id="7-03"><a href="#7-03" class="headerlink" title="7&#x2F;03"></a>7&#x2F;03</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1734y1c7Hb?p=2&spm_id_from=pageDriver&vd_source=441679270dda23308fe16f3c5602b058">https://www.bilibili.com/video/BV1734y1c7Hb?p=2&amp;spm_id_from=pageDriver&amp;vd_source=441679270dda23308fe16f3c5602b058</a></p>
<h3 id="Video-1-Diffusion-Model"><a href="#Video-1-Diffusion-Model" class="headerlink" title="Video 1 Diffusion Model"></a>Video 1 Diffusion Model</h3><p>Denoise module: picture + noise -&gt; predict noise, then -noise -&gt; picture<br>Train the noise predictor</p>
<p>Diffuse process: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for step in range(1000): </span><br><span class="line">    generate noise[i] pic[i] = pic[i-1] + noise[i]</span><br><span class="line">train: </span><br><span class="line">    given pic[i-1], pic[i]</span><br><span class="line">    learn noise[i]</span><br></pre></td></tr></table></figure>
<p><img src="/2024-03-08-LHY-ML/image-2.png" alt="alt text"></p>
<h3 id="Video-2-5"><a href="#Video-2-5" class="headerlink" title="Video 2"></a>Video 2</h3><p>Text to picture<br>DALL-E Decoder: Autoregressive&#x2F;Diffusion Model<br>Imagen (Google):</p>
<p>Framework:<br>Encoder: GPT&#x2F;BERT<br>Encoder -&gt; Generation model (Latent Representation) -&gt; Decoder</p>
<h3 id="Video-3-åŸç†"><a href="#Video-3-åŸç†" class="headerlink" title="Video 3 åŸç†"></a>Video 3 åŸç†</h3><p>è§£é‡Š Trainingï¼Œ Sampling</p>
<p>$x_0:$ a picture,</p>
<p>$x_t:$ pic + noise $t$</p>
<h3 id="Video-4-2"><a href="#Video-4-2" class="headerlink" title="Video 4"></a>Video 4</h3><p>Maximum likelihood Estimation:<br>$P_{\text{data}}(x)$ True Distribution of Data<br>$P_{\theta}(x)$ Probability distribution of data $x$ given parameters $\theta$<br>${x^1, x^2, \cdots, x^m}$ Observed data samples</p>
<p>Network: $z \rightarrow \theta \rightarrow P_{\theta}(x) \rightarrow P_{\text{data}}(x)$<br>maximize $P_{\theta}(x^1)P_{\theta}(x^2) \cdots P_{\theta}(x^m)$</p>
<p>$\theta^* &#x3D; \arg \max_\theta \log P_{\theta}(x^1)P_{\theta}(x^2) \cdots P_{\theta}(x^m)$</p>
<p>$\theta^* &#x3D; \arg \max_\theta \mathbb{E}<em>{x \sim P</em>{\text{data}}} \log P_{\theta}(x)$</p>
<p>$\theta^* &#x3D; \arg \max_\theta \int_{x} P_{\text{data}}(x) \log P_{\theta}(x) dx$</p>
<p>$\theta^* &#x3D; \arg \max_\theta \left( \int_{x} P_{\text{data}}(x) \log P_{\theta}(x) dx - \int_{x} P_{\text{data}}(x) \log P_{\text{data}}(x) dx \right)$</p>
<p>$\theta^* &#x3D; \arg \max_\theta \int_{x} P_{\text{data}}(x) \log \frac{P_{\theta}(x)}{P_{\text{data}}(x)} dx$</p>
<p>$\theta^* &#x3D; \arg \max_\theta -KL(P_{\text{data}} || P_{\theta})$</p>
<h4 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h4><p>Compute $P_{\theta}(x)$<br>Network: $G(z) &#x3D; x$<br>$P_{\theta}(x) &#x3D; \int P_{\theta}(x|z)P_{\theta}(z) dz$</p>
<p>$P_{\theta}(z|x) &#x3D; \frac{P_{\theta}(x|z)P_{\theta}(z)}{P_{\theta}(x)}$</p>
<p>DDPM</p>
<h3 id="Video-5-2"><a href="#Video-5-2" class="headerlink" title="Video 5"></a>Video 5</h3><p>VAE: Variational Auto-encoder<br>$P_{\theta}(x) &#x3D; \int P_{\theta}(x|z)P_{\theta}(z) dz$<br>$P_{\theta}(z|x) &#x3D; \frac{P_{\theta}(x|z)P_{\theta}(z)}{P_{\theta}(x)}$</p>
<h1 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h1><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1yD4y1i7Jm/?p=44&vd_source=441679270dda23308fe16f3c5602b058">https://www.bilibili.com/video/BV1yD4y1i7Jm/?p=44&amp;vd_source=441679270dda23308fe16f3c5602b058</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wxkang/p/17128108.html">https://www.cnblogs.com/wxkang/p/17128108.html</a><br>æ¯”è¾ƒä¸»æµçš„ç”Ÿæˆæ¨¡å‹ï¼šHMM, NB, GMM (Gaussian Mixture Model)</p>
<p>KL divergence: $KL(P||Q) &#x3D; \int P(x) \log \frac{P(x)}{Q(x)} dx$, $KL(P||Q) \neq KL(Q||P)$</p>
<p>AE: ä¸ PCA, SVD ç›®çš„ç›¸åŒï¼ŒçŸ©é˜µé™ç»´æŠ€æœ¯ã€‚</p>
<p>latent variable $z$, assume it follows the prior distribution of $P(z) \sim N(0,1)$</p>
<p>$P(x|z) \sim N(\mu(z), \sigma(z))$ </p>
<p>$P(x) &#x3D; \int P(z) P(x|z) dz$</p>
<p>To Maximize Likelihood of observed $x$: $L &#x3D; \sum_x \log P(x)$</p>
<p>ELBO Evidence Lower Bound</p>
<h1 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h1><p>çœ‹ä¸æ‡‚ï¼šä»¿ä½›åœ¨è°ˆæ”¶æ•›å¿«æ…¢å’Œå¹³è¡¡çŠ¶æ€ $\pi$ çš„é—®é¢˜<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143016455">https://zhuanlan.zhihu.com/p/143016455</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6625739.html">https://www.cnblogs.com/pinard/p/6625739.html</a></p>
<h3 id="Monte-Carlo-Integration"><a href="#Monte-Carlo-Integration" class="headerlink" title="Monte Carlo Integration"></a>Monte Carlo Integration</h3><p>If $X$ is uniformly distributed on $[a,b]$:<br>$\int_a^b f(x)dx &#x3D; \int_a^b f(x) \frac{1}{b-a}dx &#x3D; \mathbb{E}<em>{x \sim U(a,b)}[f(x)] &#x3D; \frac{1}{N}\sum</em>{i&#x3D;1}^N f(x_i)$</p>
<p>If we know the distribution of $X$ on $[a, b] &#x3D; p(x)$:<br>$\int_a^b f(x)dx &#x3D; \int_a^b \frac{f(x)}{p(x)} p(x) dx &#x3D; \mathbb{E}<em>{x \sim p(x)}[f(x)] &#x3D; \frac{1}{N}\sum</em>{i&#x3D;1}^N \frac{f(x_i)}{p(x_i)}$</p>
<h3 id="Acceptance-Rejection-Sampling"><a href="#Acceptance-Rejection-Sampling" class="headerlink" title="Acceptance-Rejection Sampling"></a>Acceptance-Rejection Sampling</h3><p><a target="_blank" rel="noopener" href="https://blog.quantitations.com/inference/2012/11/24/rejection-sampling-proof">https://blog.quantitations.com/inference/2012/11/24/rejection-sampling-proof</a></p>
<p>æ–¹ä¾¿é‡‡æ ·çš„å¸¸ç”¨æ¦‚ç‡åˆ†å¸ƒå‡½æ•° (proposal distribution) $q(x)$ ä»¥åŠä¸€ä¸ªå¸¸é‡ $k$ ä½¿å¾— $p(x)$ æ€»åœ¨ $k q(x)$ çš„ä¸‹æ–¹</p>
<h3 id="MCMC-1"><a href="#MCMC-1" class="headerlink" title="MCMC"></a>MCMC</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/08/2024-03-08-LHY-ML/" data-id="cm9bnagsz001vzc3dbods73t8" data-title="LHY ML" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-03-06-A-Trial-For-HW3-CS-311" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/06/2024-03-06-A-Trial-For-HW3-CS-311/" class="article-date">
  <time class="dt-published" datetime="2024-03-07T00:50:14.000Z" itemprop="datePublished">2024-03-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/06/2024-03-06-A-Trial-For-HW3-CS-311/">A Trial For HW3 CS 311</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="HW-311"><a href="#HW-311" class="headerlink" title="HW 311"></a>HW 311</h1><h2 id="Flask-and-REST-API"><a href="#Flask-and-REST-API" class="headerlink" title="Flask and REST API"></a>Flask and REST API</h2><p>Some helpful Web that helps understand flask </p>
<ol>
<li><p>Flask-RESTPlus is an extension for flask. It comes with built-in support for Swagger, which allows automatic generation of interactive API documentation that can be used by developers to test the API.<br><a target="_blank" rel="noopener" href="https://github.com/flask-restful/flask-restful">https://github.com/flask-restful/flask-restful</a><br><a target="_blank" rel="noopener" href="https://flask-restplus.readthedocs.io/en/stable/quickstart.html#">https://flask-restplus.readthedocs.io/en/stable/quickstart.html#</a></p>
</li>
<li><p>Swager UI:<br><a target="_blank" rel="noopener" href="https://github.com/swagger-api/swagger-ui">https://github.com/swagger-api/swagger-ui</a></p>
</li>
<li><p>Flask swagger:<br><a target="_blank" rel="noopener" href="https://github.com/getsling/flask-swagger">https://github.com/getsling/flask-swagger</a></p>
</li>
</ol>
<h3 id="key-word-Flask-Web"><a href="#key-word-Flask-Web" class="headerlink" title="key word: Flask + Web"></a>key word: Flask + Web</h3><p>An example for flask web: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xianyi-yk/p/14695401.html">https://www.cnblogs.com/xianyi-yk/p/14695401.html</a></p>
<h3 id="Virtual-environment"><a href="#Virtual-environment" class="headerlink" title="Virtual environment"></a>Virtual environment</h3><pre><code>```Ctrl P &gt;``` to call the pannel, 
```Python: select intepreter```
then you will have a local environment in the current work space!!!!
</code></pre>
<h2 id="Swagger"><a href="#Swagger" class="headerlink" title="Swagger"></a>Swagger</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/06/2024-03-06-A-Trial-For-HW3-CS-311/" data-id="cm9bnagsu001kzc3db1be8dua" data-title="A Trial For HW3 CS 311" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-02-07-Transport-layer" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/02/07/2024-02-07-Transport-layer/" class="article-date">
  <time class="dt-published" datetime="2024-02-07T15:55:06.000Z" itemprop="datePublished">2024-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/02/07/2024-02-07-Transport-layer/">Transport layer</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="3c-internectworking"><a href="#3c-internectworking" class="headerlink" title="3c internectworking"></a>3c internectworking</h1><h2 id="Service-model"><a href="#Service-model" class="headerlink" title="Service model"></a>Service model</h2><p>no guarantees</p>
<h2 id="IP-packet-header"><a href="#IP-packet-header" class="headerlink" title="IP packet header"></a>IP packet header</h2><h2 id="Fragmentation"><a href="#Fragmentation" class="headerlink" title="Fragmentation"></a>Fragmentation</h2><p>Examples with detail</p>
<h2 id="Addressing"><a href="#Addressing" class="headerlink" title="Addressing"></a>Addressing</h2><h3 id="IP-Address-properties"><a href="#IP-Address-properties" class="headerlink" title="IP Address properties"></a>IP Address properties</h3><h3 id="4-class"><a href="#4-class" class="headerlink" title="4 class"></a>4 class</h3><h3 id="Subnets-CIDR"><a href="#Subnets-CIDR" class="headerlink" title="Subnets&#x2F;CIDR"></a>Subnets&#x2F;CIDR</h3><h2 id="packet-forwarding"><a href="#packet-forwarding" class="headerlink" title="packet forwarding"></a>packet forwarding</h2><h3 id="IP-forward-algorithm"><a href="#IP-forward-algorithm" class="headerlink" title="IP forward algorithm"></a>IP forward algorithm</h3><h3 id="Routin-tables"><a href="#Routin-tables" class="headerlink" title="Routin tables"></a>Routin tables</h3><h2 id="Address-translation"><a href="#Address-translation" class="headerlink" title="Address translation"></a>Address translation</h2><h2 id="ARP-Adression-Resolution-Protocol"><a href="#ARP-Adression-Resolution-Protocol" class="headerlink" title="ARP Adression Resolution Protocol"></a>ARP Adression Resolution Protocol</h2><h2 id="Node-autoconfiguration"><a href="#Node-autoconfiguration" class="headerlink" title="Node autoconfiguration"></a>Node autoconfiguration</h2><h3 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h3><p>dynamic host configuration ptcl</p>
<h2 id="Error-Reporting"><a href="#Error-Reporting" class="headerlink" title="Error Reporting"></a>Error Reporting</h2><h3 id="ICMP-Internet-Control-messsage-protocol"><a href="#ICMP-Internet-Control-messsage-protocol" class="headerlink" title="ICMP Internet Control messsage protocol"></a>ICMP Internet Control messsage protocol</h3><h3 id="MTU-Discovery"><a href="#MTU-Discovery" class="headerlink" title="MTU Discovery"></a>MTU Discovery</h3><h1 id="3d-Routing"><a href="#3d-Routing" class="headerlink" title="3d Routing"></a>3d Routing</h1><h2 id="Forwarding-One-path-vs-Routing-Mutiple-path"><a href="#Forwarding-One-path-vs-Routing-Mutiple-path" class="headerlink" title="Forwarding(One path) vs Routing(Mutiple path)"></a>Forwarding(One path) vs Routing(Mutiple path)</h2><h2 id="Distance-Vector-protocol-Bellman-ford"><a href="#Distance-Vector-protocol-Bellman-ford" class="headerlink" title="Distance Vector protocol: Bellman_ford"></a>Distance Vector protocol: Bellman_ford</h2><h2 id="Link-state-protocol"><a href="#Link-state-protocol" class="headerlink" title="Link-state protocol"></a>Link-state protocol</h2><p>each router teel all the otehr routers about its link</p>
<h2 id="Reliable-flooding-LSP"><a href="#Reliable-flooding-LSP" class="headerlink" title="Reliable flooding (LSP)"></a>Reliable flooding (LSP)</h2><p>link metrics</p>
<h2 id="Interdomain-routing-definition"><a href="#Interdomain-routing-definition" class="headerlink" title="Interdomain routing - definition"></a>Interdomain routing - definition</h2><h3 id="BGP"><a href="#BGP" class="headerlink" title="BGP"></a>BGP</h3><h3 id="ASs"><a href="#ASs" class="headerlink" title="ASs"></a>ASs</h3><p>BGP between ASs: compute gglobal policy-compliant routers<br>OSPF in an AS: compute shortest routes between routers</p>
<h1 id="4a-Transport"><a href="#4a-Transport" class="headerlink" title="4a Transport"></a>4a Transport</h1><h2 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h2><p>Intra-domain protocols</p>
<ul>
<li>Lowest cost shortest protocol&#x2F; disseminate full information about the topology</li>
<li>Distance vector protocol(RIPv2)</li>
<li>link state pctl: OSPF</li>
</ul>
<p>Inter-domainprotocol (BGP)</p>
<ul>
<li>most preffeered policy-compliant route</li>
<li>information hiding for scalability and secrecy</li>
</ul>
<h2 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h2><p>well known ports<br>choosing ports</p>
<h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><h3 id="byte-stram-abstraction"><a href="#byte-stram-abstraction" class="headerlink" title="byte stram abstraction"></a>byte stram abstraction</h3><p>segment format</p>
<h3 id="three-handshake"><a href="#three-handshake" class="headerlink" title="three handshake"></a>three handshake</h3><h3 id="flow-controlslly-window-synfrome"><a href="#flow-controlslly-window-synfrome" class="headerlink" title="flow controlslly window synfrome"></a>flow controlslly window synfrome</h3><h3 id="Nagelsâ€™-algorithm"><a href="#Nagelsâ€™-algorithm" class="headerlink" title="Nagelsâ€™ algorithm"></a>Nagelsâ€™ algorithm</h3><h1 id="4b-RPC-RTP"><a href="#4b-RPC-RTP" class="headerlink" title="4b RPC- RTP"></a>4b RPC- RTP</h1><h2 id="RPC-Remote-procedure-call"><a href="#RPC-Remote-procedure-call" class="headerlink" title="RPC Remote procedure call"></a>RPC Remote procedure call</h2><p>gRPC: stubs, request and responses<br>Acknowledge models<br>Syn, Asyn ptcls<br>Message and encoding</p>
<h3 id="Data-transmission-in-real-time"><a href="#Data-transmission-in-real-time" class="headerlink" title="Data transmission in real time"></a>Data transmission in real time</h3><p>Read- time data transmission (RTP)<br>Stram transmission ptcl</p>
<h1 id="4c-src-and-congestion"><a href="#4c-src-and-congestion" class="headerlink" title="4c src and congestion"></a>4c src and congestion</h1><h2 id="Traffic-flows"><a href="#Traffic-flows" class="headerlink" title="Traffic flows"></a>Traffic flows</h2><p>Taxonomy of congestion flow<br>RED</p>
<h2 id="Congestion-control"><a href="#Congestion-control" class="headerlink" title="Congestion control"></a>Congestion control</h2><h2 id=""><a href="#" class="headerlink" title=""></a></h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/02/07/2024-02-07-Transport-layer/" data-id="cm9bnagst001izc3da1gq4ho9" data-title="Transport layer" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/music/" rel="tag">music</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/music/" style="font-size: 10px;">music</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/04/01/2025-04-01-Linux-server-proxy-issue/">2025-04-01-Linux-server-proxy-issue</a>
          </li>
        
          <li>
            <a href="/2025/04/01/2025-04-01-mt-implementation-log/">2025-04-01-mt-implementation-log</a>
          </li>
        
          <li>
            <a href="/2025/03/09/2025-03-08-medication/">2025-03-08 medication</a>
          </li>
        
          <li>
            <a href="/2025/02/15/2025-02-15-DS/">2025-02-15-DS</a>
          </li>
        
          <li>
            <a href="/2025/01/29/2025-01-29-CV/">2025-01-29-CV</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>