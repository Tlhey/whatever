---
title: papers
date: 2024-05-08 20:11:06
tags:
---

# 1. Counterfactual fairness
Counterfactual fairness
link: https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf

###
Definitions:
#### defs
$A$: Protected attributes, sensitive features\
$X$: features of individuals, excluding A\
$U$: latent features not observed, represented\
$Y$: predictor    
#### Fairness through unawareness (FTU):
_An algorithm is fair so long as any protected attributes $A$ are not explicitly used in the decision-making process._
Shortcoming: $X$ might intersects $A$

#### Individual Fairness (IF).
For distance metric(should be carefully choosen), $d(\cdot , \cdot)$, if $d(i, j)$ is small, then $\hat Y(X^{(i)}, A^{(i)}) \approx \hat Y(X^{(j)}, A^{(j)})$

#### Demographic Parity (DP)(äººå£ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„å¹³ç­‰)
Predictor $\hat Y$ satisfies demographic partiy if $P(\hat Y|A=0)=P(\hat Y|A=1)$ 
#### Equality of Opportunity
$P(\hat Y|A=0, Y=1)=P(\hat Y|A=1, Y=1)$ 

### Causal Models(å› æœæ¨æ–­), Counterfacutalã€
Casual Model $(U, V, F)$,\
$U$: latent background variables,\
$V$: observed variables, \
$F=\{f_1. f_2, \cdots, f_n\}$, for each $V_i=f_i(pa_i, U_{pa_i})\in V, pa_i \subseteq V \backslash {V_i}$ 

**Three Steps of Inference**\
- Abductionï¼šfor a given prior on $U$, compute the posterior distribution of $U$ given the evidence $W = w$
- Actionï¼šsubstitute the equations for $Z$ with the interventional values $z$, resulting in the modified set of equations $F_z$
- Prediction: 
## é¢˜å¤–è¯
### Casual Models (å› æœæ¨æ–­)
https://www.zhihu.com/column/c_1217887302124773376
#### Three levels:
1. Association: $A-B$ 
2. Interventionï¼š$A/A' \rightarrow B?$
3. Counterfactual $ want\ B', how A\rightarrow A'$
#### Beyasian Network
In Directed acyclic Graph (DAG):
![alt text](papers/image.png)

https://www.cnblogs.com/mantch/p/11179933.html
Component:
1. head-to-head $a\rightarrow c\leftarrow b$ \
$P(a,b,c) = P(a)P(b)P(c|a,b)$,\
unknown $c$, $a, b$ are blocked thus independent
2. tail-to-tail $a\leftarrow c\rightarrow b$ 
- $c$ unknown, $P(a,b,c)=P(c)P(a|c)P(b|c)$, $a, b$, not independent
- $c$ known, $P(a,b,c)=P(c)P(a|c)P(b|c)$, $P(a,b|c)=P(a,b,c)/P(c)=P(a|c)*P(b|c)$, $a, b $independent
3. head-to-tail (Markov Chain) $A\rightarrow C\rightarrow B$ 
- $c$ unknown, $a, b$, not independent
- $c$ known, $a, b$ independent

**Factor Graph**

#### 
Confounder

## æ€»ç»“
æˆ‘ä»ä¸€å¤©å‰å¼€å§‹çœ‹è®ºæ–‡ï¼Œè¢«casual modelçš„æ¦‚å¿µå¸å¼•äº†ã€‚æˆ‘è®¤ä¸ºæ˜¯å¾ˆå¥½çš„ä¸€ä¸ªç†è§£æ–¹å¼ã€‚ä»æ—©ä¸Šäº”ç‚¹å‡†å¤‡åˆ°åä¸€ç‚¹ã€‚\
ä»Šå¤©åšäº†preï¼Œæ•ˆæœå¾ˆå·®ã€‚
1. å¯¹æ¦‚ç‡çš„å„ç§å…¬å¼å¾ˆä¸å¤ªäº†è§£ã€‚å¯¹è´å¶æ–¯å’ŒMCMCä¸ä¼šã€‚
2. æ²¡æœ‰å»æƒ³è¿‡$U ,A, X$çš„å…³ç³»ã€‚æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„è§£é‡Šè®ºæ–‡ä¸­çš„é€»è¾‘å…³ç³»ã€‚

åœ¨å¼€ä¼šçš„æ—¶å€™æ•™æˆè¯´é‡è¦ä¸œè¥¿ï¼š
1. Counterfactual \
è¿™ç¯‡æœ€é‡è¦çš„æ˜¯ï¼š **Definition 5:** $P(\hat Y_{A\leftarrow a}(U)|X=x, A=a)=P(\hat Y_{A\leftarrow a'}(U)|X=x, A=a)$ \
å¾ˆå¤šâ€˜æ¦‚ç‡â€™åªæ˜¯è¡¨ç¤ºæ–¹æ³•ã€‚ï¼ˆä½†æ˜¯ç¡®å®ä¸å¾ˆç†è§£æ¦‚ç‡ï¼‰\
ç®—æ³•çš„æ€æƒ³åœ¨äºï¼š1. å¼•å…¥å› æœå›¾ã€‚2.å¯»æ‰¾Uï¼ˆ17å¹´MCMCï¼Œç°åœ¨å¯ä»¥GANï¼Œæˆ–å…¶ä»–ç”Ÿæˆå¼å­¦ä¹ æ–¹æ³•ï¼‰ã€‚
2. $U \rightarrow Xï¼ŒA$ 
åœ¨è®¡ç®—ä¸­ç”¨$X, A \rightarrow U$ æœ‰ä¸€äº›ç±»ä¼¼Adversarial learning. å¯ä»¥ç ”ç©¶æ€ä¹ˆå¥—ç”¨ã€‚
3. GAD
4. æœ‰ç‚¹æƒ³åštransfer learning çš„é‚£ç§


# FairGAD
https://openreview.net/forum?id=3cE6NKYy8x

https://arxiv.org/abs/2307.04937
## Fair GAD problem
**GAD**\
$G=(V, E, X)$, \
node feature matrix $X\in \R^{n\times d}$, \
Adjacency matrix $A\in \{0,1\}^{n\times n}$, \
Anomaly labels $Y\in \{0, 1\}^n$, predicted $\hat Y$, \
**Fair GAD**\
sensitive attributes $S\in \{0, 1\}^n$, a binary feature $X$.\
Performance matrix: accuracy and _AUCROC_: Area under the ROC Curve \
Unfairness Mextrics, Statistic Parity(SP):$SP = |P(\hat Y=1|S=0)âˆ’P(\hat Y =1|S=1)|$, \
Equality of Odds _(EOO)_: $SP = |P(\hat Y=1|S=0, Y=1)âˆ’P(\hat Y =1|S=1, Y=1)|$
## Data
- Reddit: 
graph structureï¼š linking two user posted the name subreddit within 24h.
Node feature: Embedding from post histories.
- Twitter: 
graph structure:: A follows B.
Node feature: demographic infromation using M3 system, multimodal, multilingual, multi attirbute demographix inderence framework.

<!-- ## GAD Methods
### DOMINANT (Ding et al., 2019a)
### CONAD (Xu et al., 2022)
### COLA (Liu et al., 2021)
### VGOD (Huang et al., 2023)

## Non-Graph AD methods
- DONE (Bandyopadhyay et al., 2020)
- AdONE (Bandyopadhyay et al., 2020)
- ECOD (Li et al., 2022)
- VAE (Kingma & Welling, 2014)
- ONE (Bandyopadhyay et al., 2019)
- LOF (Breunig et al., 2000)
- F (Liu et al., 2008)

## Fainess Method:
### FAIROD (Shekhar et al., 2021)
### CORRELATION (Shekhar et al., 2021)
### HIN (Zeng et al., 2021)
### EDITS (Dong et al., 2022)
### FAIRWALK (Rahman et al., 2019)

## Distance 
### Wasserstein Distance
### Minkowski distance -->



# 2024.05.23 Meeting summary 
1. è®¨è®ºäº†FairGADã€‚å¦‚æœä¸€ä¸ªæ–‡ç« çš„è´¡çŒ®æ˜¯æ•°æ®é›†ï¼Œé‚£ä¹ˆéœ€è¦è¯¦ç»†çš„Benchmarking: æœ‰ä¸€ç¯‡surveyçš„æ€§è´¨ï¼Œæ˜ç™½å„ç§æ–¹æ³•åœ¨æ•°æ®é›†ä¸Šè¡¨ç°æ€ä¹ˆæ ·ï¼Œæå‡ºä¸€ä¸ªè¯„åˆ¤æ ‡å‡†ï¼Œåªç”¨EOOä½œä¸ºfairçš„åˆ¤æ–­å¤ªç®€çŸ­äº†ã€‚
2. åŸºäºsentivityçš„Counterfactual fairnessçš„è¯„åˆ¤æ ‡å‡†ï¼Œæˆ‘ä»¬ç”¨ä»€ä¹ˆæ ·çš„è¯„åˆ¤æ ‡å‡†å’Œ
2.1 æœ€ç®€å•çš„æ„é€ æ–¹æ³• anomaly datasetï¼šclassification with y=1,2,3,4,5ã€‚æ‹¿å¾ˆå¤š1ï¼Œsampleè¾ƒå°‘2345.
2.2 æ‰¾ä¸€äº›graphä¸Šæ•°æ®é›†ï¼Œç”¨GADçš„æ–¹æ³•ï¼Œå˜æˆfairGADçš„æ•°æ®é›†ã€‚ä½†æ˜¯FairGADï¼Œæ˜¯GADæ•°æ®é›†inject fairnessï¼Œå¯èƒ½ä¸å¤ªå¥½ã€‚ 
2.3 




## Task of this week 
create synthetic data for fair GAD
1. Note this paper: https://arxiv.org/pdf/2304.01391 for a survey on graph counterfactual. To create a synthetic dataset, see their Section 3.5.1, where the data creation method is detailed in https://arxiv.org/abs/2201.03662.
2. See pygod https://github.com/pygod-team/pygod for outlier injection method to the graph dataset. Also, see Jing's paper https://proceedings.mlr.press/v231/gu24a/gu24a.pdf for improvement.
3. Next Friday, you can try to talk about how to generate the synthetic data and how this falls into counterfactual category.


æ‰€ä»¥å°±æ˜¯è¦å€Ÿé‰´åˆ›å»ºæ•°æ®é›†çš„æ–¹æ³•ã€‚è¿˜æœ‰å­¦ä¹ ä¸€äº›counterfactualã€‚ 


## 2024 Counterfactual Learning on Graphs: A Survey 
3.5.1 How to create synthetic dataset 

## 2022 Learning Fair Node Representations with Graph Counter factual Fairness
Two limitation on existing CF on graph:
1. $S_i$ affect the predetection. Red
2. $S_i$ affect $A, X_i$ Green 

GEAR: Graph Counterfactually Fair Node Representation
1. subgraph generation
Node **Importance Score** by prune range of casualmodel to **ego-centric subgraph**( node and its neighbour)
2. Counterfactual Data Argmentation: 
Graph Auto encodder and fair contrains: **self-pertubation**(flip its $S_i$), **neighbour pertubatiob**
3. Node Representation Learning  :
Siamese network to minimize discrepancy 

**Def, Graph conterfactual fairness:**
An encoder $\Phi(\cdot)$ satisfies graph counterfactual fairness if for any node $i$:
$$
P((Z_i)_{S \leftarrow s'} | X = \mathbf{X}, A = \mathbf{A}) = P((Z_i)_{S \leftarrow s''} | X = \mathbf{X}, A = \mathbf{A}),
$$
for all $s' \neq s''$, where $s', s'' \in \{0, 1\}^n$ are arbitrary sensitive attribute values of all nodes, $Z_i = (\Phi(\mathbf{X}, \mathbf{A}))_i$ denotes the node representations.

$\Phi$, minimize the discrepancy between representation $\Phi(X_{S\leftarrow s'}, A_{S\leftarrow s'})$ and $\Phi(X_{S\leftarrow s''}, A_{S\leftarrow s''})$


### GEAR
### 1) subgraph generation
Personalized Pagerank algorithm:
Importance score $\mathbf R=\alpha (\mathbf I-(1-\alpha \mathbf {\bar A}))$, $\mathbf I$, identity\
$R_{i,j}$ How node $j$ is important for node $i$, $\alpha \in [0,1]$

$\mathbf {\bar A}=\mathbf A \mathbf D^{-1} $ column-normalized adjacency matric, $\mathbf D: \mathbf D_{i, i}=\sum_j A{i, j}$

$\mathcal{G}^{(i)}=Sub(i, \mathcal{G}, k)$ :, subgraph generation

- $\mathcal{G}^{(i)} = \{ \mathcal{V}^{(i)}, \mathcal{E}^{(i)}, \mathbf{X}^{(i)} \} = \{ \mathbf{A}^{(i)}, \mathbf{X}^{(i)} \},
$ Vertive, Edge, Features with $S=\{s_i\}_{i=1}^n $ includes in $X$, and $X^{\neg s} = \{ x_1^{\neg s}, ..., x_n^{\neg s} \} $, where $ x_i^{\neg s} = x_i \setminus s_i$

- $\mathcal{V}^{(i)} = \text{TOP}(\mathbf{R}_{i,:}, k),$

- $\mathbf{A}^{(i)} = \mathbf{A}_{\mathcal{V}^{(i)}, \mathcal{V}^{(i)}}, \quad \mathbf{X}^{(i)} = \mathbf{X}_{\mathcal{V}^{(i)}, :},
$, 

### 2ï¼‰Counterfactual Data Augmentation
**GraphVAG**: graph variational auto-encoder\
latent embedding $H=\{h_1, h_2, \cdots, h_k\}$  $H$ is sampled from $q(H|X, A)$,  $p(ğ»)$ is a standard Normal prior distribution\
$\mathcal{L}=$

$\tilde{s}_i$: summary of neighbor info, aggregationof all nodes in subgarph $\mathcal{G}^{(i)}$\
$\tilde{s}_i = \frac{1}{|\mathcal{V}^{(i)}|} \sum_{j \in \mathcal{V}^{(i)}} s_j$

Discriminator,$D(\cdot)$\
$D(\mathbf{H}, b)$  predicts the probability of whether the summary of sensitive attribute values is in range $b$

Fairness Constraint\
$L_d = \sum_{b \in B} \mathbb{E} [\log(D(\mathbf{H}, b))]$\
$L_d$ is a regularizer to minimize the mutual information between the summary of sensitive attribute values and the
embeddings

**Final Loss** for Counterfactual Data Augmentation\
$L_a = L_r + \beta L_d$\
$\beta$ is a hyperparameter for the weight of fairness constraint\
Use alternating SGD for optimization: 
1) minimize $L_{a}$ by fixing the discriminator and updating parameters in other parts; 
2) minimize $âˆ’L_{a}$ with respect to the discriminator while other parts fixed.


#### Self-Perturbation
$\overline{\mathcal{G}}^{(i)} = \{ \mathcal{G}^{(i)}_{S_i \leftarrow 1-s_i} \}$ (flipping sensitive feature)

#### Neighbor-Perturbation
$\underline{\mathcal{G}}^{(i)} = \left\{ \mathcal{G}^{(i)}_{S^{(i)}_{\setminus i} \leftarrow \text{SMP}(S^{(i)}_{\mathcal{V}^{(i)}_{\setminus i}})} \right\}$

subgraph $\mathcal{G}^{(i)}$ ego($i$)-center subgraph with noes $\mathcal{V}^{(i)}$, exclude node $i$: $\mathcal{V}^{(i)}_{\setminus i}$, randomly preterbe the sentsitice value of other nodes: $SMP(\mathcal{V}^{(i)}_{\setminus i})$



Reconstruction Loss (GraphVAE Module)\
$L_r = \mathbb{E}_{q(\mathbf{H}|X, A)} \left[ -\log(p(X, A | \mathbf{H}, S)) \right] + \text{KL}[q(\mathbf{H} | X, A) \| p(\mathbf{H})]$


### 3) Fair Representation learning
**Fairness Loss**
$
L_f = \frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}} \left( (1 - \lambda_s) d(z_i, \bar{z}_i) + \lambda_s d(z_i, \underline{z}_i) \right),
$\
$\lambda_s$ hyperparam control neig-preturbation weight

**Node Representations**
- $
z_i = (\phi(\mathbf{X}^{(i)}, \mathbf{A}^{(i)}))_i,
$
- $
\bar{z}_i = \text{AGG} \left( \left\{ (\phi(\mathbf{X}^{(i)}_{S_i \leftarrow 1-s_i}, \mathbf{A}^{(i)}_{S_i \leftarrow 1-s_i}))_i \right\} \right),
$
- $
\underline{z}_i = \text{AGG} \left( \left\{ (\phi(\mathbf{X}^{(i)}_{S_i \leftarrow \text{SMP}(S^{(i)}_{\mathcal{V}^{(i)}_{\setminus i}})}, \mathbf{A}^{(i)}_{S_i \leftarrow \text{SMP}(S^{(i)}_{\mathcal{V}^{(i)}_{\setminus i}})})_i \right\} \right),
$

Prediction Loss
$L_p = \frac{1}{n} \sum_{i \in [n]} l(f(z_i), y_i),$ $l$: could be CE(Cross entropy), $f(\cdot)$ makes predictions for downstream tasks with the representations, i.e.$ \hat y_i=f(z_i)$

Overall Loss
$
L = L_p + \lambda L_f + \mu \| \theta \|^2,
$

### Dataset creation

Sensitive Attributes
$S_i \sim \text{Bernoulli}(p),$ $p=0.4$ percent $S_i=1$

Latent Embeddings
$Z_i \sim \mathcal{N}(0, \mathbf{I}),$ \
$\mathbf{I}$ identity, dimension of $Z_i$: $d_s=50$

Node Features
$X_i = \mathcal{S}(Z_i) + S_i \mathbf{v},$\
sampling operation $S(\cdot)$ select 25 dims from $Z_i$, $\mathbf{v} \sim \mathcal{N}(0, \mathbf{I})$

Graph Structure
$P(A_{i,j} = 1) = \sigma(\text{cos}(Z_i, Z_j) + a \mathbf{1}(S_i = S_j)),$\
$\sigma$ sigmoid function, $\mathbf{1}(S_i = S_j)==S_i = S_j. \alpha=0.01$

Node Labels
$Y_i = \mathcal{B}(w Z_i + w_s \frac{\sum_{j \in \mathcal{N}_i} S_j}{|\mathcal{N}_i|}),$\
$\mathcal{B}$ Bernulli distribution,$\mathcal{N}_i$ set of neighbors of node i $w, w_i$ weight vector

### Result
Using Synthetic dataset, Bail, Credit










## 24 Three Revisits to Node-Level Graph Anomaly Detection
Outliers, Message Passing and Hyperbolic Neural Networks

### Previous Outlier injection method
$\mathcal{G}=(\mathcal{V}, \mathcal{E}, X, y)$: vertice set, edge set, attibute matrix, label of class

- **Contextual(cntxt.) outlier injection**
Normalize features $x_i'=\frac{x_i}{||x_i||_1}$
Sample $o$ nodes from $\mathcal{V}$ as $\mathcal{V}_c$. without replacement
For node $i$ in $\mathcal{V}_c$, sample $q$ nodes from $\mathcal{V}_r=\mathcal{V}- \mathcal{V}_c$, among them choose the farthest one $j = \text{argmax}_k(||x_i'-x_k'||_2)$ to replace $x_i$ with $x_j$.

- **Strctural(stct.) outlier injection**
create $t$ groups sized $s$ with anomalous nodes.
sample $o=t\times s$ from $\mathcal{V}$ without replacement
Then randoms partition into $t$ groups.
Add edges to make them a clique(fully connected), then drop edges with $p$ probability

#### Score function
The farthest node will have large $||\tilde{\mathbf x}_i||_2$ \
A structural outlier node $i$ will have many neighbors leads to large $||\tilde{\mathbf a}_i||_1$ 


Score function: $score_{norm}(i)=\alpha||\tilde{\mathbf x}_i||_2+(1-\alpha)||\tilde {\mathbf a}_i||_1$,  $\tilde{\mathbf x}_i$: $x_i$ after outlier injection, $\tilde{\mathbf a}_i$: $a_i$ after outlier injection, $A_{ii}=1$\
where cntxt OD, $\alpha=1$, stct OD, $\alpha=0$ :  $\alpha$ ratio of two methods 


test 1: ROC-AUC
For each dataset, use original dataset v.s. l2-nrom for each $x_i$\
do anomaly injection. apply GAD Method to get  $score_{norm}$




### Novel Anomaly injection method





## Sum in terms of Dataset
ä»æ•°æ®é›†çš„è§’åº¦æ¥è¯´ï¼š
### FairGAD:
Reddit:
- æ•°æ®æ¥æºï¼šPost on politic related subReddit
- Labelling Y: based on FACTOID(Sakketou et al., 2022), use the num of posted link(left or right)
- Graph construciton: 







\
\
\
\
\
\
\
\
\
\
\
\
\




# 2024.05.31 Meeting 
Preparation: 
1. è®¨è®ºå¯¹äºSynthetic dataset æ€ä¹ˆåˆ›å»ºçš„ç†è§£ã€‚
2. å¯¹outlier datasetæ€ä¹ˆåˆ›å»ºçš„ç†è§£ã€‚
3. fair + outlier (å‚è€ƒFairGADé‚£ç¯‡çš„åˆ›å»º)

<!-- è¿™ä¸€å‘¨èŠ±äº†ä¸‰å››å¤©åœ¨ä¿¡ä¸€çš„èº«ä¸Šï¼Œä¸€ç§åƒ­è¶Šçš„å¿«ä¹ã€‚
ä½“æ‚Ÿæ˜¯ï¼Œ 
1. å­¦ä¸œè¥¿çš„ç›®çš„æ€§è¿˜æ˜¯ä¸å¤Ÿæ˜æ˜¾ã€‚
2. è¾¹å¬è¯¾è¾¹çœ‹è®ºæ–‡ä¼šå²·å¿æé«˜ç›®çš„æ€§å’Œæé«˜æ•ˆç‡ã€‚
3. å‡å°‘è¿‡åº¦åŠŸåˆ©çš„éœ€æ±‚ï¼Œå­¦ä¸€äº›æœ‰è¶£çš„ä¸œè¥¿ï¼Œå°½é‡é¿å¼€äººã€‚
4. èƒŒå•è¯ã€‚GREè¦å¯„äº†ã€‚ -->


**Meeting**
1. Plan for subgroups:
Mo, We 1-2 p.m.

2. intro to all projects
HNN: Convolution $\rightarrow$ HNN
CNN(T(x)) Paralell Translation equivalence



## Dataset 

![alt text](2024-05-08-papers/image-1.png) from FairGAD(2024)
##
Pokec: 
- source paper: https://arxiv.org/pdf/2009.01454
- repo: FairGNN  https://github.com/EnyanDai/FairGNN
sampled from https://snap.stanford.edu/data/soc-Pokec.html

Bail, Credit, German:
- source paper: https://arxiv.org/pdf/2108.05233 (Dong et al. 2022)
    https://arxiv.org/pdf/1102.2166 (2012)
- repo: EDITS https://github.com/yushundong/EDITS

(æ„Ÿè§‰è®ºæ–‡éƒ¨åˆ†å¼•ç”¨åäº†)

German
- source paper: https://arxiv.org/pdf/2102.13186 (2021)
- repo: NIFTY https://github.com/HongduanTian/NIFTY


UCSD34:
- repo: https://networkrepository.com/socfb-UCSD34.php



# 2024.06.03 Meeting
1. Gujingå­¦å§çš„è®ºæ–‡æ˜¯ unsupervised learningï¼ŒæŒ‰ç…§å¥¹åœ¨pygodé‡Œé¢çš„æ–¹æ³•ï¼ŒæŠŠäºŒåˆ†ç±»çš„ä»»åŠ¡ç”¨fiarness metrixï¼Œç”¨counterfacutalé‡Œçš„è¯„åˆ¤æ ‡å‡†ã€‚EOO, SP, CF(åªåœ¨Syntheticé‡Œæœ‰)

æ‰€ä»¥è¦å†™çš„æ˜¯ï¼š 
1. Fairness metrix çš„è®¡ç®—ï¼Œå¤šç§
2. ä½¿ç”¨å„ç§æ–¹æ³•è·‘ä¸€ä¸‹æ•°æ®é›†ã€‚å¾—åˆ°fairå’Œaccuracyï¼Œå‚è€ƒåˆ«çš„è®ºæ–‡ã€‚

é•¿æœŸä»»åŠ¡ï¼š
1. WSDM 22' çš„åšcounterfactual Data argumentation å’ŒGADçš„æ–¹æ³•æ— å…³ã€‚ï¼Œæ€»çš„æ¥è¯´æ˜¯åœ¨ä¸åŒGAD æ–¹æ³•ä¸Šconsistently improve fairness. WSDM æ˜¯åœ¨æ•°æ®é›†çš„encodingå’Œencodingä¸Šç”¨çš„fairnessã€‚ 
Detection ä¹Ÿæ˜¯ç”¨en/decodingåšçš„ï¼Ÿæœ‰çš„ç”¨GNNä¹Ÿå°±å¯ä»¥predictionäº†ã€‚å¯ä»¥è¯•ç€ç”»ä¸€ä¸ªå›¾ã€‚ 

2. 224Wå¯ä»¥çœ‹17-19ï¼Œ 21å’Œå‰é¢encodingéƒ¨åˆ†åœ¨å­¦ä¸€ä¸‹ã€‚
3. å› æœæ¨æ–­çš„Counterfactualéƒ¨åˆ†çš„å…¬å¼


## Execute
6.3: è§£å†³
1. Synthetic dataset have about $\frac{|V|^2}{2}$ edges when v=2000(paper), edge should be about 4000?
solved by Finding source code of paper in GEAR repo
2. Threading problem with python not shoot
solved by commenting the 22th line in loader.py # from ogb.nodeproppred import PygNodePropPredDataset

6.4
1. å¯ä»¥ä½¿ç”¨ä¸€äº›æ–¹æ³•ï¼Œ 
WSDM 22 GEAR çš„è®ºæ–‡é‡Œç”¨GCN, GraphSAGE, GIN, C-ENC, FairGNN, NIFTY-GCN, NIFTY-SAGE, and GEAR
Gu 24 HNN çš„è®ºæ–‡ç”¨pygodçš„GADçš„åº“
ä½†æ˜¯éƒ½æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä»£ç  

Gear/src
- utils.py: 
    1. load_dataset, sub function
    2. accuracy
- Preprocessing.py:
    1. load_data() deal with params
    2. generate cf subgraph(æ— å…³)
    3. generate_synthetic_data
- models.py:
    1. GCN, GIN, JK, SAGE, Encoder_DGI, GraphInMax, Encoder, Classifier,
    GraphCF, 
- main.py
    1. parser.argment()
    2. evaluate: acc, fairness
    3. compute loss, evaluate sf
    4. train test

HNN_GAD
æ ¹æ®æˆ‘çš„è§‚å¯Ÿï¼Œè¿™ç¯‡é‡Œé¢åªå†™äº†è‡ªå·±çš„æ–¹æ³•çš„ä»£ç ã€‚


6.5 Meeting 
å†³å®šç”¨ray tuneæ¥è°ƒå‚https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html

6.7
æ­£åœ¨å†™scratch_main.py
ç–‘é—®ï¼š
    1. è¿™ä¸ªtrain, val, test æ˜¯æ€ä¹ˆåˆ†çš„,å­å›¾è¿˜æ˜¯ï¼Ÿ
    ???
        evaluateå’Œtestæœ‰ä»€ä¹ˆåŒºåˆ«
    2. evaluateé‡Œçš„counterfactual metrixæ˜¯æ€ä¹ˆç®—çš„ï¼Ÿ
    3. Injectionçš„å‚æ•°
    4. githubæ€ä¹ˆä¸Šä¼ 
    5. 

6.11
    Paulå¼ºè°ƒSRSæ˜¯ä¸ºäº†ä¸°å¯Œç®€å†çš„ï¼Œè¦å¹²å¾ˆå¤šè·Ÿç”³ç ”ç›¸å…³çš„äº‹æƒ…ã€‚
    çœ‹çœ‹æ•™æˆç°åœ¨åœ¨å¹²ä»€ä¹ˆï¼Œfellowshipæ˜¯å•¥ï¼Œï¼Œï¼Ÿï¼Ÿï¼Ÿï¼Ÿpracticing interviewã€‚
    å‡å¤šå°‘å­¦æ ¡ï¼Ÿï¼Ÿï¼Ÿæ²¡å¬æ‡‚
    5-8
    16ï¼Ÿï¼Ÿï¼Ÿ

# 2024.06.12 Meeting
1. CF + guå­¦å§çš„ä¸‰ä¸ªæ–¹æ³•
2. gpuçš„é—®é¢˜è¿˜æ²¡æœ‰è§£å†³
3. injectçš„å¥½åƒä¸æ˜¯ç‰¹åˆ«å½±å“fairness


æ•°æ®é›†çš„æ„é€ æ–¹é¢åœ¨sensitivity groupå’Œæ˜¯ä¸æ˜¯outlierä¹‹é—´åŠ ä¸Šcasualityã€‚FairGADç”¨äº†debiaserçš„æ–¹æ³•ä½¿fairnessé«˜äº†ä¸€ç‚¹
run Jingâ€™s method for GAD: shengenåœ¨åš
Check with Yifei for GPUï¼šcheckäº†ï¼Œç°åœ¨ä¸€äº›modelåœ¨å¤§çš„æ•°æ®é›†ä¸Šè¿˜è¦åˆ†batchã€‚
Check CF scoresï¼šè£…äº†ä¸¤å¤©ç¯å¢ƒï¼Œ
Complete remaining experimentsï¼šæ²¡æœ‰
brainstorm so that outlier injection contains sensitivityï¼šè®¤ä¸º
CF using DA

Motivationï¼š outlier detectionï¼Œ 
Fairnesæœ‰æ•ˆçš„æ•°æ®é›†ï¼š
Outlierçš„æ³¨å…¥ï¼š




## 6.12 é—®é¢˜
### GPU
ä¸€ä¸ªä¸‹åˆä¸»è¦éƒ½åœ¨è§£å†³gpuçš„é—®é¢˜ï¼Œ
#### 1 
é¦–å…ˆç›®å‰æœ€å¤§çš„è°œå›¢æ˜¯Pygodä¸­AdONE(gpu=0)è¿™é‡Œçš„å…‰è°±ä¸ºå•¥åªèƒ½æ˜¯0
æˆ‘å»æ‰¾äº†æºä»£ç ï¼Œåº”è¯¥å¯ä»¥æ˜¯int cudaçš„idï¼Œæ‰€ä»¥ç†è®ºä¸Šåº”è¯¥æ˜¯0-7 éƒ½å¯ä»¥çš„ï¼Œä½†æ˜¯åªæœ‰0å¯è¡Œï¼Œ
ä¸»è¦ä»£ç \
pygod/pygod/detector/base \
pygod/utils/utility.py çš„```validate_device(gpu_id)```å‡½æ•°```gpu_id```å°±æ˜¯```DOMINANT(gpu=0)```é‡Œçš„```gpu```

#### 2
è¿˜æœ‰ä¸€ä¸ªå¾ˆè ¢å¾—å·²ç»è¢«è§£å†³çš„é—®é¢˜æ˜¯
ä¸ºä»€ä¹ˆ.shæ–‡ä»¶ä¼šæŠ¥ã€‚ ä¹‹å‰ä¸€ç›´ä¸æ˜ç™½ä¸ºä»€ä¹ˆå‘½ä»¤è¡Œå°±æ²¡é—®é¢˜ï¼Œä½†æ˜¯.sh å°±ä¸å¯ä»¥ï¼Œåæ¥å‘ä¿¡å•Šæ˜¯æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«
    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.35 GiB. GPU 0 has a total capacty of 23.65 GiB of which 3.01 GiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 16.79 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
è¿™ç§é”™ï¼Œ 
ä¿®æ”¹çš„æ–¹å¼æ˜¯
1. åœ¨ray tune é‡ŒæŠŠç½‘ç»œå¾—å¤§å°ä¿®æ”¹å°ä¸€ç‚¹ï¼Œå¹¶ä¸”åˆ†batchï¼Œé€šè¿‡åœ¨trainæœ€åé‡Šæ”¾å†…å­˜
```
defv train():
    ... ...
    torch.cuda.empty_cache() 
    return
```
2. åœ¨ray tune åˆ†batchã€‚åœ¨mainå¾—ç¬¬ä¸€å¥åŠ ä¸Š
```
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
```

3. æœ¬èº«å› ä¸ºdatasetå’Œmodelçš„å¤§å°ä¸åŒï¼Œæ‰€ä»¥æœ‰çš„æ¨¡å‹è¢«è·‘å‡ºæ¥çš„å¯è¡Œæ€§å°±æ˜¯è¦å°ä¸€ç‚¹
æ¯”å¦‚ä»synthetic < german < bail < credit < pokec
å‰ä¸‰ä¸ªæ˜¯å¯ä»¥è·‘æ‰€æœ‰æ¨¡å‹çš„ï¼Œ
ä½†æ˜¯creditä¸å¯ä»¥è·‘gaan, ä¼šç«™600GiBçš„å†…å­˜ï¼Œguideä¹Ÿéå¸¸æ…¢ï¼Œ credit+guideæ ¹æœ¬æ²¡ä¸Šgpuï¼Ÿï¼Ÿï¼Ÿ
ç„å­¦  

### 6.14 CF

cf_eoo, cf_dp, df, eoo, dp åœ¨è®ºæ–‡ä¸­åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆhttps://arxiv.org/pdf/2201.03662

sens rate 
 
è®ºæ–‡ç®—cfçš„æ–¹æ³•æ˜¯: 
å¯¹æ¯”åŸå›¾å’Œç»è¿‡ä¿®æ”¹sens featureï¼ˆç±»ä¼¼äºperturbeçš„æ‰‹æ³•ï¼‰ï¼Œé€šè¿‡$hat y$ä¹‹é—´çš„æ¥ç®—cf

é‡ç‚¹æ˜¯å¦‚ä½•å¾—åˆ° modified dataï¼Œ ä¹Ÿå°±æ˜¯ evaluate ä¸­çš„ data_cf

éšæœºå– sens_rate * N ä¸ªèŠ‚ç‚¹ï¼Œä½¿$S_i$ä¸º1ï¼Œå‰©ä¸‹ä¸º0.


### GEARé…ç¯å¢ƒè¸©å‘
pygå¾ˆçƒ¦äºº
æˆ‘æ˜¯å…ˆè£…äº†torch1.6.0 + cu10.2
ç„¶åå‘ç°pyg=1.3.0 æ˜¯æœ€è€ç‰ˆæœ¬çš„ï¼Œå°±googleåˆ°äº†pygçš„çš„source code ï¼š https://github.com/pyg-team/pytorch_geometric/releases/tag/1.3.0
ç„¶åå°±åº”è¯¥python setup.py install,ä½†æ˜¯**ç½‘å¾ˆæ…¢**ï¼Œ æ‰€ä»¥è¦å¤šç­‰ä¸€ä¼š
ç„¶åçœ‹åˆ°readmeä¹‹åæ‰‹åŠ¨è£…äº†ä¸ªtorch-sparseä¸€ç±»çš„whlï¼š https://data.pyg.org/whl/
åæ¥å¾ˆå‚»çš„æ‰å‘ç°python setup.py installï¼Œç­‰äº†3åˆ†é’Ÿä¹‹åæŠ¥é”™ï¼Œæ— pytest-runnerï¼Œ äºæ˜¯è¿›setup.pyçœ‹äº†ä¸€ä¸‹ä¹‹åæ‰‹åŠ¨pip install pytest-runner pytest pytest-cov mock,
ç„¶åpython setup.py installä¸€ä¸‹å­å°±å¥½äº†ï¼Œäºæ˜¯åˆæ‰‹åŠ¨ pip install pandas matplotlib Cpython cytoolz aif360



è£…åˆ°aif360 æŠ¥é”™Failed building wheel for llvmliteï¼Œåº”è¯¥æ˜¯æ²¡æœ‰llvmï¼Œäºæ˜¯æ‰‹åŠ¨æœ¬åœ°è£…
è£…äº†9.0.0çš„ç‰ˆæœ¬
    
    å¦‚æœ `llvmlite` çš„é¢„æ„å»ºäºŒè¿›åˆ¶æ–‡ä»¶å’Œ `conda` æ–¹æ³•éƒ½æ— æ³•è§£å†³é—®é¢˜ï¼Œæ™®é€šç”¨æˆ·å¯ä»¥åœ¨ç”¨æˆ·ç›®å½•ä¸­å®‰è£… LLVMï¼Œè€Œä¸éœ€è¦ `sudo` æƒé™ã€‚
    

    
    ```bash
    # ä¸‹è½½å¹¶è§£å‹ LLVM
    wget https://github.com/llvm/llvm-project/releases/download/llvmorg-11.1.0/llvm-11.1.0.src.tar.xz
    tar -xf llvm-11.1.0.src.tar.xz
    
    # åˆ›å»ºæ„å»ºç›®å½•
    mkdir llvm-11.1.0.build
    cd llvm-11.1.0.build
    
    # é…ç½®ç¼–è¯‘ï¼ˆå®‰è£…åœ¨ç”¨æˆ·ç›®å½•ï¼‰
    cmake -G "Unix Makefiles" -DCMAKE_INSTALL_PREFIX=$HOME/llvm ../llvm-11.1.0.src
    
    # ç¼–è¯‘å’Œå®‰è£…
    make -j$(nproc)
    make install
    ```
    
    ç„¶åè®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨æœ¬åœ°å®‰è£…çš„ LLVMï¼š
    
    ```bash
    export PATH=$HOME/llvm/bin:$PATH
    export LD_LIBRARY_PATH=$HOME/llvm/lib:$LD_LIBRARY_PATH
    ```
    

    
    ```bash
    pip install llvmlite --no-binary llvmlite
    ```
    
    ä¹‹åå†å®‰è£… `aif360`ï¼š
    
    ```bash
    pip install aif360
    ```

ä½†æ˜¯è¿˜æ˜¯ä¼šæŠ¥wheel build å¤±è´¥çš„é”™è¯¯ã€‚
äºæ˜¯å°±ç›´æ¥ç»•è¿‡aif360,å› ä¸ºåªç”¨äº†ä¸¤ä¸ªå‡½æ•°ï¼Œæ‰€ä»¥ç›´æ¥å¤åˆ¶å‡½æ•°å’Œå¿…è¦çš„utilè¿‡æ¥äº†ï¼Œç»•è¿‡å®‰è£…aif360çš„é—®é¢˜äº†ã€‚
å®é™…ä¸Šaif360å¯¹python 3.8ä¹‹åæ‰æ¯”è¾ƒå…¼å®¹ï¼Œæ‰€ä»¥ä»¥åç”¨æ–°ä¸€ç‚¹çš„ç¯å¢ƒã€‚

ç„¶åé‡åˆ°äº†AttributeError: Canâ€™t get attribute â€˜DataEdgeAttrâ€™
import torch_geometric.transforms as T
from ogb.nodeproppred import PygNodePropPredDataset
shoujuå­¦å§æé†’å¯ä»¥csdnï¼Œï¼ˆè¿™æ¬¡æä¾›çš„è§£å†³æ–¹æ¡ˆç¡®å®å’Œgptä¸ä¸€æ ·ï¼‰https://blog.csdn.net/oqqENvY12/article/details/129786928 ä¹Ÿæœ‰ä¸€éƒ¨åˆ†ç‰ˆæœ¬è¿‡è€çš„é—®é¢˜
ä½†æ˜¯é€šè¿‡è§‚å¯Ÿï¼Œæ˜¯è·¯å¾„é—®é¢˜ï¼ŒæŠŠä¸€ä¸ªç›¸å¯¹main.py line 541çš„ç›¸å¯¹è·¯å¾„æ”¹æˆç»å¯¹è·¯å¾„å°±æˆåŠŸäº†ï¼Œ
æˆ‘è§‰å¾—**13.23çš„æœåŠ¡å™¨åœ¨è·¯å¾„ä¸Šç¡®å®æœ‰äº›ç„ä¹**

 - è¿è¡Œç»“æŸä¹‹åæ²¡æœ‰åŠæ³•è‡ªåŠ¨å…³é—­
 - german æ•°æ®é›†æ— æ³•æ­£å¸¸ç”Ÿæˆ







### Guå­¦å§çš„ä¸‰ä¸ªmodel
ä¸‡èƒ½çš„CSDN
Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(546): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1
https://blog.csdn.net/ermmtt/article/details/132628639


### Batché—®é¢˜



### Python æ’ä»¶
è¿™ä¸ªæ˜¯æœ€å‚»çš„ï¼Œä¸‹è½½äº† vsix ä¹‹åå‘ç° vscode ç‰ˆæœ¬å¯¹ä¸ä¸Šï¼Œ ç„¶åæ›´æ–°äº†ä¸€ä¸‹ vscode å°±å¥½äº†ã€‚ã€‚ã€‚



# 2024.06.24 Meeting
## 2024.06.19
(åˆšåˆšæ‰äº†è§£ Encoder å’Œ Decoder ä¹Ÿç®—æ˜¯ GNNï¼Œç„¶åçœ‹äº†ä¸€äº›ä¸œè¥¿
(OIä½¬å†™çš„GEARä»£ç ï¼Œçœ‹ä¸æ‡‚ï¼Œ



# 2024.07.14
<!-- ç«Ÿç„¶è¿‡äº†ä¸€ä¸ªæœˆäº†ã€‚ -->

## 2024.06.10å¼€ä¼š
1. åšbenchmark. + Jingå­¦å§çš„ä¸‰ä¸ªHNNå·²ç»æ”¹æˆäº†Classä½†æ˜¯AUCè¿˜æ˜¯å¤ªä½äº†ã€‚ã€‚ã€‚
2. æ‰‹å†™encoderå’Œdecoderã€‚ç»“æ„æœªçŸ¥ï¼Œä½†æ˜¯ä¸»è¦æ˜¯ä¿®æ”¹Loss Function???åŸºäºCFçš„ï¼Œå¯ä»¥åœ¨dominantä¸Šä¿®æ”¹
3. è§£é‡Šä¸ºä»€ä¹ˆCFæ˜¯ä½çš„ï¼Œçœ‹decoderå‡ºæ¥çš„sens'ï¼Œæ˜¯è¿˜åŸäº†sensè¿˜æ˜¯éƒ½æ˜¯1/2.è¿™ä¸¤è€…éƒ½æ˜¯å¯ä»¥è§£é‡Šçš„
4. åœ¨å­¦ä¸€ä¸‹CFä¹‹ç±»çš„ç†è®ºã€‚
5. GNNNNNNNNNN






# 2024.08.15
<!-- ç«Ÿç„¶åˆè¿‡äº†ä¸€ä¸ªæœˆäº†ã€‚ -->
<!-- æŠ½è±¡ï¼Œå®Œå…¨ä¸çŸ¥é“è‡ªå·±åœ¨å¹²å•¥ã€‚ã€‚ã€‚ -->

1.  DOIMINANT 19, DONE 21, gadnr 24, ada-gad 234. (CFGN denied)
2.  benchmarkå’Œä¸€äº›sens reconstruct çš„å€¼å¯¹æ¯”ï¼ˆä¹‹å‰è²Œä¼¼åªä½œäº†guide 21çš„ï¼‰
3.  è®ºæ–‡ä¹Ÿçœ‹ä¸å‡ºæ¥åœ¨å†™å•¥
4.  

08.18 äº¤srsæŠ¥å‘Š
08.25 GRE è€ƒè¯•ï¼Œå¯„
æ‰¾solæ•™æˆè¯¢é—®music techæ–¹å‘çš„é—®é¢˜
è®ºæ–‡è®ºæ–‡è®ºæ–‡







