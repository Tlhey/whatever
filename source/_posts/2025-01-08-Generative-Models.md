---
title: 2025-01-08-Generative-Models
date: 2025-01-08 09:29:17
tags:
---
<!-- www
## 01-08
### Histogram 
### Kernel Method
Kernel Density Estimation (KDE) æ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºä¼°è®¡ä¸€ä¸ªéšæœºå˜é‡çš„**è¿ç»­çš„**æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆProbability Density Function, PDFï¼‰ã€‚

ç›´æ–¹å›¾æœ‰ä¸ªé—®é¢˜ï¼š

- **å®ƒæ˜¯ç¦»æ•£çš„**ï¼šä½ åªèƒ½çœ‹åˆ°å›ºå®šçš„æ—¶é—´é—´éš”ï¼Œæ¯”å¦‚10:00-11:00ï¼Œ11:00-12:00ã€‚
- **å®ƒä¾èµ–åˆ†ç®±ï¼ˆbinï¼‰å¤§å°**ï¼šä¸åŒçš„åˆ†ç®±æ–¹å¼ä¼šå½±å“ç›´æ–¹å›¾çš„å½¢çŠ¶ã€‚

**KDE å°±æ˜¯ä¸€ç§è¿ç»­çš„åˆ†å¸ƒä¼°è®¡æ–¹æ³•**ï¼Œå®ƒä¸ä¾èµ–äºåˆ†ç®±ï¼Œè€Œæ˜¯ç”¨ä¸€ä¸ª **â€œå¹³æ»‘çš„æ›²çº¿â€** æ¥è¡¨ç¤ºæ•°æ®çš„åˆ†å¸ƒã€‚  
ä½ å¯ä»¥æŠŠ KDE æƒ³è±¡æˆï¼š

- æ¯ä¸ªæ•°æ®ç‚¹éƒ½ç”»ä¸€ä¸ª **å°å±±ä¸˜ï¼ˆkernelï¼‰**ã€‚
- ç„¶åæŠŠè¿™äº›å°å±±ä¸˜ **å åŠ èµ·æ¥**ï¼Œå¾—åˆ°ä¸€ä¸ªè¿ç»­çš„æ›²çº¿ï¼Œä»£è¡¨æ•°æ®çš„åˆ†å¸ƒã€‚

KDE çš„å…¬å¼ä¸ºï¼š
\(\hat{f}(x) = \frac{1}{n h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)\)

**è§£é‡Šæ¯ä¸ªç¬¦å·**ï¼š

- **\( \hat{f}(x) \)**ï¼šä¼°è®¡çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚
- **\( n \)**ï¼šæ ·æœ¬æ•°é‡ã€‚
- **\( h \)**ï¼šå¹³æ»‘å‚æ•°ï¼Œç§°ä¸º **å¸¦å®½ï¼ˆbandwidthï¼‰**ã€‚å®ƒå†³å®šäº†å°å±±ä¸˜çš„â€œå®½åº¦â€ã€‚
- **\( K \)**ï¼š**æ ¸å‡½æ•°ï¼ˆKernel Functionï¼‰**ï¼Œå†³å®šäº†å°å±±ä¸˜çš„å½¢çŠ¶ã€‚

æ ¸å‡½æ•°æ˜¯ KDE çš„æ ¸å¿ƒã€‚å¸¸è§çš„æ ¸å‡½æ•°æœ‰ï¼š

1. **é«˜æ–¯æ ¸ï¼ˆGaussian Kernelï¼‰**  
   å½¢çŠ¶åƒæ­£æ€åˆ†å¸ƒçš„å°å±±ä¸˜ï¼Œæœ€å¸¸ç”¨çš„æ ¸å‡½æ•°ã€‚

   \[
   K(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
   \]

2. **ç®±å½¢æ ¸ï¼ˆBox Kernelï¼‰**  
   æ¯ä¸ªæ•°æ®ç‚¹ç”Ÿæˆä¸€ä¸ªâ€œæ–¹å½¢çš„å°å±±ä¸˜â€ï¼Œè¾¹ç•Œæ¸…æ™°ï¼Œä½†ä¸å¹³æ»‘ã€‚

3. **ä¸‰è§’æ ¸ï¼ˆTriangle Kernelï¼‰**  
   ç”Ÿæˆä¸€ä¸ªä¸‰è§’å½¢çš„å°å±±ä¸˜ã€‚

å¸¦å®½ï¼ˆBandwidthï¼‰å¯¹ KDE çš„å½±å“**

**å¸¦å®½ï¼ˆhï¼‰** æ˜¯ KDE ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°ã€‚å®ƒå†³å®šäº†æ¯ä¸ªå°å±±ä¸˜çš„ **å®½åº¦**ï¼Œä»è€Œå½±å“æœ€ç»ˆæ›²çº¿çš„å¹³æ»‘ç¨‹åº¦ã€‚

- **å¸¦å®½å°**ï¼šæ›²çº¿æ›´è´´è¿‘æ•°æ®ï¼Œä½†å®¹æ˜“å‡ºç°â€œè¿‡æ‹Ÿåˆâ€ã€‚
- **å¸¦å®½å¤§**ï¼šæ›²çº¿æ›´å¹³æ»‘ï¼Œä½†å¯èƒ½ä¼šå¿½ç•¥ä¸€äº›ç»†èŠ‚ã€‚

### Series Methods
Imagine you're trying to approximate a complex function \( f(x) \) by expressing it as a combination of simpler functions (called **basis functions**). This is like saying:

\[ f(x) = \sum_{j=1}^{\infty} \beta_j \phi_j(x) \]

Where:
- \( \phi_j(x) \) are the **basis functions** (like sine, cosine, polynomials, etc.)
- \( \beta_j \) are the **coefficients** that tell us how much of each basis function to use.

---

### ğŸ§© **Orthogonal Basis**  
The basis functions \( \phi_j \) are said to be **orthogonal** if they satisfy the condition:

\[ \int \phi_j(x) \phi_k(x) dx = 0 \quad \text{for} \quad j \neq k \]

Think of orthogonal basis functions like directions that are at right angles to each other â€” they don't overlap in their effects.

---

### ğŸ“š **Cosine Basis Example**
The notes give an example of using **cosine functions** as the basis:

\[ \phi_j(x) = \sqrt{2} \cos(2\pi j x) \]

These functions oscillate and can capture periodic patterns in the data.

---

### ğŸ“ˆ **Wavelet Basis**
There's also a reference to **wavelet basis functions**, which are used to capture both **local** and **global** structures in the data.

---

### ğŸ” **Key Formula for Density Estimation**  
The board shows that if we are estimating a **density function** \( f \), we can approximate it using the formula:

\[ \hat{f}(x) = \sum_{j=1}^{k} \hat{\beta}_j \phi_j(x) \]

Where:
- \( \hat{\beta}_j = \frac{1}{n} \sum_{i=1}^{n} \phi_j(X_i) \)  
  (This is the estimated coefficient based on sample data \( X_i \).)

---

### ğŸ§  **Key Idea: Selecting the Right \( k \)**
Choosing the right number of basis functions (\( k \)) is important. If \( k \) is too large, the model will **overfit** (too complicated). If \( k \) is too small, the model will **underfit** (too simple).




## 01-15


## Part 1: Risk Analysis

### Background and Setup

You have \( N \) points \( x_1, x_2, \ldots, x_N \in \mathbb{R}^d \). This means you have \( N \) data samples, each one is a \( d \)-dimensional vector. We assume these points lie in some space \( X \subset \mathbb{R}^d \), which is said to be a **compact set** (meaning it is closed and bounded in the mathematical sense).

### Key Smoothness Assumption

You mentioned an assumption:

\[
D^s p(x) - D^s p(y) \;\le\; L \,\|x - y\|
\]

1. **\( D^s p(x) \)**:  
   - This notation represents a high-order derivative (partial derivative) of a probability density function \(p(x)\).  
   - The superscript \( s \) is a **multi-index**.  
     - For instance, \( s = (s_1, s_2, \dots, s_d) \).  
     - Then \( D^s p(x)\) means we take the \((s_1 + s_2 + \cdots + s_d)\)-th partial derivative of \( p \) with respect to each coordinate appropriately:
       \[
       D^s p(x) \;=\; \frac{\partial^{\,s_1 + s_2 + \cdots + s_d} p(x)}{\partial x_1^{s_1}\,\partial x_2^{s_2}\,\cdots\,\partial x_d^{s_d}}.
       \]

2. **Interpretation of the inequality**:  
   \[
   D^s p(x) - D^s p(y) \;\le\; L\, \|x - y\|.
   \]  
   This says that **the change in the \(s\)-th derivative** between \(x\) and \(y\) is bounded by their distance times some constant \(L\). It suggests that \(p(x)\) is **smooth**â€”no wild jumps in its high-order derivatives. This is often used in probability density estimation and ensures continuity and differentiability in a controlled way.

3. **Why Compactness Matters**:  
   - If \( X \) is compact, itâ€™s easier to control or bound various integrals and derivatives since \( x \) cannot go off to infinity.  
   - This is important in risk analysis because it helps with bounding error terms and ensuring integrals converge.

> **Reference for multi-index notation**:  
> - [Brilliant.org: Multi-Index Notation](https://brilliant.org/wiki/multi-index-notation/)  
> - [Wikipedia: Multi-index](https://en.wikipedia.org/wiki/Multi-index)  

---

## Part 2: More on \( D^s p(x) \) and Multi-Index Notation

If \( p(x) \) is a function of multiple variables, say \( p(x_1, x_2, \dots, x_d) \), then  
\[
s = (s_1, s_2, \dots, s_d)
\]  
tells us **how many times** we differentiate with respect to each variable \( x_i \).  

For example, if \( s = (2, 1, 0, \dots, 0) \), then  
\[
D^s p(x) \;=\; \frac{\partial^3 p(x)}{\partial x_1^2 \,\partial x_2^1}.
\]

### Intuitive View
- If \(s_1 = 2\), that means â€œtake the second derivative with respect to \(x_1\)â€.  
- If \(s_2 = 1\), that means â€œtake the first derivative with respect to \(x_2\)â€.  
- We multiply these partial derivatives together in the correct order.

This notation is just a concise way to keep track of all the derivatives in multiple dimensions.

---

## Part 3: Taylor Expansion (Taylorâ€™s Theorem)

A general **Taylor expansion** around a point \( x \) says that if \(f\) is sufficiently smooth, we can write:

\[
f(x + h) 
\;=\;
f(x) \;+\; 
\sum_{k=1}^{m} \frac{1}{k!} \bigl( D^k f(x) \bigr) (h,\dots,h) \;+\; R_m,
\]

where \( R_m \) is the remainder term, and \( D^k f(x) \) is the \(k\)-th derivative of \(f\). In multiple dimensions, we typically see it in terms of partial derivatives:

\[
f(x + h) \;\approx\; 
f(x) 
\;+\; \nabla f(x) \cdot h
\;+\; \frac{1}{2} h^\top \nabla^2 f(x) \, h
\;+\; \dots
\]

This expansion is used **a lot** in analyzing the bias of estimators, numerical methods, and even in bounding differences between function values.

> **Reference for Taylor expansions**:  
> - [Wikipedia: Taylor's theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem)  
> - [MathWorld: Taylor Series](https://mathworld.wolfram.com/TaylorSeries.html)  

---

## Part 4: Kernel Density Estimation (KDE)

You also mentioned **Kernel Density Estimation**. Itâ€™s a method to estimate an unknown probability density function \( f(x) \) from data. The estimator looks like:

\[
\hat{f}(x) \;=\; \frac{1}{N h} \;\sum_{i=1}^{N} K\Bigl(\frac{x - X_i}{h}\Bigr),
\]

where
- \(K\) is a **kernel function** (often something like a Gaussian, Epanechnikov, etc.),  
- \(h\) is the **bandwidth** (a smoothing parameter).

### 4.1 Kernel Function Properties

A kernel \(K\) often satisfies:

1. \(\int K(u)\,du = 1\).  
2. \(\int u\,K(u)\,du = 0\). (Centered around 0)  
3. \(\int u^r K(u)\,du = 0\) for odd \(r\) if itâ€™s symmetric, etc.

An example of a kernel is the **Epanechnikov kernel**:

\[
G(x) \;=\; \frac{3}{4}\,\bigl(1 - x^2\bigr) \, 1(\lvert x \rvert \le 1),
\]

where \(1(\lvert x \rvert \le 1)\) is the indicator function that is 1 if \(\lvert x \rvert \le 1\) and 0 otherwise. 

> **Reference for Kernel Density Estimation**:  
> - [â€œKernel Density Estimation Explainedâ€ on Towards Data Science](https://towardsdatascience.com/kernel-density-estimation-explained-52045f84c726)  
> - [A Comprehensive Guide to KDE on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-kernel-density-estimation/)  

---

### 4.2 Bias of the KDE

The **bias** of an estimator \(\hat{f}(x)\) is:

\[
\text{Bias}(\hat{f}(x)) \;=\; 
E[\hat{f}(x)] \;-\; f(x).
\]

For KDE, we often find that:

\[
E[\hat{f}(x)]
\;=\; 
\int K\!\Bigl(\frac{x - t}{h}\Bigr) p(t)\, \frac{dt}{h}.
\]

By doing a change of variable and performing a Taylor expansion of \(p(\cdot)\), we eventually get:

\[
E[\hat{f}(x)]
\;=\; 
p(x) 
\;+\; 
\frac{h^2}{2}\,\mu_2(K)\,p''(x) 
\;+\; 
O(h^4),
\]

where \(\mu_2(K)\) is the second moment of the kernel. So the **leading term** of the bias is:

\[
\text{Bias}(\hat{f}(x))
\;=\;
\frac{h^2}{2}\,\mu_2(K)\,p''(x)
\;+\;
O(h^4).
\]

This tells us that the bias grows (roughly) like \(h^2\). If \(h\) is too big, the bias is large (over-smoothing), but if \(h\) is too small, you get high variance (under-smoothing). Thatâ€™s why **bandwidth selection** is super important.

> **Reference for bias-variance in KDE**:  
> - [Lecture notes on Kernel Density Estimation (CMU)](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-kernel-density/kde.pdf)  


## 02-28
Cold Diffusion


## 03-10
çªç„¶è§‰å¾—è‡ªå·±æ‚Ÿäº†ï¼Ÿï¼Ÿ
How others put them together
https://www.youtube.com/watch?v=B-d_3xX6ss4
æ ¹æ®song yangçš„è®ºæ–‡çš„æ—¶é—´çº¿ã€‚
1; sliced score model (å®šä¹‰äº†score based model)
(https://proceedings.mlr.press/v115/song20a/song20a.pdf)

2; annealed Langevin Dynamics
(https://papers.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html)
langevin mcmc sample; https://zhuanlan.zhihu.com/p/797467112
the whole thing; https://yang-song.net/blog/2021/score/

3; Score-based GM through SED (Song et al. ICLR 2021)
(https://arxiv.org/pdf/2011.13456)
SED è§†è§’ç»Ÿä¸€ï¼› 
smld (ve) å’Œ ddpm (vp)
sedå’Œof-odeæœ‰ä¸€äº›å¯¹åº”å…³ç³»

4; EDM 
ï¼ˆhttps://proceedings.neurips.cc/paper_files/paper/2022/file/a98846e9d9cc01cfb87eb694d946ce6b-Paper-Conference.pdfï¼‰
pf-odeè§†è§’ç»Ÿä¸€äº†diffusion æ¨¡å‹

5; CM
ç”¨äº†edm

6; 







 -->
